<html>
<head>
<title>prng.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
prng.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2021 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">from </span><span class="s1">__future__ </span><span class="s2">import </span><span class="s1">annotations</span>

<span class="s2">import </span><span class="s1">abc</span>
<span class="s2">from </span><span class="s1">functools </span><span class="s2">import </span><span class="s1">partial</span><span class="s2">, </span><span class="s1">reduce</span>
<span class="s2">import </span><span class="s1">math</span>
<span class="s2">import </span><span class="s1">operator </span><span class="s2">as </span><span class="s1">op</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">(Any</span><span class="s2">, </span><span class="s1">Callable</span><span class="s2">, </span><span class="s1">Hashable</span><span class="s2">, </span><span class="s1">Iterator</span><span class="s2">, </span><span class="s1">List</span><span class="s2">, </span><span class="s1">NamedTuple</span><span class="s2">,</span>
                    <span class="s1">Set</span><span class="s2">, </span><span class="s1">Sequence</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">, </span><span class="s1">Union)</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>

<span class="s2">from </span><span class="s1">jax </span><span class="s2">import </span><span class="s1">lax</span>
<span class="s2">from </span><span class="s1">jax </span><span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">jnp</span>
<span class="s2">from </span><span class="s1">jax </span><span class="s2">import </span><span class="s1">tree_util</span>

<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">ad_util</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">api</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">basearray</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">config </span><span class="s2">as </span><span class="s1">config_lib</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">core</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">dispatch</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">dtypes</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">pretty_printer </span><span class="s2">as </span><span class="s1">pp</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">sharding_specs</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">typing</span>
<span class="s2">from </span><span class="s1">jax._src.api </span><span class="s2">import </span><span class="s1">jit</span><span class="s2">, </span><span class="s1">vmap</span>
<span class="s2">from </span><span class="s1">jax._src.config </span><span class="s2">import </span><span class="s1">config</span>
<span class="s2">from </span><span class="s1">jax._src.dtypes </span><span class="s2">import </span><span class="s1">float0</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">ad</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">batching</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">mlir</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">pxla</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">xla</span>
<span class="s2">from </span><span class="s1">jax._src.lax </span><span class="s2">import </span><span class="s1">lax </span><span class="s2">as </span><span class="s1">lax_internal</span>
<span class="s2">from </span><span class="s1">jax._src.lax </span><span class="s2">import </span><span class="s1">utils </span><span class="s2">as </span><span class="s1">lax_utils</span>
<span class="s2">from </span><span class="s1">jax._src.lib.mlir </span><span class="s2">import </span><span class="s1">ir</span>
<span class="s2">from </span><span class="s1">jax._src.lib </span><span class="s2">import </span><span class="s1">gpu_prng</span>
<span class="s2">from </span><span class="s1">jax._src.lib </span><span class="s2">import </span><span class="s1">version </span><span class="s2">as </span><span class="s1">jaxlib_version</span>
<span class="s2">from </span><span class="s1">jax._src.lib </span><span class="s2">import </span><span class="s1">xla_client </span><span class="s2">as </span><span class="s1">xc</span>
<span class="s2">from </span><span class="s1">jax._src.lib.mlir.dialects </span><span class="s2">import </span><span class="s1">hlo</span>
<span class="s2">from </span><span class="s1">jax._src.numpy.array_methods </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_array_operators</span><span class="s2">, </span><span class="s1">_set_array_base_attributes</span><span class="s2">, </span><span class="s1">_IndexUpdateHelper)</span>
<span class="s2">from </span><span class="s1">jax._src.partition_spec </span><span class="s2">import </span><span class="s1">PartitionSpec</span>
<span class="s2">from </span><span class="s1">jax._src.sharding_impls </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">NamedSharding</span><span class="s2">, </span><span class="s1">PmapSharding</span><span class="s2">, </span><span class="s1">GSPMDSharding</span><span class="s2">, </span><span class="s1">XLACompatibleSharding)</span>
<span class="s2">from </span><span class="s1">jax._src.typing </span><span class="s2">import </span><span class="s1">Array</span>
<span class="s2">from </span><span class="s1">jax._src.util </span><span class="s2">import </span><span class="s1">safe_map</span><span class="s2">, </span><span class="s1">safe_zip</span>

<span class="s1">map</span><span class="s2">, </span><span class="s1">unsafe_map = safe_map</span><span class="s2">, </span><span class="s1">map</span>
<span class="s1">zip</span><span class="s2">, </span><span class="s1">unsafe_zip = safe_zip</span><span class="s2">, </span><span class="s1">zip</span>

<span class="s1">Device = xc.Device</span>
<span class="s1">Shard = Any  </span><span class="s0"># TODO(jakevdp): fix circular imports and import Shard</span>

<span class="s1">UINT_DTYPES = {</span>
    <span class="s3">8</span><span class="s1">: jnp.uint8</span><span class="s2">, </span><span class="s3">16</span><span class="s1">: jnp.uint16</span><span class="s2">, </span><span class="s3">32</span><span class="s1">: jnp.uint32</span><span class="s2">, </span><span class="s3">64</span><span class="s1">: jnp.uint64}  </span><span class="s0"># type: ignore[has-type]</span>

<span class="s0"># -- PRNG implementation interface</span>

<span class="s2">class </span><span class="s1">PRNGImpl(NamedTuple):</span>
  <span class="s4">&quot;&quot;&quot;Specifies PRNG key shape and operations. 
 
  A PRNG implementation is determined by a key type ``K`` and a 
  collection of functions that operate on such keys. The key type 
  ``K`` is an array type with element type uint32 and shape specified 
  by ``key_shape``. The type signature of each operations is:: 
 
    seed :: int[] -&gt; K 
    fold_in :: K -&gt; int[] -&gt; K 
    split[n] :: K -&gt; K[n] 
    random_bits[shape, bit_width] :: K -&gt; uint&lt;bit_width&gt;[shape] 
 
  A PRNG implementation is adapted to an array-like object of keys 
  ``K`` by the ``PRNGKeyArray`` class, which should be created via the 
  ``seed_with_impl`` function. 
  &quot;&quot;&quot;</span>
  <span class="s1">key_shape: core.Shape</span>
  <span class="s1">seed: Callable</span>
  <span class="s1">split: Callable</span>
  <span class="s1">random_bits: Callable</span>
  <span class="s1">fold_in: Callable</span>
  <span class="s1">tag: str = </span><span class="s5">'?'</span>

  <span class="s2">def </span><span class="s1">__hash__(self) -&gt; int:</span>
    <span class="s2">return </span><span class="s1">hash(self.tag)</span>

  <span class="s2">def </span><span class="s1">__str__(self) -&gt; str:</span>
    <span class="s2">return </span><span class="s1">self.tag</span>

  <span class="s2">def </span><span class="s1">pprint(self):</span>
    <span class="s2">return </span><span class="s1">(pp.text(</span><span class="s5">f&quot;</span><span class="s2">{</span><span class="s1">self.__class__.__name__</span><span class="s2">} </span><span class="s5">[</span><span class="s2">{</span><span class="s1">self.tag</span><span class="s2">}</span><span class="s5">]:&quot;</span><span class="s1">) +</span>
            <span class="s1">pp.nest(</span><span class="s3">2</span><span class="s2">, </span><span class="s1">pp.group(pp.brk() + pp.join(pp.brk()</span><span class="s2">, </span><span class="s1">[</span>
              <span class="s1">pp.text(</span><span class="s5">f&quot;</span><span class="s2">{</span><span class="s1">k</span><span class="s2">} </span><span class="s5">= </span><span class="s2">{</span><span class="s1">v</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s1">) </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">self._asdict().items()</span>
            <span class="s1">]))))</span>


<span class="s0"># -- PRNG key arrays</span>

<span class="s2">def </span><span class="s1">_check_prng_key_data(impl</span><span class="s2">, </span><span class="s1">key_data: typing.Array):</span>
  <span class="s1">ndim = len(impl.key_shape)</span>
  <span class="s2">if not </span><span class="s1">all(hasattr(key_data</span><span class="s2">, </span><span class="s1">attr) </span><span class="s2">for </span><span class="s1">attr </span><span class="s2">in </span><span class="s1">[</span><span class="s5">'ndim'</span><span class="s2">, </span><span class="s5">'shape'</span><span class="s2">, </span><span class="s5">'dtype'</span><span class="s1">]):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;JAX encountered invalid PRNG key data: expected key_data &quot;</span>
                    <span class="s5">f&quot;to have ndim, shape, and dtype attributes. Got </span><span class="s2">{</span><span class="s1">key_data</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">key_data.ndim &lt; </span><span class="s3">1</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;JAX encountered invalid PRNG key data: expected &quot;</span>
                    <span class="s5">f&quot;key_data.ndim &gt;= 1; got ndim=</span><span class="s2">{</span><span class="s1">key_data.ndim</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">key_data.shape[-ndim:] != impl.key_shape:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;JAX encountered invalid PRNG key data: expected key_data.shape to &quot;</span>
                    <span class="s5">f&quot;end with </span><span class="s2">{</span><span class="s1">impl.key_shape</span><span class="s2">}</span><span class="s5">; got shape=</span><span class="s2">{</span><span class="s1">key_data.shape</span><span class="s2">} </span><span class="s5">for </span><span class="s2">{</span><span class="s1">impl=</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">key_data.dtype </span><span class="s2">not in </span><span class="s1">[np.uint32</span><span class="s2">, </span><span class="s1">float0]:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;JAX encountered invalid PRNG key data: expected key_data.dtype = uint32; &quot;</span>
                    <span class="s5">f&quot;got dtype=</span><span class="s2">{</span><span class="s1">key_data.dtype</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">PRNGKeyArrayMeta(abc.ABCMeta):</span>
  <span class="s4">&quot;&quot;&quot;Metaclass for overriding PRNGKeyArray isinstance checks.&quot;&quot;&quot;</span>

  <span class="s2">def </span><span class="s1">__instancecheck__(cls</span><span class="s2">, </span><span class="s1">instance):</span>
    <span class="s2">try</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">(isinstance(instance.aval</span><span class="s2">, </span><span class="s1">core.ShapedArray) </span><span class="s2">and</span>
              <span class="s1">type(instance.aval.dtype) </span><span class="s2">is </span><span class="s1">KeyTy)</span>
    <span class="s2">except </span><span class="s1">AttributeError:</span>
      <span class="s2">return </span><span class="s1">super().__instancecheck__(instance)</span>


<span class="s2">class </span><span class="s1">PRNGKeyArray(abc.ABC</span><span class="s2">, </span><span class="s1">metaclass=PRNGKeyArrayMeta):</span>
  <span class="s4">&quot;&quot;&quot;An array whose elements are PRNG keys&quot;&quot;&quot;</span>

  <span class="s1">@abc.abstractmethod  </span><span class="s0"># TODO(frostig): rename</span>
  <span class="s2">def </span><span class="s1">unsafe_raw_array(self) -&gt; PRNGKeyArray: ...</span>

  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">unsafe_buffer_pointer(self) -&gt; int: ...</span>

  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">block_until_ready(self) -&gt; PRNGKeyArray: ...</span>

  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">copy_to_host_async(self) -&gt; </span><span class="s2">None</span><span class="s1">: ...</span>

  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">shape(self) -&gt; Tuple[int</span><span class="s2">, </span><span class="s1">...]: ...</span>

  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">ndim(self) -&gt; int: ...</span>

  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">size(self) -&gt; int: ...</span>

  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">dtype(self): ...</span>

  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">sharding(self): ...</span>

  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">at(self) -&gt; _IndexUpdateHelper: ...</span>

  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">__len__(self) -&gt; int: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">__iter__(self) -&gt; Iterator[PRNGKeyArray]: ...</span>

  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">reshape(self</span><span class="s2">, </span><span class="s1">newshape</span><span class="s2">, </span><span class="s1">order=</span><span class="s2">None</span><span class="s1">)           -&gt; PRNGKeyArray: ...</span>

  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">T(self)                   -&gt; PRNGKeyArray: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">__getitem__(self</span><span class="s2">, </span><span class="s1">_)      -&gt; PRNGKeyArray: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">ravel(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)     -&gt; PRNGKeyArray: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">squeeze(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)   -&gt; PRNGKeyArray: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">swapaxes(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)  -&gt; PRNGKeyArray: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">take(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)      -&gt; PRNGKeyArray: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">transpose(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__) -&gt; PRNGKeyArray: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">flatten(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)   -&gt; PRNGKeyArray: ...</span>

  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">is_fully_addressable(self) -&gt; bool: ...</span>
  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">is_fully_replicated(self) -&gt; bool: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">device(self) -&gt; Device: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">devices(self) -&gt; Set[Device]: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">delete(self) -&gt; </span><span class="s2">None</span><span class="s1">: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">is_deleted(self) -&gt; bool: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">on_device_size_in_bytes(self) -&gt; int: ...</span>
  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">addressable_shards(self) -&gt; List[Shard]: ...</span>
  <span class="s1">@property</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">global_shards(self) -&gt; List[Shard]: ...</span>
  <span class="s1">@abc.abstractmethod</span>
  <span class="s2">def </span><span class="s1">addressable_data(self</span><span class="s2">, </span><span class="s1">index: int) -&gt; PRNGKeyArray: ...</span>

  <span class="s0"># TODO(jakevdp): potentially add tolist(), tobytes(),</span>
  <span class="s0">#    device_buffer, device_buffers, __cuda_interface__()</span>


<span class="s2">class </span><span class="s1">PRNGKeyArrayImpl(PRNGKeyArray):</span>
  <span class="s4">&quot;&quot;&quot;An array of PRNG keys backed by an RNG implementation. 
 
  This class lifts the definition of a PRNG, provided in the form of a 
  ``PRNGImpl``, into an array-like pytree class. Instances of this 
  class behave like an array whose base elements are keys, hiding the 
  fact that keys are typically arrays (of ``uint32`` dtype) themselves. 
 
  PRNGKeyArrays are also restricted relative to JAX arrays in that 
  they do not expose arithmetic operations. They instead expose 
  wrapper methods around the PRNG implementation functions (``split``, 
  ``random_bits``, ``fold_in``). 
  &quot;&quot;&quot;</span>

  <span class="s1">impl: PRNGImpl</span>
  <span class="s1">_base_array: typing.Array</span>

  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">impl</span><span class="s2">, </span><span class="s1">key_data: Any):</span>
    <span class="s2">assert not </span><span class="s1">isinstance(key_data</span><span class="s2">, </span><span class="s1">core.Tracer)</span>
    <span class="s1">_check_prng_key_data(impl</span><span class="s2">, </span><span class="s1">key_data)</span>
    <span class="s1">self.impl = impl</span>
    <span class="s1">self._base_array = key_data</span>

  <span class="s0"># TODO(frostig): rename to unsafe_base_array, or just offer base_array attr?</span>
  <span class="s2">def </span><span class="s1">unsafe_raw_array(self):</span>
    <span class="s4">&quot;&quot;&quot;Access the raw numerical array that carries underlying key data. 
 
    Returns: 
      A uint32 JAX array whose leading dimensions are ``self.shape``. 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">self._base_array</span>

  <span class="s2">def </span><span class="s1">block_until_ready(self):</span>
    <span class="s1">_ = self._base_array.block_until_ready()</span>
    <span class="s2">return </span><span class="s1">self</span>

  <span class="s2">def </span><span class="s1">copy_to_host_async(self):</span>
    <span class="s1">_ = self._base_array.copy_to_host_async()</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">aval(self):</span>
    <span class="s2">return </span><span class="s1">keys_shaped_array(self.impl</span><span class="s2">, </span><span class="s1">self.shape)</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">shape(self):</span>
    <span class="s2">return </span><span class="s1">base_arr_shape_to_keys_shape(self.impl</span><span class="s2">, </span><span class="s1">self._base_array.shape)</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">size(self):</span>
    <span class="s2">return </span><span class="s1">math.prod(self.shape)</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">ndim(self):</span>
    <span class="s2">return </span><span class="s1">len(self.shape)</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">dtype(self):</span>
    <span class="s2">return </span><span class="s1">KeyTy(self.impl)</span>

  <span class="s1">_device = property(op.attrgetter(</span><span class="s5">'_base_array._device'</span><span class="s1">))</span>
  <span class="s1">_committed = property(op.attrgetter(</span><span class="s5">'_base_array._committed'</span><span class="s1">))</span>
  <span class="s1">device = property(op.attrgetter(</span><span class="s5">'_base_array.device'</span><span class="s1">))  </span><span class="s0"># type: ignore[assignment]</span>
  <span class="s1">devices = property(op.attrgetter(</span><span class="s5">'_base_array.devices'</span><span class="s1">))  </span><span class="s0"># type: ignore[assignment]</span>
  <span class="s1">is_fully_addressable = property(op.attrgetter(</span><span class="s5">'_base_array.is_fully_addressable'</span><span class="s1">))  </span><span class="s0"># type: ignore[assignment]</span>
  <span class="s1">is_fully_replicated = property(op.attrgetter(</span><span class="s5">'_base_array.is_fully_replicated'</span><span class="s1">))  </span><span class="s0"># type: ignore[assignment]</span>
  <span class="s1">delete = property(op.attrgetter(</span><span class="s5">'_base_array.delete'</span><span class="s1">))  </span><span class="s0"># type: ignore[assignment]</span>
  <span class="s1">is_deleted = property(op.attrgetter(</span><span class="s5">'_base_array.is_deleted'</span><span class="s1">))  </span><span class="s0"># type: ignore[assignment]</span>
  <span class="s1">on_device_size_in_bytes = property(op.attrgetter(</span><span class="s5">'_base_array.on_device_size_in_bytes'</span><span class="s1">))  </span><span class="s0"># type: ignore[assignment]</span>
  <span class="s1">unsafe_buffer_pointer = property(op.attrgetter(</span><span class="s5">'_base_array.unsafe_buffer_pointer'</span><span class="s1">))  </span><span class="s0"># type: ignore[assignment]</span>

  <span class="s2">def </span><span class="s1">addressable_data(self</span><span class="s2">, </span><span class="s1">index: int) -&gt; PRNGKeyArrayImpl:</span>
    <span class="s2">return </span><span class="s1">PRNGKeyArrayImpl(self.impl</span><span class="s2">, </span><span class="s1">self._base_array.addressable_data(index))</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">addressable_shards(self) -&gt; List[Shard]:</span>
    <span class="s2">return </span><span class="s1">[</span>
        <span class="s1">type(s)(</span>
            <span class="s1">device=s._device</span><span class="s2">,</span>
            <span class="s1">sharding=s._sharding</span><span class="s2">,</span>
            <span class="s1">global_shape=s._global_shape</span><span class="s2">,</span>
            <span class="s1">data=PRNGKeyArrayImpl(self.impl</span><span class="s2">, </span><span class="s1">s._data)</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">self._base_array.addressable_shards</span>
    <span class="s1">]</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">global_shards(self) -&gt; List[Shard]:</span>
    <span class="s2">return </span><span class="s1">[</span>
        <span class="s1">type(s)(</span>
            <span class="s1">device=s._device</span><span class="s2">,</span>
            <span class="s1">sharding=s._sharding</span><span class="s2">,</span>
            <span class="s1">global_shape=s._global_shape</span><span class="s2">,</span>
            <span class="s1">data=PRNGKeyArrayImpl(self.impl</span><span class="s2">, </span><span class="s1">s._data)</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">self._base_array.global_shards</span>
    <span class="s1">]</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">sharding(self):</span>
    <span class="s1">phys_sharding = self._base_array.sharding</span>
    <span class="s2">return </span><span class="s1">KeyTyRules.logical_op_sharding(self.aval</span><span class="s2">, </span><span class="s1">phys_sharding)</span>

  <span class="s2">def </span><span class="s1">_is_scalar(self):</span>
    <span class="s1">base_ndim = len(self.impl.key_shape)</span>
    <span class="s2">return </span><span class="s1">self._base_array.ndim == base_ndim</span>

  <span class="s2">def </span><span class="s1">__len__(self):</span>
    <span class="s2">if </span><span class="s1">self._is_scalar():</span>
      <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">'len() of unsized object'</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">len(self._base_array)</span>

  <span class="s2">def </span><span class="s1">__iter__(self) -&gt; Iterator[PRNGKeyArrayImpl]:</span>
    <span class="s2">if </span><span class="s1">self._is_scalar():</span>
      <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">'iteration over a 0-d key array'</span><span class="s1">)</span>
    <span class="s0"># TODO(frostig): we may want to avoid iteration by slicing because</span>
    <span class="s0"># a very common use of iteration is `k1, k2 = split(key)`, and</span>
    <span class="s0"># slicing/indexing may be trickier to track for linearity checking</span>
    <span class="s0"># purposes. Maybe we can:</span>
    <span class="s0"># * introduce an unpack primitive+traceable (also allow direct use)</span>
    <span class="s0"># * unpack upfront into shape[0] many keyarray slices</span>
    <span class="s0"># * return iter over these unpacked slices</span>
    <span class="s0"># Whatever we do, we'll want to do it by overriding</span>
    <span class="s0"># ShapedArray._iter when the element type is KeyTy...</span>
    <span class="s2">return </span><span class="s1">(PRNGKeyArrayImpl(self.impl</span><span class="s2">, </span><span class="s1">k) </span><span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">iter(self._base_array))</span>

  <span class="s0"># TODO(frostig): are all of the stackable methods below (reshape,</span>
  <span class="s0"># concat, broadcast_to, expand_dims), and the stackable registration,</span>
  <span class="s0"># still needed? If, with some work, none are needed, then do we want</span>
  <span class="s0"># to remove stackables altogether? This may be the only application.</span>

  <span class="s2">def </span><span class="s1">__repr__(self):</span>
    <span class="s2">return </span><span class="s1">(</span><span class="s5">f'</span><span class="s2">{</span><span class="s1">self.__class__.__name__</span><span class="s2">}</span><span class="s5">[</span><span class="s2">{</span><span class="s1">self.impl.tag</span><span class="s2">}</span><span class="s5">]'</span>
            <span class="s5">f' </span><span class="s2">{{ {</span><span class="s1">self._base_array</span><span class="s2">} }}</span><span class="s5">'</span><span class="s1">)</span>

  <span class="s2">def </span><span class="s1">pprint(self):</span>
    <span class="s1">pp_keys = pp.text(</span><span class="s5">'shape = '</span><span class="s1">) + pp.text(str(self.shape))</span>
    <span class="s1">pp_impl = pp.text(</span><span class="s5">'impl = '</span><span class="s1">) + self.impl.pprint()</span>
    <span class="s2">return </span><span class="s1">str(pp.group(</span>
      <span class="s1">pp.text(</span><span class="s5">'PRNGKeyArray:'</span><span class="s1">) +</span>
      <span class="s1">pp.nest(</span><span class="s3">2</span><span class="s2">, </span><span class="s1">pp.brk() + pp_keys + pp.brk() + pp_impl)))</span>

  <span class="s2">def </span><span class="s1">copy(self):</span>
    <span class="s2">return </span><span class="s1">self.__class__(self.impl</span><span class="s2">, </span><span class="s1">self._base_array.copy())</span>

  <span class="s1">__hash__ = </span><span class="s2">None  </span><span class="s0"># type: ignore[assignment]</span>
  <span class="s1">__array_priority__ = </span><span class="s3">100</span>

  <span class="s0"># Overwritten immediately below</span>
  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">at(self)                  -&gt; _IndexUpdateHelper: </span><span class="s2">assert False</span>
  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">T(self)                   -&gt; PRNGKeyArray: </span><span class="s2">assert False</span>
  <span class="s2">def </span><span class="s1">__getitem__(self</span><span class="s2">, </span><span class="s1">_)      -&gt; PRNGKeyArray: </span><span class="s2">assert False</span>
  <span class="s2">def </span><span class="s1">flatten(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)   -&gt; PRNGKeyArray: </span><span class="s2">assert False</span>
  <span class="s2">def </span><span class="s1">ravel(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)     -&gt; PRNGKeyArray: </span><span class="s2">assert False</span>
  <span class="s2">def </span><span class="s1">reshape(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)   -&gt; PRNGKeyArray: </span><span class="s2">assert False</span>
  <span class="s2">def </span><span class="s1">squeeze(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)   -&gt; PRNGKeyArray: </span><span class="s2">assert False</span>
  <span class="s2">def </span><span class="s1">swapaxes(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)  -&gt; PRNGKeyArray: </span><span class="s2">assert False</span>
  <span class="s2">def </span><span class="s1">take(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__)      -&gt; PRNGKeyArray: </span><span class="s2">assert False</span>
  <span class="s2">def </span><span class="s1">transpose(self</span><span class="s2">, </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">**__) -&gt; PRNGKeyArray: </span><span class="s2">assert False</span>

<span class="s1">_set_array_base_attributes(PRNGKeyArrayImpl</span><span class="s2">, </span><span class="s1">include=[</span>
    <span class="s1">*(</span><span class="s5">f&quot;__</span><span class="s2">{</span><span class="s1">op</span><span class="s2">}</span><span class="s5">__&quot; </span><span class="s2">for </span><span class="s1">op </span><span class="s2">in </span><span class="s1">_array_operators)</span><span class="s2">,</span>
    <span class="s5">'at'</span><span class="s2">, </span><span class="s5">'flatten'</span><span class="s2">, </span><span class="s5">'ravel'</span><span class="s2">, </span><span class="s5">'reshape'</span><span class="s2">,</span>
    <span class="s5">'squeeze'</span><span class="s2">, </span><span class="s5">'swapaxes'</span><span class="s2">, </span><span class="s5">'take'</span><span class="s2">, </span><span class="s5">'transpose'</span><span class="s2">, </span><span class="s5">'T'</span><span class="s1">])</span>
<span class="s1">basearray.Array.register(PRNGKeyArrayImpl)</span>

<span class="s1">ad_util.jaxval_zeros_likers[PRNGKeyArrayImpl] = jnp.zeros_like  </span><span class="s0"># type: ignore[has-type]</span>


<span class="s0"># TODO(frostig): remove, rerouting callers directly to random_seed</span>
<span class="s2">def </span><span class="s1">seed_with_impl(impl: PRNGImpl</span><span class="s2">, </span><span class="s1">seed: Union[int</span><span class="s2">, </span><span class="s1">Array]) -&gt; PRNGKeyArrayImpl:</span>
  <span class="s2">return </span><span class="s1">random_seed(seed</span><span class="s2">, </span><span class="s1">impl=impl)</span>


<span class="s2">def </span><span class="s1">keys_shaped_array(impl</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s2">return </span><span class="s1">core.ShapedArray(shape</span><span class="s2">, </span><span class="s1">KeyTy(impl))</span>

<span class="s0"># TODO(frostig): remove in favor of physical_aval call</span>
<span class="s2">def </span><span class="s1">keys_aval_to_base_arr_aval(keys_aval):</span>
  <span class="s2">return </span><span class="s1">core.physical_aval(keys_aval)</span>

<span class="s2">def </span><span class="s1">base_arr_shape_to_keys_shape(impl</span><span class="s2">, </span><span class="s1">base_arr_shape):</span>
  <span class="s1">base_ndim = len(impl.key_shape)</span>
  <span class="s2">return </span><span class="s1">base_arr_shape[:-base_ndim]</span>

<span class="s2">def </span><span class="s1">make_key_array_phys_sharding(aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, </span><span class="s1">is_sharding_from_xla):</span>
  <span class="s2">if </span><span class="s1">dispatch.is_single_device_sharding(sharding):</span>
    <span class="s2">return </span><span class="s1">sharding</span>
  <span class="s2">elif </span><span class="s1">isinstance(sharding</span><span class="s2">, </span><span class="s1">PmapSharding):</span>
    <span class="s1">key_shape = aval.dtype.impl.key_shape</span>
    <span class="s1">trailing_sharding = [sharding_specs.NoSharding()] * len(key_shape)</span>
    <span class="s1">phys_sharding_spec = sharding_specs.ShardingSpec(</span>
        <span class="s1">sharding=(*sharding.sharding_spec.sharding</span><span class="s2">, </span><span class="s1">*trailing_sharding)</span><span class="s2">,</span>
        <span class="s1">mesh_mapping=sharding.sharding_spec.mesh_mapping)</span>
    <span class="s2">return </span><span class="s1">PmapSharding(devices=sharding.devices</span><span class="s2">,</span>
                        <span class="s1">sharding_spec=phys_sharding_spec)</span>
  <span class="s2">elif </span><span class="s1">isinstance(sharding</span><span class="s2">, </span><span class="s1">NamedSharding):</span>
    <span class="s1">key_shape = aval.dtype.impl.key_shape</span>
    <span class="s1">trailing_spec = [</span><span class="s2">None</span><span class="s1">] * len(key_shape)</span>
    <span class="s2">return </span><span class="s1">NamedSharding(</span>
        <span class="s1">sharding.mesh</span><span class="s2">,</span>
        <span class="s1">PartitionSpec(*sharding.spec</span><span class="s2">, </span><span class="s1">*trailing_spec))</span>
  <span class="s2">elif </span><span class="s1">is_sharding_from_xla:</span>
    <span class="s2">return </span><span class="s1">sharding</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">hlos = sharding._to_xla_hlo_sharding(aval.ndim)</span>
    <span class="s2">return </span><span class="s1">GSPMDSharding(</span>
        <span class="s1">sharding._device_assignment</span><span class="s2">,</span>
        <span class="s1">KeyTyRules.physical_hlo_sharding(aval</span><span class="s2">, </span><span class="s1">hlos))</span>

<span class="s2">class </span><span class="s1">KeyTyRules:</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">full(shape</span><span class="s2">, </span><span class="s1">fill_value</span><span class="s2">, </span><span class="s1">dtype):</span>
    <span class="s1">physical_shape = (*shape</span><span class="s2">, </span><span class="s1">*dtype.impl.key_shape)</span>
    <span class="s2">if </span><span class="s1">isinstance(fill_value</span><span class="s2">, </span><span class="s1">PRNGKeyArray):</span>
      <span class="s1">key_data = jnp.broadcast_to(random_unwrap(fill_value)</span><span class="s2">, </span><span class="s1">physical_shape)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">key_data = lax.full(physical_shape</span><span class="s2">, </span><span class="s1">fill_value</span><span class="s2">, </span><span class="s1">dtype=np.dtype(</span><span class="s5">'uint32'</span><span class="s1">))</span>
    <span class="s0"># TODO(frostig,mattjj,vanderplas,lenamartens): consider this consumed from</span>
    <span class="s0"># the outset.</span>
    <span class="s2">return </span><span class="s1">random_wrap(key_data</span><span class="s2">, </span><span class="s1">impl=dtype.impl)</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">physical_element_aval(dtype) -&gt; core.ShapedArray:</span>
    <span class="s2">return </span><span class="s1">core.ShapedArray(dtype.impl.key_shape</span><span class="s2">, </span><span class="s1">jnp.dtype(</span><span class="s5">'uint32'</span><span class="s1">))</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">physical_const(val) -&gt; Array:</span>
    <span class="s2">return </span><span class="s1">val.unsafe_raw_array()</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">physical_hlo_sharding(aval</span><span class="s2">, </span><span class="s1">hlo_sharding: xc.HloSharding) -&gt; xc.HloSharding:</span>
    <span class="s1">key_shape = aval.dtype.impl.key_shape</span>
    <span class="s1">op_sharding_proto = hlo_sharding.to_proto()  </span><span class="s0"># type: ignore</span>
    <span class="s1">new_op_sharding = op_sharding_proto.clone()</span>
    <span class="s1">tad = list(new_op_sharding.tile_assignment_dimensions)</span>
    <span class="s1">suffix = [tad.pop()] </span><span class="s2">if </span><span class="s1">op_sharding_proto.replicate_on_last_tile_dim </span><span class="s2">else </span><span class="s1">[]</span>
    <span class="s1">tad.extend([</span><span class="s3">1</span><span class="s1">] * len(key_shape) + suffix)</span>
    <span class="s1">new_op_sharding.tile_assignment_dimensions = tad</span>
    <span class="s2">return </span><span class="s1">xc.HloSharding.from_proto(new_op_sharding)</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">logical_op_sharding(aval</span><span class="s2">, </span><span class="s1">phys_sharding) -&gt; XLACompatibleSharding:</span>
    <span class="s2">if </span><span class="s1">dispatch.is_single_device_sharding(phys_sharding):</span>
      <span class="s2">return </span><span class="s1">phys_sharding</span>
    <span class="s2">elif </span><span class="s1">isinstance(phys_sharding</span><span class="s2">, </span><span class="s1">PmapSharding):</span>
      <span class="s1">key_shape = aval.dtype.impl.key_shape</span>
      <span class="s1">logical_sharding_spec = sharding_specs.ShardingSpec(</span>
          <span class="s1">sharding=phys_sharding.sharding_spec.sharding[:-len(key_shape)]</span><span class="s2">,</span>
          <span class="s1">mesh_mapping=phys_sharding.sharding_spec.mesh_mapping)</span>
      <span class="s2">return </span><span class="s1">PmapSharding(devices=phys_sharding.devices</span><span class="s2">,</span>
                          <span class="s1">sharding_spec=logical_sharding_spec)</span>
    <span class="s2">elif </span><span class="s1">isinstance(phys_sharding</span><span class="s2">, </span><span class="s1">NamedSharding):</span>
      <span class="s1">key_shape = aval.dtype.impl.key_shape</span>
      <span class="s2">return </span><span class="s1">pxla.create_mesh_pspec_sharding(</span>
          <span class="s1">phys_sharding.mesh</span><span class="s2">,</span>
          <span class="s1">PartitionSpec(*phys_sharding.spec[:-len(key_shape)]))</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">key_shape = aval.dtype.impl.key_shape</span>
      <span class="s1">phys_op_sharding = phys_sharding._to_xla_hlo_sharding(</span>
          <span class="s1">aval.ndim + len(key_shape)).to_proto()</span>
      <span class="s1">logical_op_sharding = phys_op_sharding.clone()</span>
      <span class="s1">tad = list(logical_op_sharding.tile_assignment_dimensions)</span>
      <span class="s1">tad = tad[:-len(key_shape)]</span>
      <span class="s1">logical_op_sharding.tile_assignment_dimensions = tad</span>
      <span class="s2">return </span><span class="s1">GSPMDSharding(phys_sharding._device_assignment</span><span class="s2">,</span>
                           <span class="s1">xc.HloSharding.from_proto(logical_op_sharding))</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">result_handler(sticky_device</span><span class="s2">, </span><span class="s1">aval):</span>
    <span class="s2">def </span><span class="s1">handler(_</span><span class="s2">, </span><span class="s1">buf):</span>
      <span class="s1">buf.aval = core.ShapedArray(buf.shape</span><span class="s2">, </span><span class="s1">buf.dtype)</span>
      <span class="s2">return </span><span class="s1">PRNGKeyArrayImpl(aval.dtype.impl</span><span class="s2">, </span><span class="s1">buf)</span>
    <span class="s2">return </span><span class="s1">handler</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">local_sharded_result_handler(aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, </span><span class="s1">indices):</span>
    <span class="s1">phys_aval = core.physical_aval(aval)</span>
    <span class="s1">key_shape = aval.dtype.impl.key_shape</span>
    <span class="s1">phys_handler_maker = pxla.local_result_handlers[core.ShapedArray]</span>

    <span class="s0"># set up a grounded sharding (with a grounded sharding spec)</span>
    <span class="s2">if </span><span class="s1">isinstance(sharding</span><span class="s2">, </span><span class="s1">(PmapSharding</span><span class="s2">, </span><span class="s1">NamedSharding)):</span>
      <span class="s1">phys_sharding = make_key_array_phys_sharding(</span>
          <span class="s1">aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, </span><span class="s1">is_sharding_from_xla=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">assert False, </span><span class="s5">f'impossible sharding </span><span class="s2">{</span><span class="s1">sharding</span><span class="s2">} </span><span class="s5">in local sharded result handler'</span>

    <span class="s0"># set up grounded indices</span>
    <span class="s1">trailing_inds = [slice(</span><span class="s2">None</span><span class="s1">)] * len(key_shape)</span>
    <span class="s1">phys_indices = [(*inds</span><span class="s2">, </span><span class="s1">*trailing_inds) </span><span class="s2">for </span><span class="s1">inds </span><span class="s2">in </span><span class="s1">indices]</span>

    <span class="s0"># make a physical handler</span>
    <span class="s1">phys_handler = phys_handler_maker(phys_aval</span><span class="s2">, </span><span class="s1">phys_sharding</span><span class="s2">, </span><span class="s1">phys_indices)</span>

    <span class="s0"># set up a handler that calls the physical one and wraps back up</span>
    <span class="s2">def </span><span class="s1">handler(bufs):</span>
      <span class="s2">return </span><span class="s1">PRNGKeyArrayImpl(aval.dtype.impl</span><span class="s2">, </span><span class="s1">phys_handler(bufs))</span>

    <span class="s2">return </span><span class="s1">handler</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">global_sharded_result_handler(aval</span><span class="s2">, </span><span class="s1">out_sharding</span><span class="s2">, </span><span class="s1">committed</span><span class="s2">,</span>
                                    <span class="s1">is_out_sharding_from_xla):</span>
    <span class="s1">phys_aval = core.physical_aval(aval)</span>
    <span class="s1">phys_handler_maker = pxla.global_result_handlers[core.ShapedArray]</span>

    <span class="s1">phys_sharding = make_key_array_phys_sharding(</span>
        <span class="s1">aval</span><span class="s2">, </span><span class="s1">out_sharding</span><span class="s2">, </span><span class="s1">is_out_sharding_from_xla)</span>
    <span class="s1">phys_handler = phys_handler_maker(phys_aval</span><span class="s2">, </span><span class="s1">phys_sharding</span><span class="s2">, </span><span class="s1">committed</span><span class="s2">,</span>
                                      <span class="s1">is_out_sharding_from_xla)</span>
    <span class="s2">def </span><span class="s1">handler(bufs):</span>
      <span class="s2">return </span><span class="s1">PRNGKeyArrayImpl(aval.dtype.impl</span><span class="s2">, </span><span class="s1">phys_handler(bufs))</span>
    <span class="s2">return </span><span class="s1">handler</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">make_sharded_array(aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, </span><span class="s1">arrays</span><span class="s2">, </span><span class="s1">committed):</span>
    <span class="s1">phys_aval = core.physical_aval(aval)</span>
    <span class="s1">phys_handler_maker = pxla.global_result_handlers[core.ShapedArray]</span>
    <span class="s1">phys_arrays = [random_unwrap(arr) </span><span class="s2">for </span><span class="s1">arr </span><span class="s2">in </span><span class="s1">arrays]</span>

    <span class="s1">phys_sharding = make_key_array_phys_sharding(aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, False</span><span class="s1">)</span>
    <span class="s1">phys_handler = phys_handler_maker(phys_aval</span><span class="s2">, </span><span class="s1">phys_sharding</span><span class="s2">, </span><span class="s1">committed</span><span class="s2">, False</span><span class="s1">)</span>
    <span class="s1">phys_result = phys_handler(phys_arrays)</span>
    <span class="s2">return </span><span class="s1">PRNGKeyArrayImpl(aval.dtype.impl</span><span class="s2">, </span><span class="s1">phys_result)</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">device_put_sharded(vals</span><span class="s2">, </span><span class="s1">aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, </span><span class="s1">devices):</span>
    <span class="s1">physical_aval = keys_aval_to_base_arr_aval(aval)</span>
    <span class="s1">physical_buffers = tree_util.tree_map(random_unwrap</span><span class="s2">, </span><span class="s1">vals)</span>
    <span class="s1">physical_sharding = make_key_array_phys_sharding(aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, False</span><span class="s1">)</span>
    <span class="s1">physical_result = pxla.batched_device_put(physical_aval</span><span class="s2">, </span><span class="s1">physical_sharding</span><span class="s2">, </span><span class="s1">physical_buffers</span><span class="s2">, </span><span class="s1">list(devices))</span>
    <span class="s2">return </span><span class="s1">random_wrap(physical_result</span><span class="s2">, </span><span class="s1">impl=aval.dtype.impl)</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">device_put_replicated(val</span><span class="s2">, </span><span class="s1">aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, </span><span class="s1">devices):</span>
    <span class="s1">physical_aval = keys_aval_to_base_arr_aval(aval)</span>
    <span class="s2">assert </span><span class="s1">len(xla.aval_to_xla_shapes(physical_aval)) == </span><span class="s3">1</span>
    <span class="s1">physical_buf = random_unwrap(val)</span>
    <span class="s1">physical_sharding = make_key_array_phys_sharding(aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, False</span><span class="s1">)</span>
    <span class="s1">physical_result = pxla.batched_device_put(physical_aval</span><span class="s2">, </span><span class="s1">physical_sharding</span><span class="s2">, </span><span class="s1">[physical_buf] * len(devices)</span><span class="s2">, </span><span class="s1">devices)</span>
    <span class="s2">return </span><span class="s1">random_wrap(physical_result</span><span class="s2">, </span><span class="s1">impl=aval.dtype.impl)</span>


<span class="s2">class </span><span class="s1">KeyTy:</span>
  <span class="s1">impl: Hashable  </span><span class="s0"># prng.PRNGImpl. TODO(mattjj,frostig): protocol really</span>
  <span class="s1">_rules = KeyTyRules</span>

  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">impl):</span>
    <span class="s1">self.impl = impl</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">name(self) -&gt; str:</span>
    <span class="s2">return </span><span class="s5">f'key&lt;</span><span class="s2">{</span><span class="s1">self.impl.tag</span><span class="s2">}</span><span class="s5">&gt;'</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">itemsize(self) -&gt; int:</span>
    <span class="s2">return </span><span class="s1">math.prod(self.impl.key_shape) * np.dtype(</span><span class="s5">'uint32'</span><span class="s1">).itemsize</span>

  <span class="s2">def </span><span class="s1">__repr__(self) -&gt; str:</span>
    <span class="s2">return </span><span class="s1">self.name</span>

  <span class="s2">def </span><span class="s1">__eq__(self</span><span class="s2">, </span><span class="s1">other):</span>
    <span class="s2">return </span><span class="s1">type(other) </span><span class="s2">is </span><span class="s1">KeyTy </span><span class="s2">and </span><span class="s1">self.impl == other.impl</span>

  <span class="s2">def </span><span class="s1">__hash__(self) -&gt; int:</span>
    <span class="s2">return </span><span class="s1">hash((self.__class__</span><span class="s2">, </span><span class="s1">self.impl))</span>


<span class="s1">dtypes.opaque_dtypes.add(KeyTy)</span>


<span class="s1">core.pytype_aval_mappings[PRNGKeyArrayImpl] = </span><span class="s2">lambda </span><span class="s1">x: x.aval</span>
<span class="s1">xla.pytype_aval_mappings[PRNGKeyArrayImpl] = </span><span class="s2">lambda </span><span class="s1">x: x.aval</span>

<span class="s1">xla.canonicalize_dtype_handlers[PRNGKeyArrayImpl] = </span><span class="s2">lambda </span><span class="s1">x: x</span>


<span class="s2">def </span><span class="s1">key_array_shard_arg_handler(x: PRNGKeyArrayImpl</span><span class="s2">, </span><span class="s1">devices</span><span class="s2">, </span><span class="s1">indices</span><span class="s2">, </span><span class="s1">sharding):</span>
  <span class="s0"># TODO(frostig): Remove the need for `core.get_aval`.</span>
  <span class="s1">aval = core.get_aval(x)</span>
  <span class="s1">key_shape = aval.dtype.impl.key_shape</span>
  <span class="s1">arr = x.unsafe_raw_array()</span>

  <span class="s0"># TODO(yashkatariya,frostig): This assumes that the last dimensions are not</span>
  <span class="s0"># sharded. This is only true when enable_custom_prng is True.</span>
  <span class="s1">trailing_inds = [slice(</span><span class="s2">None</span><span class="s1">)] * len(key_shape)</span>
  <span class="s1">phys_indices = [(*inds</span><span class="s2">, </span><span class="s1">*trailing_inds) </span><span class="s2">for </span><span class="s1">inds </span><span class="s2">in </span><span class="s1">indices]</span>
  <span class="s1">phys_sharding = make_key_array_phys_sharding(</span>
      <span class="s1">aval</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, </span><span class="s1">is_sharding_from_xla=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">pxla.shard_arg_handlers[type(arr)](</span>
      <span class="s1">arr</span><span class="s2">, </span><span class="s1">devices</span><span class="s2">, </span><span class="s1">phys_indices</span><span class="s2">, </span><span class="s1">phys_sharding</span>
  <span class="s1">)</span>


<span class="s1">pxla.shard_arg_handlers[PRNGKeyArrayImpl] = key_array_shard_arg_handler</span>


<span class="s2">def </span><span class="s1">key_array_constant_handler(x</span><span class="s2">, </span><span class="s1">canonicalize_dtypes):</span>
  <span class="s1">arr = x.unsafe_raw_array()</span>
  <span class="s2">return </span><span class="s1">mlir.get_constant_handler(type(arr))(arr</span><span class="s2">, </span><span class="s1">canonicalize_dtypes)</span>
<span class="s1">mlir.register_constant_handler(PRNGKeyArrayImpl</span><span class="s2">, </span><span class="s1">key_array_constant_handler)</span>


<span class="s0"># -- primitives</span>

<span class="s2">def </span><span class="s1">iterated_vmap_unary(n</span><span class="s2">, </span><span class="s1">f):</span>
  <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(n):</span>
    <span class="s1">f = api.vmap(f)</span>
  <span class="s2">return </span><span class="s1">f</span>

<span class="s0"># TODO(frostig): Revise the following two functions? These basically</span>
<span class="s0"># undo the singleton dimensions added by `batching.defbroadcasting`.</span>
<span class="s0"># It works, but introduces some possibly-redundant squeezes. Can we</span>
<span class="s0"># borrow from other broadcasting primitives instead?</span>

<span class="s2">def </span><span class="s1">squeeze_vmap(f</span><span class="s2">, </span><span class="s1">left):</span>
  <span class="s2">def </span><span class="s1">squeeze_vmap_f(x</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s2">if </span><span class="s1">left:</span>
      <span class="s1">x = jnp.squeeze(x</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
      <span class="s1">axes = (</span><span class="s2">None, </span><span class="s3">0</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">y = jnp.squeeze(y</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
      <span class="s1">axes = (</span><span class="s3">0</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">api.vmap(f</span><span class="s2">, </span><span class="s1">in_axes=axes</span><span class="s2">, </span><span class="s1">out_axes=</span><span class="s3">0</span><span class="s1">)(x</span><span class="s2">, </span><span class="s1">y)</span>
  <span class="s2">return </span><span class="s1">squeeze_vmap_f</span>

<span class="s2">def </span><span class="s1">iterated_vmap_binary_bcast(shape1</span><span class="s2">, </span><span class="s1">shape2</span><span class="s2">, </span><span class="s1">f):</span>
  <span class="s1">ndim1</span><span class="s2">, </span><span class="s1">ndim2 = len(shape1)</span><span class="s2">, </span><span class="s1">len(shape2)</span>
  <span class="s2">if </span><span class="s1">ndim1 == ndim2 == </span><span class="s3">0</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">f</span>
  <span class="s2">if </span><span class="s3">0 </span><span class="s2">in </span><span class="s1">[ndim1</span><span class="s2">, </span><span class="s1">ndim2]:</span>
    <span class="s2">if </span><span class="s1">ndim1 == </span><span class="s3">0</span><span class="s1">:</span>
      <span class="s2">return lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: iterated_vmap_unary(ndim2</span><span class="s2">, lambda </span><span class="s1">y: f(x</span><span class="s2">, </span><span class="s1">y))(y)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: iterated_vmap_unary(ndim1</span><span class="s2">, lambda </span><span class="s1">x: f(x</span><span class="s2">, </span><span class="s1">y))(x)</span>
  <span class="s2">assert </span><span class="s1">len(shape1) == len(shape2)</span>
  <span class="s2">for </span><span class="s1">sz1</span><span class="s2">, </span><span class="s1">sz2 </span><span class="s2">in </span><span class="s1">reversed(zip(shape1</span><span class="s2">, </span><span class="s1">shape2)):</span>
    <span class="s2">if </span><span class="s1">sz1 == sz2:</span>
      <span class="s1">f = api.vmap(f</span><span class="s2">, </span><span class="s1">out_axes=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">assert </span><span class="s1">sz1 == </span><span class="s3">1 </span><span class="s2">or </span><span class="s1">sz2 == </span><span class="s3">1</span><span class="s2">, </span><span class="s1">(sz1</span><span class="s2">, </span><span class="s1">sz2)</span>
      <span class="s1">f = squeeze_vmap(f</span><span class="s2">, </span><span class="s1">sz1 == </span><span class="s3">1</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">f</span>


<span class="s2">def </span><span class="s1">random_seed(seeds</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s0"># Avoid overflow error in X32 mode by first converting ints to int64.</span>
  <span class="s0"># This breaks JIT invariance for large ints, but supports the common</span>
  <span class="s0"># use-case of instantiating with Python hashes in X32 mode.</span>
  <span class="s2">if </span><span class="s1">isinstance(seeds</span><span class="s2">, </span><span class="s1">int):</span>
    <span class="s1">seeds_arr = jnp.asarray(np.int64(seeds))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">seeds_arr = jnp.asarray(seeds)</span>
  <span class="s2">return </span><span class="s1">random_seed_p.bind(seeds_arr</span><span class="s2">, </span><span class="s1">impl=impl)</span>

<span class="s1">random_seed_p = core.Primitive(</span><span class="s5">'random_seed'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(random_seed_p)</span>
<span class="s1">batching.defvectorized(random_seed_p)</span>

<span class="s1">@random_seed_p.def_abstract_eval</span>
<span class="s2">def </span><span class="s1">random_seed_abstract_eval(seeds_aval</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s2">return </span><span class="s1">keys_shaped_array(impl</span><span class="s2">, </span><span class="s1">seeds_aval.shape)</span>

<span class="s1">@random_seed_p.def_impl</span>
<span class="s2">def </span><span class="s1">random_seed_impl(seeds</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s1">base_arr = random_seed_impl_base(seeds</span><span class="s2">, </span><span class="s1">impl=impl)</span>
  <span class="s2">return </span><span class="s1">PRNGKeyArrayImpl(impl</span><span class="s2">, </span><span class="s1">base_arr)</span>

<span class="s2">def </span><span class="s1">random_seed_impl_base(seeds</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s1">seed = iterated_vmap_unary(seeds.ndim</span><span class="s2">, </span><span class="s1">impl.seed)</span>
  <span class="s2">return </span><span class="s1">seed(seeds)</span>

<span class="s2">def </span><span class="s1">random_seed_lowering(ctx</span><span class="s2">, </span><span class="s1">seeds</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s1">aval</span><span class="s2">, </span><span class="s1">= ctx.avals_in</span>
  <span class="s1">seed = iterated_vmap_unary(aval.ndim</span><span class="s2">, </span><span class="s1">impl.seed)</span>
  <span class="s1">seed_lowering = mlir.lower_fun(seed</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">mlir.delegate_lowering(</span>
      <span class="s1">ctx</span><span class="s2">, </span><span class="s1">seed_lowering</span><span class="s2">, </span><span class="s1">seeds</span><span class="s2">,</span>
      <span class="s1">avals_out=map(keys_aval_to_base_arr_aval</span><span class="s2">, </span><span class="s1">ctx.avals_out))</span>

<span class="s1">mlir.register_lowering(random_seed_p</span><span class="s2">, </span><span class="s1">random_seed_lowering)</span>


<span class="s2">def </span><span class="s1">random_split(keys</span><span class="s2">, </span><span class="s1">count):</span>
  <span class="s2">return </span><span class="s1">random_split_p.bind(keys</span><span class="s2">, </span><span class="s1">count=count)</span>

<span class="s1">random_split_p = core.Primitive(</span><span class="s5">'random_split'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(random_split_p)</span>
<span class="s1">batching.defvectorized(random_split_p)</span>

<span class="s1">@random_split_p.def_abstract_eval</span>
<span class="s2">def </span><span class="s1">random_split_abstract_eval(keys_aval</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">count):</span>
  <span class="s2">return </span><span class="s1">keys_shaped_array(keys_aval.dtype.impl</span><span class="s2">, </span><span class="s1">(*keys_aval.shape</span><span class="s2">, </span><span class="s1">count))</span>

<span class="s1">@random_split_p.def_impl</span>
<span class="s2">def </span><span class="s1">random_split_impl(keys</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">count):</span>
  <span class="s1">base_arr = random_split_impl_base(</span>
      <span class="s1">keys.impl</span><span class="s2">, </span><span class="s1">keys.unsafe_raw_array()</span><span class="s2">, </span><span class="s1">keys.ndim</span><span class="s2">, </span><span class="s1">count=count)</span>
  <span class="s2">return </span><span class="s1">PRNGKeyArrayImpl(keys.impl</span><span class="s2">, </span><span class="s1">base_arr)</span>

<span class="s2">def </span><span class="s1">random_split_impl_base(impl</span><span class="s2">, </span><span class="s1">base_arr</span><span class="s2">, </span><span class="s1">keys_ndim</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">count):</span>
  <span class="s1">split = iterated_vmap_unary(keys_ndim</span><span class="s2">, lambda </span><span class="s1">k: impl.split(k</span><span class="s2">, </span><span class="s1">count))</span>
  <span class="s2">return </span><span class="s1">split(base_arr)</span>

<span class="s2">def </span><span class="s1">random_split_lowering(ctx</span><span class="s2">, </span><span class="s1">keys</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">count):</span>
  <span class="s1">aval</span><span class="s2">, </span><span class="s1">= ctx.avals_in</span>
  <span class="s1">impl = aval.dtype.impl</span>
  <span class="s1">split = iterated_vmap_unary(aval.ndim</span><span class="s2">, lambda </span><span class="s1">k: impl.split(k</span><span class="s2">, </span><span class="s1">count))</span>
  <span class="s1">split_lowering = mlir.lower_fun(split</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">mlir.delegate_lowering(</span>
      <span class="s1">ctx</span><span class="s2">, </span><span class="s1">split_lowering</span><span class="s2">, </span><span class="s1">keys</span><span class="s2">,</span>
      <span class="s1">avals_in=[keys_aval_to_base_arr_aval(aval)]</span><span class="s2">,</span>
      <span class="s1">avals_out=map(keys_aval_to_base_arr_aval</span><span class="s2">, </span><span class="s1">ctx.avals_out))</span>

<span class="s1">mlir.register_lowering(random_split_p</span><span class="s2">, </span><span class="s1">random_split_lowering)</span>


<span class="s2">def </span><span class="s1">random_fold_in(keys</span><span class="s2">, </span><span class="s1">msgs):</span>
  <span class="s2">return </span><span class="s1">random_fold_in_p.bind(keys</span><span class="s2">, </span><span class="s1">jnp.asarray(msgs))</span>

<span class="s1">random_fold_in_p = core.Primitive(</span><span class="s5">'random_fold_in'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(random_fold_in_p)</span>
<span class="s1">batching.defbroadcasting(random_fold_in_p)</span>

<span class="s1">@random_fold_in_p.def_abstract_eval</span>
<span class="s2">def </span><span class="s1">random_fold_in_abstract_eval(keys_aval</span><span class="s2">, </span><span class="s1">msgs_aval):</span>
  <span class="s1">shape = lax_internal.broadcasting_shape_rule(</span>
      <span class="s5">'random_fold_in'</span><span class="s2">, </span><span class="s1">keys_aval</span><span class="s2">, </span><span class="s1">msgs_aval)</span>
  <span class="s1">named_shape = lax_utils.standard_named_shape_rule(keys_aval</span><span class="s2">, </span><span class="s1">msgs_aval)</span>
  <span class="s2">return </span><span class="s1">core.ShapedArray(shape</span><span class="s2">, </span><span class="s1">keys_aval.dtype</span><span class="s2">, </span><span class="s1">named_shape=named_shape)</span>

<span class="s1">@random_fold_in_p.def_impl</span>
<span class="s2">def </span><span class="s1">random_fold_in_impl(keys</span><span class="s2">, </span><span class="s1">msgs):</span>
  <span class="s1">base_arr = random_fold_in_impl_base(</span>
      <span class="s1">keys.impl</span><span class="s2">, </span><span class="s1">keys.unsafe_raw_array()</span><span class="s2">, </span><span class="s1">msgs</span><span class="s2">, </span><span class="s1">keys.shape)</span>
  <span class="s2">return </span><span class="s1">PRNGKeyArrayImpl(keys.impl</span><span class="s2">, </span><span class="s1">base_arr)</span>

<span class="s2">def </span><span class="s1">random_fold_in_impl_base(impl</span><span class="s2">, </span><span class="s1">base_arr</span><span class="s2">, </span><span class="s1">msgs</span><span class="s2">, </span><span class="s1">keys_shape):</span>
  <span class="s1">fold_in = iterated_vmap_binary_bcast(</span>
      <span class="s1">keys_shape</span><span class="s2">, </span><span class="s1">np.shape(msgs)</span><span class="s2">, </span><span class="s1">impl.fold_in)</span>
  <span class="s2">return </span><span class="s1">fold_in(base_arr</span><span class="s2">, </span><span class="s1">msgs)</span>

<span class="s2">def </span><span class="s1">random_fold_in_lowering(ctx</span><span class="s2">, </span><span class="s1">keys</span><span class="s2">, </span><span class="s1">msgs):</span>
  <span class="s1">keys_aval</span><span class="s2">, </span><span class="s1">msgs_aval = ctx.avals_in</span>
  <span class="s1">impl = keys_aval.dtype.impl</span>
  <span class="s1">fold_in = iterated_vmap_binary_bcast(</span>
      <span class="s1">keys_aval.shape</span><span class="s2">, </span><span class="s1">msgs_aval.shape</span><span class="s2">, </span><span class="s1">impl.fold_in)</span>
  <span class="s1">fold_in_lowering = mlir.lower_fun(fold_in</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">mlir.delegate_lowering(</span>
      <span class="s1">ctx</span><span class="s2">, </span><span class="s1">fold_in_lowering</span><span class="s2">, </span><span class="s1">keys</span><span class="s2">, </span><span class="s1">msgs</span><span class="s2">,</span>
      <span class="s1">avals_in=[keys_aval_to_base_arr_aval(keys_aval)</span><span class="s2">, </span><span class="s1">msgs_aval]</span><span class="s2">,</span>
      <span class="s1">avals_out=map(keys_aval_to_base_arr_aval</span><span class="s2">, </span><span class="s1">ctx.avals_out))</span>

<span class="s1">mlir.register_lowering(random_fold_in_p</span><span class="s2">, </span><span class="s1">random_fold_in_lowering)</span>


<span class="s2">def </span><span class="s1">random_bits(keys</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s1">shape = core.as_named_shape(shape)</span>
  <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">size </span><span class="s2">in </span><span class="s1">shape.named_items:</span>
    <span class="s0"># TODO(frostig,mattjj,apaszke): Is this real_size check necessary,</span>
    <span class="s0"># and is it meant to raise a user-facing ValueError? Should it be</span>
    <span class="s0"># an `assert` (or RuntimeError) instead? Why do we check it in</span>
    <span class="s0"># calls to `random_bits` instead of a more common paralleism path?</span>
    <span class="s1">real_size = lax.psum(</span><span class="s3">1</span><span class="s2">, </span><span class="s1">name)</span>
    <span class="s2">if </span><span class="s1">real_size != size:</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;The shape of axis </span><span class="s2">{</span><span class="s1">name</span><span class="s2">} </span><span class="s5">was specified as </span><span class="s2">{</span><span class="s1">size</span><span class="s2">}</span><span class="s5">, &quot;</span>
                       <span class="s5">f&quot;but it really is </span><span class="s2">{</span><span class="s1">real_size</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s1">)</span>
    <span class="s1">axis_index = lax.axis_index(name)</span>
    <span class="s1">keys = random_fold_in(keys</span><span class="s2">, </span><span class="s1">axis_index)</span>
  <span class="s2">return </span><span class="s1">random_bits_p.bind(keys</span><span class="s2">, </span><span class="s1">bit_width=bit_width</span><span class="s2">, </span><span class="s1">shape=shape.positional)</span>

<span class="s1">random_bits_p = core.Primitive(</span><span class="s5">'random_bits'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(random_bits_p)</span>
<span class="s1">batching.defvectorized(random_bits_p)</span>

<span class="s1">@random_bits_p.def_abstract_eval</span>
<span class="s2">def </span><span class="s1">random_bits_abstract_eval(keys_aval</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s1">out_shape = (*keys_aval.shape</span><span class="s2">, </span><span class="s1">*shape)</span>
  <span class="s1">out_dtype = dtypes.dtype(</span><span class="s5">f'uint</span><span class="s2">{</span><span class="s1">bit_width</span><span class="s2">}</span><span class="s5">'</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">core.ShapedArray(out_shape</span><span class="s2">, </span><span class="s1">out_dtype)</span>

<span class="s1">@random_bits_p.def_impl</span>
<span class="s2">def </span><span class="s1">random_bits_impl(keys</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s2">return </span><span class="s1">random_bits_impl_base(keys.impl</span><span class="s2">, </span><span class="s1">keys.unsafe_raw_array()</span><span class="s2">, </span><span class="s1">keys.ndim</span><span class="s2">,</span>
                               <span class="s1">bit_width=bit_width</span><span class="s2">, </span><span class="s1">shape=shape)</span>

<span class="s2">def </span><span class="s1">random_bits_impl_base(impl</span><span class="s2">, </span><span class="s1">base_arr</span><span class="s2">, </span><span class="s1">keys_ndim</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s1">bits = iterated_vmap_unary(</span>
      <span class="s1">keys_ndim</span><span class="s2">, lambda </span><span class="s1">k: impl.random_bits(k</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape))</span>
  <span class="s2">return </span><span class="s1">bits(base_arr)</span>

<span class="s2">def </span><span class="s1">random_bits_lowering(ctx</span><span class="s2">, </span><span class="s1">keys</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s1">aval</span><span class="s2">, </span><span class="s1">= ctx.avals_in</span>
  <span class="s1">impl = aval.dtype.impl</span>
  <span class="s1">bits = iterated_vmap_unary(</span>
      <span class="s1">aval.ndim</span><span class="s2">, lambda </span><span class="s1">k: impl.random_bits(k</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape))</span>
  <span class="s1">bits_lowering = mlir.lower_fun(bits</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s1">ctx_new = ctx.replace(avals_in=[keys_aval_to_base_arr_aval(aval)])</span>
  <span class="s1">out = bits_lowering(ctx_new</span><span class="s2">, </span><span class="s1">keys)</span>
  <span class="s1">ctx.set_tokens_out(ctx_new.tokens_out)</span>
  <span class="s2">return </span><span class="s1">out</span>

<span class="s1">mlir.register_lowering(random_bits_p</span><span class="s2">, </span><span class="s1">random_bits_lowering)</span>


<span class="s0"># The following wrap/unwrap primitives are at least a stopgap for</span>
<span class="s0"># backwards compatibility, namely when `config.jax_enable_custom_prng`</span>
<span class="s0"># is False. We need to convert key arrays to and from underlying</span>
<span class="s0"># uint32 base array, and we may need to do so under a jit. For</span>
<span class="s0"># example, we want to support:</span>
<span class="s0">#</span>
<span class="s0">#   keys = jax.jit(random.split)(key)</span>
<span class="s0">#</span>
<span class="s0"># where `key` and `keys` are both acceptably old-style uint32 arrays</span>
<span class="s0"># so long as enable_custom_prng is False. The way we handle this is</span>
<span class="s0"># that `random.split` adapts the input/output by converting to/from</span>
<span class="s0"># key arrays across its call to `random_split`. So we rely on these</span>
<span class="s0"># wrap/unwrap casting primitives to allow that conversion under jit.</span>
<span class="s0">#</span>
<span class="s0"># We may want to keep both around for testing and debugging escape</span>
<span class="s0"># hatches. We can rename them `unsafe` for emphasis, and/or issue a</span>
<span class="s0"># warning on entry to the traceable.</span>
<span class="s0">#</span>
<span class="s0"># TODO(frostig): Consider removal once we always enable_custom_prng.</span>

<span class="s2">def </span><span class="s1">random_wrap(base_arr</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s1">_check_prng_key_data(impl</span><span class="s2">, </span><span class="s1">base_arr)</span>
  <span class="s2">return </span><span class="s1">random_wrap_p.bind(base_arr</span><span class="s2">, </span><span class="s1">impl=impl)</span>

<span class="s1">random_wrap_p = core.Primitive(</span><span class="s5">'random_wrap'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(random_wrap_p)</span>

<span class="s1">@random_wrap_p.def_abstract_eval</span>
<span class="s2">def </span><span class="s1">random_wrap_abstract_eval(base_arr_aval</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s1">shape = base_arr_shape_to_keys_shape(impl</span><span class="s2">, </span><span class="s1">base_arr_aval.shape)</span>
  <span class="s2">return </span><span class="s1">keys_shaped_array(impl</span><span class="s2">, </span><span class="s1">shape)</span>

<span class="s1">@random_wrap_p.def_impl</span>
<span class="s2">def </span><span class="s1">random_wrap_impl(base_arr</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s2">return </span><span class="s1">PRNGKeyArrayImpl(impl</span><span class="s2">, </span><span class="s1">base_arr)</span>

<span class="s2">def </span><span class="s1">random_wrap_lowering(ctx</span><span class="s2">, </span><span class="s1">base_arr</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s2">return </span><span class="s1">[base_arr]</span>

<span class="s2">def </span><span class="s1">random_wrap_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">impl):</span>
  <span class="s1">x</span><span class="s2">, </span><span class="s1">= batched_args</span>
  <span class="s1">d</span><span class="s2">, </span><span class="s1">= batch_dims</span>
  <span class="s1">x = batching.bdim_at_front(x</span><span class="s2">, </span><span class="s1">d</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">random_wrap(x</span><span class="s2">, </span><span class="s1">impl=impl)</span><span class="s2">, </span><span class="s3">0</span>

<span class="s1">mlir.register_lowering(random_wrap_p</span><span class="s2">, </span><span class="s1">random_wrap_lowering)</span>
<span class="s1">batching.primitive_batchers[random_wrap_p] = random_wrap_batch_rule</span>


<span class="s2">def </span><span class="s1">random_unwrap(keys):</span>
  <span class="s2">if not </span><span class="s1">isinstance(keys</span><span class="s2">, </span><span class="s1">PRNGKeyArrayImpl):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">f'random_unwrap takes key array operand, got </span><span class="s2">{</span><span class="s1">type(keys)</span><span class="s2">}</span><span class="s5">'</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">random_unwrap_p.bind(keys)</span>

<span class="s1">random_unwrap_p = core.Primitive(</span><span class="s5">'random_unwrap'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(random_unwrap_p)</span>
<span class="s1">batching.defvectorized(random_unwrap_p)</span>

<span class="s1">@random_unwrap_p.def_abstract_eval</span>
<span class="s2">def </span><span class="s1">random_unwrap_abstract_eval(keys_aval):</span>
  <span class="s2">return </span><span class="s1">keys_aval_to_base_arr_aval(keys_aval)</span>

<span class="s1">@random_unwrap_p.def_impl</span>
<span class="s2">def </span><span class="s1">random_unwrap_impl(keys):</span>
  <span class="s2">return </span><span class="s1">keys.unsafe_raw_array()</span>

<span class="s2">def </span><span class="s1">random_unwrap_lowering(ctx</span><span class="s2">, </span><span class="s1">keys):</span>
  <span class="s2">return </span><span class="s1">[keys]</span>

<span class="s1">mlir.register_lowering(random_unwrap_p</span><span class="s2">, </span><span class="s1">random_unwrap_lowering)</span>


<span class="s0"># -- threefry2x32 PRNG implementation</span>


<span class="s2">def </span><span class="s1">_is_threefry_prng_key(key: typing.Array) -&gt; bool:</span>
  <span class="s2">try</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">key.shape == (</span><span class="s3">2</span><span class="s2">,</span><span class="s1">) </span><span class="s2">and </span><span class="s1">key.dtype == np.uint32</span>
  <span class="s2">except </span><span class="s1">AttributeError:</span>
    <span class="s2">return False</span>


<span class="s2">def </span><span class="s1">threefry_seed(seed: typing.Array) -&gt; typing.Array:</span>
  <span class="s4">&quot;&quot;&quot;Create a single raw threefry PRNG key from an integer seed. 
 
  Args: 
    seed: a 64- or 32-bit integer used as the value of the key. 
 
  Returns: 
    The PRNG key contents, modeled as an array of shape (2,) and dtype 
    uint32. The key is constructed from a 64-bit seed by effectively 
    bit-casting to a pair of uint32 values (or from a 32-bit seed by 
    first padding out with zeros). 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">_threefry_seed(seed)</span>

<span class="s1">@partial(jit</span><span class="s2">, </span><span class="s1">inline=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">_threefry_seed(seed: typing.Array) -&gt; typing.Array:</span>
  <span class="s2">if </span><span class="s1">seed.shape:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">f&quot;PRNG key seed must be a scalar; got </span><span class="s2">{</span><span class="s1">seed</span><span class="s2">!r}</span><span class="s5">.&quot;</span><span class="s1">)</span>
  <span class="s2">if not </span><span class="s1">np.issubdtype(seed.dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">f&quot;PRNG key seed must be an integer; got </span><span class="s2">{</span><span class="s1">seed</span><span class="s2">!r}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s1">convert = </span><span class="s2">lambda </span><span class="s1">k: lax.reshape(lax.convert_element_type(k</span><span class="s2">, </span><span class="s1">np.uint32)</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">])</span>
  <span class="s1">k1 = convert(</span>
      <span class="s1">lax.shift_right_logical(seed</span><span class="s2">, </span><span class="s1">lax_internal._const(seed</span><span class="s2">, </span><span class="s3">32</span><span class="s1">)))</span>
  <span class="s2">with </span><span class="s1">config_lib.numpy_dtype_promotion(</span><span class="s5">'standard'</span><span class="s1">):</span>
    <span class="s0"># TODO(jakevdp): in X64 mode, this can generate 64-bit computations for 32-bit</span>
    <span class="s0"># inputs. We should avoid this.</span>
    <span class="s1">k2 = convert(jnp.bitwise_and(seed</span><span class="s2">, </span><span class="s1">np.uint32(</span><span class="s3">0xFFFFFFFF</span><span class="s1">)))</span>
  <span class="s2">return </span><span class="s1">lax.concatenate([k1</span><span class="s2">, </span><span class="s1">k2]</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_make_rotate_left(dtype):</span>
  <span class="s2">if not </span><span class="s1">jnp.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;_rotate_left only accepts integer dtypes.&quot;</span><span class="s1">)</span>
  <span class="s1">nbits = np.array(jnp.iinfo(dtype).bits</span><span class="s2">, </span><span class="s1">dtype)</span>

  <span class="s2">def </span><span class="s1">_rotate_left(x</span><span class="s2">, </span><span class="s1">d):</span>
    <span class="s2">if </span><span class="s1">lax.dtype(d) != dtype:</span>
      <span class="s1">d = lax.convert_element_type(d</span><span class="s2">, </span><span class="s1">dtype)</span>
    <span class="s2">if </span><span class="s1">lax.dtype(x) != dtype:</span>
      <span class="s1">x = lax.convert_element_type(x</span><span class="s2">, </span><span class="s1">dtype)</span>
    <span class="s2">return </span><span class="s1">lax.shift_left(x</span><span class="s2">, </span><span class="s1">d) | lax.shift_right_logical(x</span><span class="s2">, </span><span class="s1">nbits - d)</span>
  <span class="s2">return </span><span class="s1">_rotate_left</span>


<span class="s0">### hash function and split</span>

<span class="s2">def </span><span class="s1">_threefry2x32_abstract_eval(*args):</span>
  <span class="s2">if </span><span class="s1">any(a.dtype != jnp.uint32 </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">args):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;Arguments to threefry2x32 must have uint32 type, got {}&quot;</span>
                    <span class="s1">.format(args))</span>
  <span class="s2">if </span><span class="s1">all(isinstance(arg</span><span class="s2">, </span><span class="s1">core.ShapedArray) </span><span class="s2">for </span><span class="s1">arg </span><span class="s2">in </span><span class="s1">args):</span>
    <span class="s1">shape = lax_internal.broadcasting_shape_rule(*args)</span>
    <span class="s1">named_shape = core.join_named_shapes(*(a.named_shape </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">args))</span>
    <span class="s1">aval = core.ShapedArray(shape</span><span class="s2">, </span><span class="s1">jnp.dtype(jnp.uint32)</span><span class="s2">, </span><span class="s1">named_shape=named_shape)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">aval = core.UnshapedArray(jnp.dtype(jnp.uint32))</span>
  <span class="s2">return </span><span class="s1">(aval</span><span class="s2">,</span><span class="s1">) * </span><span class="s3">2</span>


<span class="s1">rotate_left = _make_rotate_left(np.uint32)</span>


<span class="s2">def </span><span class="s1">apply_round(v</span><span class="s2">, </span><span class="s1">rot):</span>
  <span class="s1">v = v[:]</span>
  <span class="s1">v[</span><span class="s3">0</span><span class="s1">] = v[</span><span class="s3">0</span><span class="s1">] + v[</span><span class="s3">1</span><span class="s1">]</span>
  <span class="s1">v[</span><span class="s3">1</span><span class="s1">] = rotate_left(v[</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rot)</span>
  <span class="s1">v[</span><span class="s3">1</span><span class="s1">] = v[</span><span class="s3">0</span><span class="s1">] ^ v[</span><span class="s3">1</span><span class="s1">]</span>
  <span class="s2">return </span><span class="s1">v</span>


<span class="s2">def </span><span class="s1">rotate_list(xs):</span>
  <span class="s2">return </span><span class="s1">xs[</span><span class="s3">1</span><span class="s1">:] + xs[:</span><span class="s3">1</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">rolled_loop_step(i</span><span class="s2">, </span><span class="s1">state):</span>
  <span class="s1">x</span><span class="s2">, </span><span class="s1">ks</span><span class="s2">, </span><span class="s1">rotations = state</span>
  <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">rotations[</span><span class="s3">0</span><span class="s1">]:</span>
    <span class="s1">x = apply_round(x</span><span class="s2">, </span><span class="s1">r)</span>
  <span class="s1">new_x = [x[</span><span class="s3">0</span><span class="s1">] + ks[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">x[</span><span class="s3">1</span><span class="s1">] + ks[</span><span class="s3">1</span><span class="s1">] + jnp.asarray(i + </span><span class="s3">1</span><span class="s2">, </span><span class="s1">dtype=np.uint32)]</span>
  <span class="s2">return </span><span class="s1">new_x</span><span class="s2">, </span><span class="s1">rotate_list(ks)</span><span class="s2">, </span><span class="s1">rotate_list(rotations)</span>


<span class="s2">def </span><span class="s1">_threefry2x32_lowering(key1</span><span class="s2">, </span><span class="s1">key2</span><span class="s2">, </span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">, </span><span class="s1">use_rolled_loops=</span><span class="s2">True</span><span class="s1">):</span>
  <span class="s4">&quot;&quot;&quot;Apply the Threefry 2x32 hash. 
 
  Args: 
    keypair: a pair of 32bit unsigned integers used for the key. 
    count: an array of dtype uint32 used for the counts. 
 
  Returns: 
    An array of dtype uint32 with the same shape as `count`. 
  &quot;&quot;&quot;</span>
  <span class="s1">x = [x1</span><span class="s2">, </span><span class="s1">x2]</span>

  <span class="s1">rotations = [np.array([</span><span class="s3">13</span><span class="s2">, </span><span class="s3">15</span><span class="s2">, </span><span class="s3">26</span><span class="s2">, </span><span class="s3">6</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.uint32)</span><span class="s2">,</span>
               <span class="s1">np.array([</span><span class="s3">17</span><span class="s2">, </span><span class="s3">29</span><span class="s2">, </span><span class="s3">16</span><span class="s2">, </span><span class="s3">24</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.uint32)]</span>
  <span class="s1">ks = [key1</span><span class="s2">, </span><span class="s1">key2</span><span class="s2">, </span><span class="s1">key1 ^ key2 ^ np.uint32(</span><span class="s3">0x1BD11BDA</span><span class="s1">)]</span>

  <span class="s1">x[</span><span class="s3">0</span><span class="s1">] = x[</span><span class="s3">0</span><span class="s1">] + ks[</span><span class="s3">0</span><span class="s1">]</span>
  <span class="s1">x[</span><span class="s3">1</span><span class="s1">] = x[</span><span class="s3">1</span><span class="s1">] + ks[</span><span class="s3">1</span><span class="s1">]</span>

  <span class="s2">if </span><span class="s1">use_rolled_loops:</span>
    <span class="s1">x</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = lax.fori_loop(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s1">rolled_loop_step</span><span class="s2">, </span><span class="s1">(x</span><span class="s2">, </span><span class="s1">rotate_list(ks)</span><span class="s2">, </span><span class="s1">rotations))</span>

  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">rotations[</span><span class="s3">0</span><span class="s1">]:</span>
      <span class="s1">x = apply_round(x</span><span class="s2">, </span><span class="s1">r)</span>
    <span class="s1">x[</span><span class="s3">0</span><span class="s1">] = x[</span><span class="s3">0</span><span class="s1">] + ks[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">x[</span><span class="s3">1</span><span class="s1">] = x[</span><span class="s3">1</span><span class="s1">] + ks[</span><span class="s3">2</span><span class="s1">] + np.uint32(</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">rotations[</span><span class="s3">1</span><span class="s1">]:</span>
      <span class="s1">x = apply_round(x</span><span class="s2">, </span><span class="s1">r)</span>
    <span class="s1">x[</span><span class="s3">0</span><span class="s1">] = x[</span><span class="s3">0</span><span class="s1">] + ks[</span><span class="s3">2</span><span class="s1">]</span>
    <span class="s1">x[</span><span class="s3">1</span><span class="s1">] = x[</span><span class="s3">1</span><span class="s1">] + ks[</span><span class="s3">0</span><span class="s1">] + np.uint32(</span><span class="s3">2</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">rotations[</span><span class="s3">0</span><span class="s1">]:</span>
      <span class="s1">x = apply_round(x</span><span class="s2">, </span><span class="s1">r)</span>
    <span class="s1">x[</span><span class="s3">0</span><span class="s1">] = x[</span><span class="s3">0</span><span class="s1">] + ks[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">x[</span><span class="s3">1</span><span class="s1">] = x[</span><span class="s3">1</span><span class="s1">] + ks[</span><span class="s3">1</span><span class="s1">] + np.uint32(</span><span class="s3">3</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">rotations[</span><span class="s3">1</span><span class="s1">]:</span>
      <span class="s1">x = apply_round(x</span><span class="s2">, </span><span class="s1">r)</span>
    <span class="s1">x[</span><span class="s3">0</span><span class="s1">] = x[</span><span class="s3">0</span><span class="s1">] + ks[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">x[</span><span class="s3">1</span><span class="s1">] = x[</span><span class="s3">1</span><span class="s1">] + ks[</span><span class="s3">2</span><span class="s1">] + np.uint32(</span><span class="s3">4</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">rotations[</span><span class="s3">0</span><span class="s1">]:</span>
      <span class="s1">x = apply_round(x</span><span class="s2">, </span><span class="s1">r)</span>
    <span class="s1">x[</span><span class="s3">0</span><span class="s1">] = x[</span><span class="s3">0</span><span class="s1">] + ks[</span><span class="s3">2</span><span class="s1">]</span>
    <span class="s1">x[</span><span class="s3">1</span><span class="s1">] = x[</span><span class="s3">1</span><span class="s1">] + ks[</span><span class="s3">0</span><span class="s1">] + np.uint32(</span><span class="s3">5</span><span class="s1">)</span>

  <span class="s2">return </span><span class="s1">tuple(x)</span>


<span class="s2">def </span><span class="s1">_threefry2x32_gpu_lowering(lowering_func</span><span class="s2">, </span><span class="s1">ctx</span><span class="s2">, </span><span class="s1">k1</span><span class="s2">, </span><span class="s1">k2</span><span class="s2">, </span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">aval_out_2 = ctx.avals_out</span>
  <span class="s2">assert </span><span class="s1">aval_out == aval_out_2</span>
  <span class="s1">k1_aval</span><span class="s2">, </span><span class="s1">k2_aval</span><span class="s2">, </span><span class="s1">x1_aval</span><span class="s2">, </span><span class="s1">x2_aval = ctx.avals_in</span>
  <span class="s1">rank = len(aval_out.shape)</span>
  <span class="s2">if </span><span class="s3">0 </span><span class="s2">in </span><span class="s1">aval_out.shape:</span>
    <span class="s1">zeros = mlir.full_like_aval(ctx</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s1">aval_out)</span>
    <span class="s2">return </span><span class="s1">[zeros</span><span class="s2">, </span><span class="s1">zeros]</span>
  <span class="s2">def </span><span class="s1">_broadcast(x</span><span class="s2">, </span><span class="s1">aval):</span>
    <span class="s2">return </span><span class="s1">mlir.broadcast_in_dim(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">aval_out</span><span class="s2">,</span>
                                 <span class="s1">broadcast_dimensions=range(rank - len(aval.shape)</span><span class="s2">, </span><span class="s1">rank))</span>

  <span class="s1">out_len = reduce(op.mul</span><span class="s2">, </span><span class="s1">aval_out.shape</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
  <span class="s2">if not </span><span class="s1">core.is_constant_dim(out_len):</span>
    <span class="s1">length = mlir.shape_tensor(mlir.eval_dynamic_shape(ctx</span><span class="s2">, </span><span class="s1">[out_len]))</span>
    <span class="s1">length = mlir.hlo.ConvertOp(</span>
        <span class="s1">ir.RankedTensorType.get((</span><span class="s3">1</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">ir.IntegerType.get_signless(</span><span class="s3">64</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">length).result</span>
    <span class="s1">output_shape = mlir.shape_tensor(mlir.eval_dynamic_shape(ctx</span><span class="s2">, </span><span class="s1">aval_out.shape))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">length = int(out_len)  </span><span class="s0"># will be passed statically</span>
    <span class="s1">output_shape = </span><span class="s2">None</span>

  <span class="s2">if </span><span class="s1">(jaxlib_version &gt;= (</span><span class="s3">0</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">9</span><span class="s1">)):</span>
    <span class="s2">return </span><span class="s1">lowering_func(</span>
            <span class="s1">(_broadcast(k1</span><span class="s2">, </span><span class="s1">k1_aval)</span><span class="s2">, </span><span class="s1">_broadcast(k2</span><span class="s2">, </span><span class="s1">k2_aval))</span><span class="s2">,</span>
            <span class="s1">(_broadcast(x1</span><span class="s2">, </span><span class="s1">x1_aval)</span><span class="s2">, </span><span class="s1">_broadcast(x2</span><span class="s2">, </span><span class="s1">x2_aval))</span><span class="s2">, </span><span class="s1">length</span><span class="s2">,</span>
            <span class="s1">output_shape)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">output_shape </span><span class="s2">is not None</span><span class="s1">:</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;native lowering with shape polymorphism &quot;</span>
                       <span class="s5">&quot;for threefry on GPU requires jaxlib version 0.4.9&quot;</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">lowering_func(</span>
            <span class="s1">(_broadcast(k1</span><span class="s2">, </span><span class="s1">k1_aval)</span><span class="s2">, </span><span class="s1">_broadcast(k2</span><span class="s2">, </span><span class="s1">k2_aval))</span><span class="s2">,</span>
            <span class="s1">(_broadcast(x1</span><span class="s2">, </span><span class="s1">x1_aval)</span><span class="s2">, </span><span class="s1">_broadcast(x2</span><span class="s2">, </span><span class="s1">x2_aval))</span><span class="s2">, </span><span class="s1">length)</span>

<span class="s1">threefry2x32_p = core.Primitive(</span><span class="s5">&quot;threefry2x32&quot;</span><span class="s1">)</span>
<span class="s1">threefry2x32_p.multiple_results = </span><span class="s2">True</span>
<span class="s1">threefry2x32_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">threefry2x32_p))</span>
<span class="s1">threefry2x32_p.def_abstract_eval(_threefry2x32_abstract_eval)</span>
<span class="s1">batching.defbroadcasting(threefry2x32_p)</span>
<span class="s1">mlir.register_lowering(threefry2x32_p</span><span class="s2">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">partial(_threefry2x32_lowering</span><span class="s2">, </span><span class="s1">use_rolled_loops=</span><span class="s2">False</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">multiple_results=</span><span class="s2">True</span><span class="s1">))</span>
<span class="s1">mlir.register_lowering(threefry2x32_p</span><span class="s2">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">partial(_threefry2x32_lowering</span><span class="s2">, </span><span class="s1">use_rolled_loops=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">multiple_results=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">, </span><span class="s1">platform=</span><span class="s5">'cpu'</span><span class="s1">)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">threefry2x32_p</span><span class="s2">,</span>
    <span class="s1">partial(_threefry2x32_gpu_lowering</span><span class="s2">, </span><span class="s1">gpu_prng.cuda_threefry2x32)</span><span class="s2">,</span>
    <span class="s1">platform=</span><span class="s5">'cuda'</span><span class="s1">)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">threefry2x32_p</span><span class="s2">,</span>
    <span class="s1">partial(_threefry2x32_gpu_lowering</span><span class="s2">, </span><span class="s1">gpu_prng.rocm_threefry2x32)</span><span class="s2">,</span>
    <span class="s1">platform=</span><span class="s5">'rocm'</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">iota_2x32_shape(shape):</span>
  <span class="s4">&quot;&quot;&quot;Reshaped ``uint64`` iota, as two parallel ``uint32`` arrays. 
 
  Setting aside representation, this function essentially computes the 
  equivalent of:: 
 
    jax.lax.iota(dtype=np.uint64, size=math.prod(shape)).reshape(shape) 
 
  However: 
 
  * It returns two parallel ``uint32`` arrays instead of one 
    ``uint64`` array. This renders it invariant under either setting of 
    the system-wide ``jax_enable_x64`` configuration flag. 
 
  * It lowers in a way such that the compiler's automatic SPMD 
    partitioner recognizes its partitionability. 
 
  For example:: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from jax import lax 
    &gt;&gt;&gt; from jax._src import prng 
 
    &gt;&gt;&gt; prng.iota_2x32_shape((3, 4)) 
    [Array([[0, 0, 0, 0], 
            [0, 0, 0, 0], 
            [0, 0, 0, 0]], dtype=uint32), 
     Array([[ 0,  1,  2,  3], 
            [ 4,  5,  6,  7], 
            [ 8,  9, 10, 11]], dtype=uint32)] 
 
    &gt;&gt;&gt; def reshaped_iota(shape): 
    ...   return lax.iota(size=math.prod(shape), dtype=np.uint32).reshape(shape) 
    ... 
    &gt;&gt;&gt; reshaped_iota((3, 4)) 
    Array([[ 0,  1,  2,  3], 
           [ 4,  5,  6,  7], 
           [ 8,  9, 10, 11]], dtype=uint32) 
 
  Args: 
    shape: the output shape 
 
  Returns: 
    A pair of ``uint32`` arrays ``(counts_hi, counts_lo)``, both of 
    shape ``shape``, representing the higher-order and lower-order 32 
    bits of the 64 bit unsigned iota. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">len(shape) == </span><span class="s3">0</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">(jnp.zeros(()</span><span class="s2">, </span><span class="s1">np.dtype(</span><span class="s5">'uint32'</span><span class="s1">))</span><span class="s2">,</span><span class="s1">) * </span><span class="s3">2</span>
  <span class="s2">return </span><span class="s1">iota_2x32_shape_p.bind(shape=shape)</span>

<span class="s1">iota_2x32_shape_p = core.Primitive(</span><span class="s5">'iota_2x32_shape'</span><span class="s1">)</span>
<span class="s1">iota_2x32_shape_p.multiple_results = </span><span class="s2">True</span>
<span class="s1">iota_2x32_shape_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">iota_2x32_shape_p))</span>

<span class="s1">@iota_2x32_shape_p.def_abstract_eval</span>
<span class="s2">def </span><span class="s1">iota_2x32_shape_abstract_eval(*</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s2">return </span><span class="s1">(core.ShapedArray(shape</span><span class="s2">, </span><span class="s1">np.dtype(</span><span class="s5">'uint32'</span><span class="s1">))</span><span class="s2">,</span><span class="s1">) * </span><span class="s3">2</span>

<span class="s2">def </span><span class="s1">bcast_iotas_to_reshaped_iota(</span>
    <span class="s1">add: Callable[[ir.Value</span><span class="s2">, </span><span class="s1">ir.Value]</span><span class="s2">, </span><span class="s1">ir.Value]</span><span class="s2">,</span>
    <span class="s1">mul: Callable[[core.DimSize</span><span class="s2">, </span><span class="s1">ir.Value]</span><span class="s2">, </span><span class="s1">ir.Value]</span><span class="s2">,</span>
    <span class="s1">shape: core.Shape</span><span class="s2">,</span>
    <span class="s1">iotas: Sequence[ir.Value]) -&gt; ir.Value:</span>
  <span class="s1">strides: core.Shape = (*(np.cumprod(shape[</span><span class="s3">1</span><span class="s1">:][::-</span><span class="s3">1</span><span class="s1">])[::-</span><span class="s3">1</span><span class="s1">])</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)  </span><span class="s0"># type: ignore</span>
  <span class="s2">return </span><span class="s1">reduce(add</span><span class="s2">, </span><span class="s1">[mul(s</span><span class="s2">, </span><span class="s1">i) </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">s </span><span class="s2">in </span><span class="s1">zip(iotas</span><span class="s2">, </span><span class="s1">strides)])  </span><span class="s0"># type: ignore</span>

<span class="s2">def </span><span class="s1">iota_2x32_shape_lowering(ctx</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">_ = ctx.avals_out</span>
  <span class="s1">aval_u64 = core.ShapedArray(shape</span><span class="s2">, </span><span class="s1">np.dtype(</span><span class="s5">'uint64'</span><span class="s1">))</span>

  <span class="s2">def </span><span class="s1">_add(x: ir.Value</span><span class="s2">, </span><span class="s1">y: ir.Value) -&gt; ir.Value:</span>
    <span class="s2">return </span><span class="s1">mlir.hlo.AddOp(x</span><span class="s2">, </span><span class="s1">y).result</span>

  <span class="s2">def </span><span class="s1">_mul(x: core.DimSize</span><span class="s2">, </span><span class="s1">y: ir.Value) -&gt; ir.Value:</span>
    <span class="s2">if </span><span class="s1">core.is_constant_dim(x):</span>
      <span class="s1">x_const = mlir.ir_constant(np.array(x</span><span class="s2">, </span><span class="s1">np.dtype(</span><span class="s5">'uint64'</span><span class="s1">))</span><span class="s2">,</span>
                                 <span class="s1">canonicalize_types=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">x_const</span><span class="s2">, </span><span class="s1">= mlir.eval_dynamic_shape(ctx</span><span class="s2">, </span><span class="s1">(x</span><span class="s2">,</span><span class="s1">))</span>
      <span class="s1">x_const = hlo.ConvertOp(</span>
          <span class="s1">ir.RankedTensorType.get(</span>
              <span class="s1">()</span><span class="s2">,</span>
              <span class="s1">mlir.dtype_to_ir_type(np.dtype(</span><span class="s5">'uint64'</span><span class="s1">)))</span><span class="s2">, </span><span class="s1">x_const).result</span>
    <span class="s1">x_bcast = mlir.broadcast_in_dim(ctx</span><span class="s2">, </span><span class="s1">x_const</span><span class="s2">, </span><span class="s1">aval_u64</span><span class="s2">,</span>
                                    <span class="s1">broadcast_dimensions=[])</span>
    <span class="s2">return </span><span class="s1">mlir.hlo.MulOp(x_bcast</span><span class="s2">, </span><span class="s1">y).result</span>

  <span class="s2">assert </span><span class="s1">len(shape) &gt; </span><span class="s3">0</span>

  <span class="s1">iotas = [mlir.iota(ctx</span><span class="s2">, </span><span class="s1">aval_u64</span><span class="s2">, </span><span class="s1">dimension=dimension)</span>
           <span class="s2">for </span><span class="s1">dimension </span><span class="s2">in </span><span class="s1">range(len(shape))]</span>
  <span class="s1">counts = bcast_iotas_to_reshaped_iota(_add</span><span class="s2">, </span><span class="s1">_mul</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">iotas)</span>
  <span class="s1">shift = mlir.ir_constant(np.array(</span><span class="s3">32</span><span class="s2">, </span><span class="s1">np.dtype(</span><span class="s5">'uint64'</span><span class="s1">))</span><span class="s2">,</span>
                           <span class="s1">canonicalize_types=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s1">shift = mlir.broadcast_in_dim(ctx</span><span class="s2">, </span><span class="s1">shift</span><span class="s2">, </span><span class="s1">aval_u64</span><span class="s2">,</span>
                                <span class="s1">broadcast_dimensions=[])</span>
  <span class="s1">counts_shifted = mlir.hlo.ShiftRightLogicalOp(counts</span><span class="s2">, </span><span class="s1">shift).result</span>
  <span class="s1">counts_lo = mlir.hlo.ConvertOp(mlir.aval_to_ir_type(aval_out)</span><span class="s2">, </span><span class="s1">counts).result</span>
  <span class="s1">counts_hi = mlir.hlo.ConvertOp(mlir.aval_to_ir_type(aval_out)</span><span class="s2">,</span>
                                  <span class="s1">counts_shifted).result</span>
  <span class="s2">return </span><span class="s1">counts_hi</span><span class="s2">, </span><span class="s1">counts_lo</span>
<span class="s1">mlir.register_lowering(iota_2x32_shape_p</span><span class="s2">, </span><span class="s1">iota_2x32_shape_lowering)</span>


<span class="s1">@partial(jit</span><span class="s2">, </span><span class="s1">inline=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">threefry_2x32(keypair</span><span class="s2">, </span><span class="s1">count):</span>
  <span class="s4">&quot;&quot;&quot;Apply the Threefry 2x32 hash. 
 
  Args: 
    keypair: a pair of 32bit unsigned integers used for the key. 
    count: an array of dtype uint32 used for the counts. 
 
  Returns: 
    An array of dtype uint32 with the same shape as `count`. 
  &quot;&quot;&quot;</span>
  <span class="s1">key1</span><span class="s2">, </span><span class="s1">key2 = keypair</span>
  <span class="s2">if not </span><span class="s1">lax.dtype(key1) == lax.dtype(key2) == lax.dtype(count) == np.uint32:</span>
    <span class="s1">msg = </span><span class="s5">&quot;threefry_2x32 requires uint32 arguments, got {}&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format([lax.dtype(x) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">[key1</span><span class="s2">, </span><span class="s1">key2</span><span class="s2">, </span><span class="s1">count]]))</span>

  <span class="s1">odd_size = count.size % </span><span class="s3">2</span>
  <span class="s2">if not </span><span class="s1">isinstance(odd_size</span><span class="s2">, </span><span class="s1">int):</span>
    <span class="s1">msg = (</span><span class="s5">&quot;jax.random functions have limited support for shape polymorphism. &quot;</span>
           <span class="s5">&quot;In particular, the product of the known dimensions must be even.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">core.InconclusiveDimensionOperation(msg)</span>

  <span class="s2">if </span><span class="s1">odd_size:</span>
    <span class="s1">x = list(jnp.split(jnp.concatenate([count.ravel()</span><span class="s2">, </span><span class="s1">np.uint32([</span><span class="s3">0</span><span class="s1">])])</span><span class="s2">, </span><span class="s3">2</span><span class="s1">))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">x = list(jnp.split(count.ravel()</span><span class="s2">, </span><span class="s3">2</span><span class="s1">))</span>

  <span class="s1">x = threefry2x32_p.bind(key1</span><span class="s2">, </span><span class="s1">key2</span><span class="s2">, </span><span class="s1">x[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">x[</span><span class="s3">1</span><span class="s1">])</span>
  <span class="s1">out = jnp.concatenate(x)</span>
  <span class="s2">assert </span><span class="s1">out.dtype == np.uint32</span>
  <span class="s2">return </span><span class="s1">lax.reshape(out[:-</span><span class="s3">1</span><span class="s1">] </span><span class="s2">if </span><span class="s1">odd_size </span><span class="s2">else </span><span class="s1">out</span><span class="s2">, </span><span class="s1">count.shape)</span>


<span class="s2">def </span><span class="s1">threefry_split(key: typing.Array</span><span class="s2">, </span><span class="s1">num: core.DimSize) -&gt; typing.Array:</span>
  <span class="s2">if not </span><span class="s1">core.is_special_dim_size(num):</span>
    <span class="s1">num = core.concrete_or_error(op.index</span><span class="s2">, </span><span class="s1">num)</span>
  <span class="s2">if </span><span class="s1">config.jax_threefry_partitionable:</span>
    <span class="s2">return </span><span class="s1">_threefry_split_foldlike(key</span><span class="s2">, </span><span class="s1">num)  </span><span class="s0"># type: ignore</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">_threefry_split_original(key</span><span class="s2">, </span><span class="s1">num)  </span><span class="s0"># type: ignore</span>

<span class="s1">@partial(jit</span><span class="s2">, </span><span class="s1">static_argnums=(</span><span class="s3">1</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">inline=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">_threefry_split_original(key</span><span class="s2">, </span><span class="s1">num) -&gt; typing.Array:</span>
  <span class="s1">counts = lax.iota(np.uint32</span><span class="s2">, </span><span class="s1">num * </span><span class="s3">2</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">lax.reshape(threefry_2x32(key</span><span class="s2">, </span><span class="s1">counts)</span><span class="s2">, </span><span class="s1">(num</span><span class="s2">, </span><span class="s3">2</span><span class="s1">))</span>

<span class="s1">@partial(jit</span><span class="s2">, </span><span class="s1">static_argnums=(</span><span class="s3">1</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">inline=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">_threefry_split_foldlike(key</span><span class="s2">, </span><span class="s1">num) -&gt; typing.Array:</span>
  <span class="s1">k1</span><span class="s2">, </span><span class="s1">k2 = key</span>
  <span class="s1">counts1</span><span class="s2">, </span><span class="s1">counts2 = iota_2x32_shape((num</span><span class="s2">,</span><span class="s1">))</span>
  <span class="s1">bits1</span><span class="s2">, </span><span class="s1">bits2 = threefry2x32_p.bind(k1</span><span class="s2">, </span><span class="s1">k2</span><span class="s2">, </span><span class="s1">counts1</span><span class="s2">, </span><span class="s1">counts2)</span>
  <span class="s2">return </span><span class="s1">jnp.stack([bits1</span><span class="s2">, </span><span class="s1">bits2]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">threefry_fold_in(key: typing.Array</span><span class="s2">, </span><span class="s1">data: typing.Array) -&gt; typing.Array:</span>
  <span class="s2">assert not </span><span class="s1">data.shape</span>
  <span class="s2">return </span><span class="s1">_threefry_fold_in(key</span><span class="s2">, </span><span class="s1">jnp.uint32(data))</span>

<span class="s1">@partial(jit</span><span class="s2">, </span><span class="s1">inline=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">_threefry_fold_in(key</span><span class="s2">, </span><span class="s1">data):</span>
  <span class="s2">return </span><span class="s1">threefry_2x32(key</span><span class="s2">, </span><span class="s1">threefry_seed(data))</span>


<span class="s2">def </span><span class="s1">threefry_random_bits(key: typing.Array</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s4">&quot;&quot;&quot;Sample uniform random bits of given width and shape using PRNG key.&quot;&quot;&quot;</span>
  <span class="s2">if not </span><span class="s1">_is_threefry_prng_key(key):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;threefry_random_bits got invalid prng key.&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">bit_width </span><span class="s2">not in </span><span class="s1">(</span><span class="s3">8</span><span class="s2">, </span><span class="s3">16</span><span class="s2">, </span><span class="s3">32</span><span class="s2">, </span><span class="s3">64</span><span class="s1">):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;requires 8-, 16-, 32- or 64-bit field width.&quot;</span><span class="s1">)</span>

  <span class="s2">if </span><span class="s1">config.jax_threefry_partitionable:</span>
    <span class="s2">return </span><span class="s1">_threefry_random_bits_partitionable(key</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">_threefry_random_bits_original(key</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape)</span>

<span class="s2">def </span><span class="s1">_threefry_random_bits_partitionable(key: typing.Array</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s2">if </span><span class="s1">all(core.is_constant_dim(d) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape) </span><span class="s2">and </span><span class="s1">math.prod(shape) &gt; </span><span class="s3">2 </span><span class="s1">** </span><span class="s3">64</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'random bits array of size exceeding 2 ** 64'</span><span class="s1">)</span>

  <span class="s1">k1</span><span class="s2">, </span><span class="s1">k2 = key</span>
  <span class="s1">counts1</span><span class="s2">, </span><span class="s1">counts2 = iota_2x32_shape(shape)</span>
  <span class="s1">bits1</span><span class="s2">, </span><span class="s1">bits2 = threefry2x32_p.bind(k1</span><span class="s2">, </span><span class="s1">k2</span><span class="s2">, </span><span class="s1">counts1</span><span class="s2">, </span><span class="s1">counts2)</span>

  <span class="s1">dtype = UINT_DTYPES[bit_width]</span>
  <span class="s2">if </span><span class="s1">bit_width == </span><span class="s3">64</span><span class="s1">:</span>
    <span class="s1">bits_hi = lax.convert_element_type(bits1</span><span class="s2">, </span><span class="s1">dtype)</span>
    <span class="s1">bits_lo = lax.convert_element_type(bits2</span><span class="s2">, </span><span class="s1">dtype)</span>
    <span class="s2">return </span><span class="s1">lax.shift_left(bits_hi</span><span class="s2">, </span><span class="s1">dtype(</span><span class="s3">32</span><span class="s1">)) | bits_lo</span>
  <span class="s2">elif </span><span class="s1">bit_width == </span><span class="s3">32</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">bits1 ^ bits2</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">lax.convert_element_type(bits1 ^ bits2</span><span class="s2">, </span><span class="s1">dtype)</span>

<span class="s1">@partial(jit</span><span class="s2">, </span><span class="s1">static_argnums=(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">inline=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">_threefry_random_bits_original(key: typing.Array</span><span class="s2">, </span><span class="s1">bit_width</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s1">size = math.prod(shape)</span>
  <span class="s0"># Compute ceil(bit_width * size / 32) in a way that is friendly to shape</span>
  <span class="s0"># polymorphism</span>
  <span class="s1">max_count</span><span class="s2">, </span><span class="s1">r = divmod(bit_width * size</span><span class="s2">, </span><span class="s3">32</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">r &gt; </span><span class="s3">0</span><span class="s1">:</span>
    <span class="s1">max_count += </span><span class="s3">1</span>

  <span class="s2">if </span><span class="s1">core.is_constant_dim(max_count):</span>
    <span class="s1">nblocks</span><span class="s2">, </span><span class="s1">rem = divmod(max_count</span><span class="s2">, </span><span class="s1">jnp.iinfo(np.uint32).max)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">nblocks</span><span class="s2">, </span><span class="s1">rem = </span><span class="s3">0</span><span class="s2">, </span><span class="s1">max_count</span>

  <span class="s2">if not </span><span class="s1">nblocks:</span>
    <span class="s1">bits = threefry_2x32(key</span><span class="s2">, </span><span class="s1">lax.iota(np.uint32</span><span class="s2">, </span><span class="s1">rem))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">keys = threefry_split(key</span><span class="s2">, </span><span class="s1">nblocks + </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">subkeys</span><span class="s2">, </span><span class="s1">last_key = keys[:-</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">keys[-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">blocks = vmap(threefry_2x32</span><span class="s2">, </span><span class="s1">in_axes=(</span><span class="s3">0</span><span class="s2">, None</span><span class="s1">))(subkeys</span><span class="s2">, </span><span class="s1">lax.iota(np.uint32</span><span class="s2">, </span><span class="s1">jnp.iinfo(np.uint32).max))</span>
    <span class="s1">last = threefry_2x32(last_key</span><span class="s2">, </span><span class="s1">lax.iota(np.uint32</span><span class="s2">, </span><span class="s1">rem))</span>
    <span class="s1">bits = lax.concatenate([blocks.ravel()</span><span class="s2">, </span><span class="s1">last]</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)</span>

  <span class="s1">dtype = UINT_DTYPES[bit_width]</span>
  <span class="s2">if </span><span class="s1">bit_width == </span><span class="s3">64</span><span class="s1">:</span>
    <span class="s1">bits = [lax.convert_element_type(x</span><span class="s2">, </span><span class="s1">dtype) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">jnp.split(bits</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)]</span>
    <span class="s1">bits = lax.shift_left(bits[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype(</span><span class="s3">32</span><span class="s1">)) | bits[</span><span class="s3">1</span><span class="s1">]</span>
  <span class="s2">elif </span><span class="s1">bit_width </span><span class="s2">in </span><span class="s1">[</span><span class="s3">8</span><span class="s2">, </span><span class="s3">16</span><span class="s1">]:</span>
    <span class="s0"># this is essentially bits.view(dtype)[:size]</span>
    <span class="s1">bits = lax.bitwise_and(</span>
      <span class="s1">np.uint32(np.iinfo(dtype).max)</span><span class="s2">,</span>
      <span class="s1">lax.shift_right_logical(</span>
        <span class="s1">lax.broadcast(bits</span><span class="s2">, </span><span class="s1">(</span><span class="s3">1</span><span class="s2">,</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">lax.mul(</span>
          <span class="s1">np.uint32(bit_width)</span><span class="s2">,</span>
          <span class="s1">lax.broadcasted_iota(np.uint32</span><span class="s2">, </span><span class="s1">(</span><span class="s3">32 </span><span class="s1">// bit_width</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">)</span>
      <span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s1">bits = lax.reshape(bits</span><span class="s2">, </span><span class="s1">((max_count * </span><span class="s3">32 </span><span class="s1">// bit_width)</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s1">))</span>
    <span class="s1">bits = lax.convert_element_type(bits</span><span class="s2">, </span><span class="s1">dtype)[:size]</span>
  <span class="s2">return </span><span class="s1">lax.reshape(bits</span><span class="s2">, </span><span class="s1">shape)</span>


<span class="s1">threefry_prng_impl = PRNGImpl(</span>
    <span class="s1">key_shape=(</span><span class="s3">2</span><span class="s2">,</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">seed=threefry_seed</span><span class="s2">,</span>
    <span class="s1">split=threefry_split</span><span class="s2">,</span>
    <span class="s1">random_bits=threefry_random_bits</span><span class="s2">,</span>
    <span class="s1">fold_in=threefry_fold_in</span><span class="s2">,</span>
    <span class="s1">tag=</span><span class="s5">'fry'</span><span class="s1">)</span>


<span class="s0"># -- RngBitGenerator PRNG implementation</span>

<span class="s0"># This code is experimental!</span>
<span class="s0"># https://www.tensorflow.org/xla/operation_semantics#rngbitgenerator</span>
<span class="s0"># Notice that the RngBitGenerator operations are not guaranteed to be</span>
<span class="s0"># stable/deterministic across backends or compiler versions. Correspondingly, we</span>
<span class="s0"># reserve the right to change any of these implementations at any time!</span>

<span class="s2">def </span><span class="s1">_rbg_seed(seed: typing.Array) -&gt; typing.Array:</span>
  <span class="s2">assert not </span><span class="s1">seed.shape</span>
  <span class="s1">halfkey = threefry_seed(seed)</span>
  <span class="s2">return </span><span class="s1">jnp.concatenate([halfkey</span><span class="s2">, </span><span class="s1">halfkey])</span>

<span class="s2">def </span><span class="s1">_rbg_split(key: typing.Array</span><span class="s2">, </span><span class="s1">num: int) -&gt; typing.Array:</span>
  <span class="s2">if </span><span class="s1">config.jax_threefry_partitionable:</span>
    <span class="s1">_threefry_split = _threefry_split_foldlike</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">_threefry_split = _threefry_split_original</span>
  <span class="s2">return </span><span class="s1">vmap(</span>
      <span class="s1">_threefry_split</span><span class="s2">, </span><span class="s1">(</span><span class="s3">0</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)(key.reshape(</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">num).reshape(num</span><span class="s2">, </span><span class="s3">4</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_rbg_fold_in(key: typing.Array</span><span class="s2">, </span><span class="s1">data: typing.Array) -&gt; typing.Array:</span>
  <span class="s2">assert not </span><span class="s1">data.shape</span>
  <span class="s2">return </span><span class="s1">vmap(_threefry_fold_in</span><span class="s2">, </span><span class="s1">(</span><span class="s3">0</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)(key.reshape(</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">data).reshape(</span><span class="s3">4</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_rbg_random_bits(key: typing.Array</span><span class="s2">, </span><span class="s1">bit_width: int</span><span class="s2">, </span><span class="s1">shape: Sequence[int]</span>
                     <span class="s1">) -&gt; typing.Array:</span>
  <span class="s2">if not </span><span class="s1">key.shape == (</span><span class="s3">4</span><span class="s2">,</span><span class="s1">) </span><span class="s2">and </span><span class="s1">key.dtype == jnp.dtype(</span><span class="s5">'uint32'</span><span class="s1">):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;_rbg_random_bits got invalid prng key.&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">bit_width </span><span class="s2">not in </span><span class="s1">(</span><span class="s3">8</span><span class="s2">, </span><span class="s3">16</span><span class="s2">, </span><span class="s3">32</span><span class="s2">, </span><span class="s3">64</span><span class="s1">):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;requires 8-, 16-, 32- or 64-bit field width.&quot;</span><span class="s1">)</span>
  <span class="s1">_</span><span class="s2">, </span><span class="s1">bits = lax.rng_bit_generator(key</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype=UINT_DTYPES[bit_width])</span>
  <span class="s2">return </span><span class="s1">bits</span>

<span class="s1">rbg_prng_impl = PRNGImpl(</span>
    <span class="s1">key_shape=(</span><span class="s3">4</span><span class="s2">,</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">seed=_rbg_seed</span><span class="s2">,</span>
    <span class="s1">split=_rbg_split</span><span class="s2">,</span>
    <span class="s1">random_bits=_rbg_random_bits</span><span class="s2">,</span>
    <span class="s1">fold_in=_rbg_fold_in</span><span class="s2">,</span>
    <span class="s1">tag=</span><span class="s5">'rbg'</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_unsafe_rbg_split(key: typing.Array</span><span class="s2">, </span><span class="s1">num: int) -&gt; typing.Array:</span>
  <span class="s0"># treat 10 iterations of random bits as a 'hash function'</span>
  <span class="s1">_</span><span class="s2">, </span><span class="s1">keys = lax.rng_bit_generator(key</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10 </span><span class="s1">* num</span><span class="s2">, </span><span class="s3">4</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s5">'uint32'</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">lax.slice_in_dim(keys</span><span class="s2">, </span><span class="s1">start_index=</span><span class="s2">None, </span><span class="s1">limit_index=</span><span class="s2">None, </span><span class="s1">stride=</span><span class="s3">10</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_unsafe_rbg_fold_in(key: typing.Array</span><span class="s2">, </span><span class="s1">data: typing.Array) -&gt; typing.Array:</span>
  <span class="s2">assert not </span><span class="s1">data.shape</span>
  <span class="s1">_</span><span class="s2">, </span><span class="s1">random_bits = lax.rng_bit_generator(_rbg_seed(data)</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">4</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s5">'uint32'</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">key ^ random_bits[-</span><span class="s3">1</span><span class="s1">]</span>

<span class="s1">unsafe_rbg_prng_impl = PRNGImpl(</span>
    <span class="s1">key_shape=(</span><span class="s3">4</span><span class="s2">,</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">seed=_rbg_seed</span><span class="s2">,</span>
    <span class="s1">split=_unsafe_rbg_split</span><span class="s2">,</span>
    <span class="s1">random_bits=_rbg_random_bits</span><span class="s2">,</span>
    <span class="s1">fold_in=_unsafe_rbg_fold_in</span><span class="s2">,</span>
    <span class="s1">tag=</span><span class="s5">'urbg'</span><span class="s1">)</span>
</pre>
</body>
</html>