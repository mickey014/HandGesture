<html>
<head>
<title>api.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
api.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2018 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">&quot;&quot;&quot;JAX user-facing transformations and utilities. 
 
The transformations here mostly wrap internal transformations, providing 
convenience flags to control behavior and handling Python containers of 
arguments and outputs. The Python containers handled are pytrees (see 
tree_util.py), which include nested tuples/lists/dicts, where the leaves are 
arrays. 
&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">__future__ </span><span class="s3">import </span><span class="s1">annotations</span>

<span class="s3">import </span><span class="s1">collections</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">inspect</span>
<span class="s3">import </span><span class="s1">math</span>
<span class="s3">import </span><span class="s1">typing</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">(Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Generator</span><span class="s3">, </span><span class="s1">Hashable</span><span class="s3">, </span><span class="s1">Iterable</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">Literal</span><span class="s3">,</span>
                    <span class="s1">NamedTuple</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Tuple</span><span class="s3">, </span><span class="s1">TypeVar</span><span class="s3">, </span><span class="s1">Union</span><span class="s3">,</span>
                    <span class="s1">overload</span><span class="s3">, </span><span class="s1">cast)</span>
<span class="s3">import </span><span class="s1">weakref</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">contextlib </span><span class="s3">import </span><span class="s1">contextmanager</span><span class="s3">, </span><span class="s1">ExitStack</span>

<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">linear_util </span><span class="s3">as </span><span class="s1">lu</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">stages</span>
<span class="s3">from </span><span class="s1">jax._src.tree_util </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">tree_map</span><span class="s3">, </span><span class="s1">tree_flatten</span><span class="s3">, </span><span class="s1">tree_unflatten</span><span class="s3">, </span><span class="s1">tree_structure</span><span class="s3">, </span><span class="s1">tree_transpose</span><span class="s3">,</span>
    <span class="s1">tree_leaves</span><span class="s3">, </span><span class="s1">Partial</span><span class="s3">, </span><span class="s1">PyTreeDef</span><span class="s3">, </span><span class="s1">all_leaves</span><span class="s3">, </span><span class="s1">keystr</span><span class="s3">, </span><span class="s1">broadcast_prefix</span><span class="s3">,</span>
    <span class="s1">prefix_errors</span><span class="s3">, </span><span class="s1">generate_key_paths)</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dispatch</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">effects</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">array</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dtypes</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">sharding_impls</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">sharding_specs</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">source_info_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">traceback_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">pjit</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">xla_bridge </span><span class="s3">as </span><span class="s1">xb</span>
<span class="s3">from </span><span class="s1">jax._src.core </span><span class="s3">import </span><span class="s1">eval_jaxpr</span>
<span class="s3">from </span><span class="s1">jax._src.api_util </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">flatten_fun</span><span class="s3">, </span><span class="s1">apply_flat_fun</span><span class="s3">, </span><span class="s1">flatten_fun_nokwargs</span><span class="s3">, </span><span class="s1">flatten_fun_nokwargs2</span><span class="s3">,</span>
    <span class="s1">argnums_partial</span><span class="s3">, </span><span class="s1">argnums_partial_except</span><span class="s3">, </span><span class="s1">flatten_axes</span><span class="s3">, </span><span class="s1">donation_vector</span><span class="s3">,</span>
    <span class="s1">rebase_donate_argnums</span><span class="s3">, </span><span class="s1">_ensure_index</span><span class="s3">, </span><span class="s1">_ensure_index_tuple</span><span class="s3">,</span>
    <span class="s1">shaped_abstractify</span><span class="s3">, </span><span class="s1">_ensure_str_tuple</span><span class="s3">,</span>
    <span class="s1">check_callable</span><span class="s3">, </span><span class="s1">debug_info</span><span class="s3">, </span><span class="s1">result_paths</span><span class="s3">, </span><span class="s1">flat_out_axes</span><span class="s3">, </span><span class="s1">debug_info_final)</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">lax </span><span class="s3">as </span><span class="s1">lax_internal</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">jax_jit</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">xla_client </span><span class="s3">as </span><span class="s1">xc</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">pmap_lib</span>
<span class="s3">from </span><span class="s1">jax._src.sharding </span><span class="s3">import </span><span class="s1">Sharding</span>
<span class="s3">from </span><span class="s1">jax._src.sharding_impls </span><span class="s3">import </span><span class="s1">PmapSharding</span>
<span class="s3">from </span><span class="s1">jax._src.traceback_util </span><span class="s3">import </span><span class="s1">api_boundary</span>
<span class="s3">from </span><span class="s1">jax._src.util </span><span class="s3">import </span><span class="s1">unzip2</span><span class="s3">, </span><span class="s1">safe_map</span><span class="s3">, </span><span class="s1">safe_zip</span><span class="s3">, </span><span class="s1">wrap_name</span><span class="s3">, </span><span class="s1">wraps</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">util</span>


<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">partial_eval </span><span class="s3">as </span><span class="s1">pe</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">xla</span>

<span class="s3">from </span><span class="s1">jax._src.config </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">config</span><span class="s3">,</span>
    <span class="s1">disable_jit </span><span class="s3">as </span><span class="s1">_disable_jit</span><span class="s3">,</span>
    <span class="s1">debug_nans </span><span class="s3">as </span><span class="s1">config_debug_nans</span><span class="s3">,</span>
    <span class="s1">debug_infs </span><span class="s3">as </span><span class="s1">config_debug_infs</span><span class="s3">,</span>
    <span class="s1">_thread_local_state </span><span class="s3">as </span><span class="s1">config_thread_local_state</span><span class="s3">,</span>
    <span class="s1">explicit_device_put_scope </span><span class="s3">as </span><span class="s1">config_explicit_device_put_scope</span><span class="s3">,</span>
    <span class="s1">explicit_device_get_scope </span><span class="s3">as </span><span class="s1">config_explicit_device_get_scope)</span>
<span class="s3">from </span><span class="s1">jax._src.core </span><span class="s3">import </span><span class="s1">ShapedArray</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">ad</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">batching</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">pxla</span>


<span class="s1">traceback_util.register_exclusion(__file__)</span>

<span class="s1">_dtype = partial(dtypes.dtype</span><span class="s3">, </span><span class="s1">canonicalize=</span><span class="s3">True</span><span class="s1">)</span>

<span class="s1">AxisName = Hashable</span>

<span class="s1">Device = xc.Device</span>

<span class="s0"># These TypeVars are used below to express the fact that function types</span>
<span class="s0"># (i.e. call signatures) are invariant under the vmap transformation.</span>
<span class="s1">F = TypeVar(</span><span class="s4">&quot;F&quot;</span><span class="s3">, </span><span class="s1">bound=Callable)</span>
<span class="s1">T = TypeVar(</span><span class="s4">&quot;T&quot;</span><span class="s1">)</span>
<span class="s1">U = TypeVar(</span><span class="s4">&quot;U&quot;</span><span class="s1">)</span>

<span class="s1">map</span><span class="s3">, </span><span class="s1">unsafe_map = safe_map</span><span class="s3">, </span><span class="s1">map</span>
<span class="s1">zip</span><span class="s3">, </span><span class="s1">unsafe_zip = safe_zip</span><span class="s3">, </span><span class="s1">zip</span>


<span class="s3">def </span><span class="s1">_nan_check_posthook(fun</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs</span><span class="s3">, </span><span class="s1">output):</span>
  <span class="s2">&quot;&quot;&quot;Hook function called by the C++ jit/pmap to perform NaN checking.&quot;&quot;&quot;</span>
  <span class="s1">buffers = []</span>
  <span class="s3">for </span><span class="s1">leaf </span><span class="s3">in </span><span class="s1">tree_leaves(output):</span>
    <span class="s3">if </span><span class="s1">hasattr(leaf</span><span class="s3">, </span><span class="s4">&quot;device_buffers&quot;</span><span class="s1">):</span>
      <span class="s1">buffers.extend(leaf.device_buffers)</span>

  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">dispatch.check_special(pjit.pjit_p</span><span class="s3">, </span><span class="s1">buffers)</span>
  <span class="s3">except </span><span class="s1">FloatingPointError:</span>
    <span class="s0"># compiled_fun can only raise in this case</span>
    <span class="s3">assert </span><span class="s1">config.jax_debug_nans </span><span class="s3">or </span><span class="s1">config.jax_debug_infs</span>
    <span class="s1">print(</span><span class="s4">&quot;Invalid nan value encountered in the output of a C++-jit/pmap &quot;</span>
          <span class="s4">&quot;function. Calling the de-optimized version.&quot;</span><span class="s1">)</span>
    <span class="s1">fun._cache_miss(*args</span><span class="s3">, </span><span class="s1">**kwargs)[</span><span class="s5">0</span><span class="s1">]  </span><span class="s0"># probably won't return</span>

<span class="s3">def </span><span class="s1">_update_debug_special_global(_):</span>
  <span class="s3">if </span><span class="s1">config._read(</span><span class="s4">&quot;jax_debug_nans&quot;</span><span class="s1">) </span><span class="s3">or </span><span class="s1">config._read(</span><span class="s4">&quot;jax_debug_infs&quot;</span><span class="s1">):</span>
    <span class="s1">jax_jit.global_state().post_hook = _nan_check_posthook</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">jax_jit.global_state().post_hook = </span><span class="s3">None</span>

<span class="s3">def </span><span class="s1">_update_debug_special_thread_local(_):</span>
  <span class="s3">if </span><span class="s1">(getattr(config_thread_local_state</span><span class="s3">, </span><span class="s4">&quot;jax_debug_nans&quot;</span><span class="s3">, False</span><span class="s1">) </span><span class="s3">or</span>
      <span class="s1">getattr(config_thread_local_state</span><span class="s3">, </span><span class="s4">&quot;jax_debug_infs&quot;</span><span class="s3">, False</span><span class="s1">)):</span>
    <span class="s1">jax_jit.thread_local_state().post_hook = _nan_check_posthook</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">jax_jit.thread_local_state().post_hook = </span><span class="s3">None</span>

<span class="s1">config_debug_nans._add_hooks(_update_debug_special_global</span><span class="s3">,</span>
                             <span class="s1">_update_debug_special_thread_local)</span>
<span class="s1">config_debug_infs._add_hooks(_update_debug_special_global</span><span class="s3">,</span>
                             <span class="s1">_update_debug_special_thread_local)</span>


<span class="s1">float0 = dtypes.float0</span>


<span class="s3">def </span><span class="s1">jit(</span>
  <span class="s1">fun: Callable</span><span class="s3">,</span>
  <span class="s1">in_shardings=sharding_impls.UNSPECIFIED</span><span class="s3">,</span>
  <span class="s1">out_shardings=sharding_impls.UNSPECIFIED</span><span class="s3">,</span>
  <span class="s1">static_argnums: Union[int</span><span class="s3">, </span><span class="s1">Sequence[int]</span><span class="s3">, None</span><span class="s1">] = </span><span class="s3">None,</span>
  <span class="s1">static_argnames: Union[str</span><span class="s3">, </span><span class="s1">Iterable[str]</span><span class="s3">, None</span><span class="s1">] = </span><span class="s3">None,</span>
  <span class="s1">donate_argnums: Union[int</span><span class="s3">, </span><span class="s1">Sequence[int]] = ()</span><span class="s3">,</span>
  <span class="s1">keep_unused: bool = </span><span class="s3">False,</span>
  <span class="s1">device: Optional[xc.Device] = </span><span class="s3">None,</span>
  <span class="s1">backend: Optional[str] = </span><span class="s3">None,</span>
  <span class="s1">inline: bool = </span><span class="s3">False,</span>
  <span class="s1">abstracted_axes: Optional[Any] = </span><span class="s3">None,</span>
<span class="s1">) -&gt; stages.Wrapped:</span>
  <span class="s2">&quot;&quot;&quot;Sets up ``fun`` for just-in-time compilation with XLA. 
 
  Args: 
    fun: Function to be jitted. ``fun`` should be a pure function, as 
      side-effects may only be executed once. 
 
      The arguments and return value of ``fun`` should be arrays, 
      scalars, or (nested) standard Python containers (tuple/list/dict) thereof. 
      Positional arguments indicated by ``static_argnums`` can be anything at 
      all, provided they are hashable and have an equality operation defined. 
      Static arguments are included as part of a compilation cache key, which is 
      why hash and equality operators must be defined. 
 
      JAX keeps a weak reference to ``fun`` for use as a compilation cache key, 
      so the object ``fun`` must be weakly-referenceable. Most :class:`Callable` 
      objects will already satisfy this requirement. 
    in_shardings: Pytree of structure matching that of arguments to ``fun``, 
      with all actual arguments replaced by resource assignment specifications. 
      It is also valid to specify a pytree prefix (e.g. one value in place of a 
      whole subtree), in which case the leaves get broadcast to all values in 
      that subtree. 
 
      The ``in_shardings`` argument is optional. JAX will infer the shardings 
      from the input :py:class:`jax.Array`'s and defaults to replicating the input 
      if the sharding cannot be inferred. 
 
      The valid resource assignment specifications are: 
        - :py:class:`XLACompatibleSharding`, which will decide how the value 
            will be partitioned. With this, using a mesh context manager is not 
            required. 
 
      The size of every dimension has to be a multiple of the total number of 
      resources assigned to it. This is similar to pjit's in_shardings. 
    out_shardings: Like ``in_shardings``, but specifies resource 
      assignment for function outputs. This is similar to pjit's 
      out_shardings. 
 
      The ``out_shardings`` argument is optional. If not specified, :py:func:`jax.jit` 
      will use GSPMD's sharding propagation to figure out what the sharding of the 
      output(s) should be. 
    static_argnums: An optional int or collection of ints that specify which 
      positional arguments to treat as static (compile-time constant). 
      Operations that only depend on static arguments will be constant-folded in 
      Python (during tracing), and so the corresponding argument values can be 
      any Python object. 
 
      Static arguments should be hashable, meaning both ``__hash__`` and 
      ``__eq__`` are implemented, and immutable. Calling the jitted function 
      with different values for these constants will trigger recompilation. 
      Arguments that are not arrays or containers thereof must be marked as 
      static. 
 
      If neither ``static_argnums`` nor ``static_argnames`` is provided, no 
      arguments are treated as static. If ``static_argnums`` is not provided but 
      ``static_argnames`` is, or vice versa, JAX uses 
      :code:`inspect.signature(fun)` to find any positional arguments that 
      correspond to ``static_argnames`` 
      (or vice versa). If both ``static_argnums`` and ``static_argnames`` are 
      provided, ``inspect.signature`` is not used, and only actual 
      parameters listed in either ``static_argnums`` or ``static_argnames`` will 
      be treated as static. 
    static_argnames: An optional string or collection of strings specifying 
      which named arguments to treat as static (compile-time constant). See the 
      comment on ``static_argnums`` for details. If not 
      provided but ``static_argnums`` is set, the default is based on calling 
      ``inspect.signature(fun)`` to find corresponding named arguments. 
    donate_argnums: Specify which positional argument buffers are &quot;donated&quot; to 
      the computation. It is safe to donate argument buffers if you no longer 
      need them once the computation has finished. In some cases XLA can make 
      use of donated buffers to reduce the amount of memory needed to perform a 
      computation, for example recycling one of your input buffers to store a 
      result. You should not reuse buffers that you donate to a computation, JAX 
      will raise an error if you try to. By default, no argument buffers are 
      donated. 
      Note that donate_argnums only work for positional arguments, and keyword 
      arguments will not be donated. 
 
      For more details on buffer donation see the 
      `FAQ &lt;https://jax.readthedocs.io/en/latest/faq.html#buffer-donation&gt;`_. 
    keep_unused: If `False` (the default), arguments that JAX determines to be 
      unused by `fun` *may* be dropped from resulting compiled XLA executables. 
      Such arguments will not be transferred to the device nor provided to the 
      underlying executable. If `True`, unused arguments will not be pruned. 
    device: This is an experimental feature and the API is likely to change. 
      Optional, the Device the jitted function will run on. (Available devices 
      can be retrieved via :py:func:`jax.devices`.) The default is inherited 
      from XLA's DeviceAssignment logic and is usually to use 
      ``jax.devices()[0]``. 
    backend: This is an experimental feature and the API is likely to change. 
      Optional, a string representing the XLA backend: ``'cpu'``, ``'gpu'``, or 
      ``'tpu'``. 
    inline: Specify whether this function should be inlined into enclosing 
      jaxprs (rather than being represented as an application of the xla_call 
      primitive with its own subjaxpr). Default False. 
 
  Returns: 
    A wrapped version of ``fun``, set up for just-in-time compilation. 
 
  Examples: 
    In the following example, ``selu`` can be compiled into a single fused kernel 
    by XLA: 
 
    &gt;&gt;&gt; import jax 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; @jax.jit 
    ... def selu(x, alpha=1.67, lmbda=1.05): 
    ...   return lmbda * jax.numpy.where(x &gt; 0, x, alpha * jax.numpy.exp(x) - alpha) 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; key = jax.random.PRNGKey(0) 
    &gt;&gt;&gt; x = jax.random.normal(key, (10,)) 
    &gt;&gt;&gt; print(selu(x))  # doctest: +SKIP 
    [-0.54485  0.27744 -0.29255 -0.91421 -0.62452 -0.24748 
    -0.85743 -0.78232  0.76827  0.59566 ] 
 
    To pass arguments such as ``static_argnames`` when decorating a function, a common 
    pattern is to use :func:`functools.partial`: 
 
    &gt;&gt;&gt; from functools import partial 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; @partial(jax.jit, static_argnames=['n']) 
    ... def g(x, n): 
    ...   for i in range(n): 
    ...     x = x ** 2 
    ...   return x 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; g(jnp.arange(4), 3) 
    Array([   0,    1,  256, 6561], dtype=int32) 
  &quot;&quot;&quot;</span>
  <span class="s1">(in_shardings</span><span class="s3">, </span><span class="s1">out_shardings</span><span class="s3">, </span><span class="s1">donate_argnums</span><span class="s3">, </span><span class="s1">static_argnums</span><span class="s3">,</span>
    <span class="s1">static_argnames) = pjit.pre_infer_params(</span>
        <span class="s1">fun</span><span class="s3">, </span><span class="s1">in_shardings</span><span class="s3">, </span><span class="s1">out_shardings</span><span class="s3">, </span><span class="s1">donate_argnums</span><span class="s3">,</span>
        <span class="s1">static_argnums</span><span class="s3">, </span><span class="s1">static_argnames</span><span class="s3">, </span><span class="s1">device</span><span class="s3">, </span><span class="s1">backend</span><span class="s3">, </span><span class="s1">abstracted_axes)</span>

  <span class="s3">def </span><span class="s1">infer_params(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">pjit_info_args = pjit.PjitInfo(</span>
        <span class="s1">fun=fun</span><span class="s3">, </span><span class="s1">in_shardings=in_shardings</span><span class="s3">,</span>
        <span class="s1">out_shardings=out_shardings</span><span class="s3">, </span><span class="s1">static_argnums=static_argnums</span><span class="s3">,</span>
        <span class="s1">static_argnames=static_argnames</span><span class="s3">, </span><span class="s1">donate_argnums=donate_argnums</span><span class="s3">,</span>
        <span class="s1">device=device</span><span class="s3">, </span><span class="s1">backend=backend</span><span class="s3">, </span><span class="s1">keep_unused=keep_unused</span><span class="s3">,</span>
        <span class="s1">inline=inline</span><span class="s3">, </span><span class="s1">resource_env=</span><span class="s3">None, </span><span class="s1">abstracted_axes=abstracted_axes)</span>
    <span class="s3">return </span><span class="s1">pjit.common_infer_params(pjit_info_args</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>

  <span class="s1">has_explicit_sharding = pjit._pjit_explicit_sharding(</span>
      <span class="s1">in_shardings</span><span class="s3">, </span><span class="s1">out_shardings</span><span class="s3">, </span><span class="s1">device</span><span class="s3">, </span><span class="s1">backend)</span>
  <span class="s3">return </span><span class="s1">pjit.post_infer_params(fun</span><span class="s3">, </span><span class="s1">infer_params</span><span class="s3">, </span><span class="s1">static_argnums</span><span class="s3">,</span>
                                <span class="s1">static_argnames</span><span class="s3">, </span><span class="s1">donate_argnums</span><span class="s3">,</span>
                                <span class="s1">abstracted_axes</span><span class="s3">, </span><span class="s1">has_explicit_sharding)</span>


<span class="s1">@contextmanager</span>
<span class="s3">def </span><span class="s1">disable_jit(disable: bool = </span><span class="s3">True</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Context manager that disables :py:func:`jit` behavior under its dynamic context. 
 
  For debugging it is useful to have a mechanism that disables :py:func:`jit` 
  everywhere in a dynamic context. Note that this not only disables explicit 
  uses of :func:`jit` by the user, but will also remove any implicit JIT compilation 
  used by the JAX library: this includes implicit JIT computation of `body` and 
  `cond` functions passed to higher-level primitives like :func:`~jax.lax.scan` and 
  :func:`~jax.lax.while_loop`, JIT used in implementations of :mod:`jax.numpy` functions, 
  and any other case where :func:`jit` is used within an API's implementation. 
 
  Values that have a data dependence on the arguments to a jitted function are 
  traced and abstracted. For example, an abstract value may be a 
  :py:class:`ShapedArray` instance, representing the set of all possible arrays 
  with a given shape and dtype, but not representing one concrete array with 
  specific values. You might notice those if you use a benign side-effecting 
  operation in a jitted function, like a print: 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; @jax.jit 
  ... def f(x): 
  ...   y = x * 2 
  ...   print(&quot;Value of y is&quot;, y) 
  ...   return y + 3 
  ... 
  &gt;&gt;&gt; print(f(jax.numpy.array([1, 2, 3])))  # doctest:+ELLIPSIS 
  Value of y is Traced&lt;ShapedArray(int32[3])&gt;with&lt;DynamicJaxprTrace...&gt; 
  [5 7 9] 
 
  Here ``y`` has been abstracted by :py:func:`jit` to a :py:class:`ShapedArray`, 
  which represents an array with a fixed shape and type but an arbitrary value. 
  The value of ``y`` is also traced. If we want to see a concrete value while 
  debugging, and avoid the tracer too, we can use the :py:func:`disable_jit` 
  context manager: 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; with jax.disable_jit(): 
  ...   print(f(jax.numpy.array([1, 2, 3]))) 
  ... 
  Value of y is [2 4 6] 
  [5 7 9] 
  &quot;&quot;&quot;</span>
  <span class="s3">with </span><span class="s1">_disable_jit(disable):</span>
    <span class="s3">yield</span>


<span class="s3">def </span><span class="s1">xla_computation(fun: Callable</span><span class="s3">,</span>
                    <span class="s1">static_argnums: Union[int</span><span class="s3">, </span><span class="s1">Iterable[int]] = ()</span><span class="s3">,</span>
                    <span class="s1">axis_env: Optional[Sequence[Tuple[AxisName</span><span class="s3">, </span><span class="s1">int]]] = </span><span class="s3">None,</span>
                    <span class="s1">in_parts=</span><span class="s3">None, </span><span class="s1">out_parts=</span><span class="s3">None,</span>
                    <span class="s1">backend: Optional[str] = </span><span class="s3">None,</span>
                    <span class="s1">tuple_args: bool = </span><span class="s3">False,</span>
                    <span class="s1">instantiate_const_outputs: Optional[bool] = </span><span class="s3">None,</span>
                    <span class="s1">return_shape: bool = </span><span class="s3">False,</span>
                    <span class="s1">donate_argnums: Union[int</span><span class="s3">, </span><span class="s1">Iterable[int]] = ()) -&gt; Callable:</span>
  <span class="s2">&quot;&quot;&quot;Creates a function that produces its XLA computation given example args. 
 
  Args: 
    fun: Function from which to form XLA computations. 
    static_argnums: See the :py:func:`jax.jit` docstring. 
    axis_env: Optional, a sequence of pairs where the first element is an axis 
      name and the second element is a positive integer representing the size of 
      the mapped axis with that name. This parameter is useful when lowering 
      functions that involve parallel communication collectives, and it 
      specifies the axis name/size environment that would be set up by 
      applications of :py:func:`jax.pmap`. See the examples below. 
    in_parts: Optional, how each argument to ``fun`` should be partitioned or 
      replicated. This is used to specify partitioned XLA computations, see 
      ``sharded_jit`` for more info. 
    out_parts: Optional, how each output of ``fun`` should be partitioned or 
      replicated. This is used to specify partitioned XLA computations, see 
      ``sharded_jit`` for more info. 
    backend: This is an experimental feature and the API is likely to change. 
      Optional, a string representing the XLA backend: ``'cpu'``, ``'gpu'``, or 
      ``'tpu'``. 
    tuple_args: Optional bool, defaults to ``False``. If ``True``, the resulting 
      XLA computation will have a single tuple argument that is unpacked into 
      the specified function arguments. If `None`, tupling will be enabled when 
      there are more than 100 arguments, since some platforms have limits on 
      argument arity. 
    instantiate_const_outputs: Deprecated argument, does nothing. 
    return_shape: Optional boolean, defaults to ``False``. If ``True``, the 
      wrapped function returns a pair where the first element is the XLA 
      computation and the second element is a pytree with the same structure as 
      the output of ``fun`` and where the leaves are objects with ``shape``, 
      ``dtype``, and ``named_shape`` attributes representing the corresponding 
      types of the output leaves. 
    donate_argnums: Specify which arguments are &quot;donated&quot; to the computation. 
      It is safe to donate arguments if you no longer need them once the 
      computation has finished. In some cases XLA can make use of donated 
      buffers to reduce the amount of memory needed to perform a computation, 
      for example recycling one of your input buffers to store a result. You 
      should not reuse buffers that you donate to a computation, JAX will raise 
      an error if you try to. 
 
  Returns: 
    A wrapped version of ``fun`` that when applied to example arguments returns 
    a built XLA Computation (see xla_client.py), from which representations of 
    the unoptimized XLA HLO computation can be extracted using methods like 
    ``as_hlo_text``, ``as_serialized_hlo_module_proto``, and 
    ``as_hlo_dot_graph``. If the argument ``return_shape`` is ``True``, then the 
    wrapped function returns a pair where the first element is the XLA 
    Computation and the second element is a pytree representing the structure, 
    shapes, dtypes, and named shapes of the output of ``fun``. 
 
    Concrete example arguments are not always necessary. For those arguments not 
    indicated by ``static_argnums``, any object with ``shape`` and ``dtype`` 
    attributes is acceptable (excepting namedtuples, which are treated as Python 
    containers). 
 
  For example: 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; def f(x): return jax.numpy.sin(jax.numpy.cos(x)) 
  &gt;&gt;&gt; c = jax.xla_computation(f)(3.) 
  &gt;&gt;&gt; print(c.as_hlo_text())  # doctest: +SKIP 
  HloModule xla_computation_f.6 
  &lt;BLANKLINE&gt; 
  ENTRY xla_computation_f.6 { 
    constant.2 = pred[] constant(false) 
    parameter.1 = f32[] parameter(0) 
    cosine.3 = f32[] cosine(parameter.1) 
    sine.4 = f32[] sine(cosine.3) 
    ROOT tuple.5 = (f32[]) tuple(sine.4) 
  } 
  &lt;BLANKLINE&gt; 
  &lt;BLANKLINE&gt; 
 
 
  Alternatively, the assignment to ``c`` above could be written: 
 
  &gt;&gt;&gt; import types 
  &gt;&gt;&gt; scalar = types.SimpleNamespace(shape=(), dtype=np.dtype(np.float32)) 
  &gt;&gt;&gt; c = jax.xla_computation(f)(scalar) 
 
 
  Here's an example that involves a parallel collective and axis name: 
 
  &gt;&gt;&gt; def f(x): return x - jax.lax.psum(x, 'i') 
  &gt;&gt;&gt; c = jax.xla_computation(f, axis_env=[('i', 4)])(2) 
  &gt;&gt;&gt; print(c.as_hlo_text())  # doctest: +SKIP 
  HloModule jaxpr_computation.9 
  primitive_computation.3 { 
    parameter.4 = s32[] parameter(0) 
    parameter.5 = s32[] parameter(1) 
    ROOT add.6 = s32[] add(parameter.4, parameter.5) 
  } 
  ENTRY jaxpr_computation.9 { 
    tuple.1 = () tuple() 
    parameter.2 = s32[] parameter(0) 
    all-reduce.7 = s32[] all-reduce(parameter.2), replica_groups={{0,1,2,3}}, to_apply=primitive_computation.3 
    ROOT subtract.8 = s32[] subtract(parameter.2, all-reduce.7) 
  } 
  &lt;BLANKLINE&gt; 
  &lt;BLANKLINE&gt; 
 
  Notice the ``replica_groups`` that were generated. Here's an example that 
  generates more interesting ``replica_groups``: 
 
  &gt;&gt;&gt; from jax import lax 
  &gt;&gt;&gt; def g(x): 
  ...   rowsum = lax.psum(x, 'i') 
  ...   colsum = lax.psum(x, 'j') 
  ...   allsum = lax.psum(x, ('i', 'j')) 
  ...   return rowsum, colsum, allsum 
  ... 
  &gt;&gt;&gt; axis_env = [('i', 4), ('j', 2)] 
  &gt;&gt;&gt; c = xla_computation(g, axis_env=axis_env)(5.) 
  &gt;&gt;&gt; print(c.as_hlo_text())  # doctest: +SKIP 
  HloModule jaxpr_computation__1.19 
  [removed uninteresting text here] 
  ENTRY jaxpr_computation__1.19 { 
    tuple.1 = () tuple() 
    parameter.2 = f32[] parameter(0) 
    all-reduce.7 = f32[] all-reduce(parameter.2), replica_groups={{0,2,4,6},{1,3,5,7}}, to_apply=primitive_computation__1.3 
    all-reduce.12 = f32[] all-reduce(parameter.2), replica_groups={{0,1},{2,3},{4,5},{6,7}}, to_apply=primitive_computation__1.8 
    all-reduce.17 = f32[] all-reduce(parameter.2), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=primitive_computation__1.13 
    ROOT tuple.18 = (f32[], f32[], f32[]) tuple(all-reduce.7, all-reduce.12, all-reduce.17) 
  } 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">instantiate_const_outputs </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">&quot;instantiate_const_outputs has been deprecated. Please use the ahead of&quot;</span>
        <span class="s4">&quot; time APIs. You can read more here:&quot;</span>
        <span class="s4">&quot; https://jax.readthedocs.io/en/latest/aot.html&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">in_parts </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">&quot;in_parts has been deprecated. Please use the ahead of time APIs. You&quot;</span>
        <span class="s4">&quot; can read more here: https://jax.readthedocs.io/en/latest/aot.html&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">out_parts </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">&quot;out_parts has been deprecated. Please use the ahead of time APIs. You&quot;</span>
        <span class="s4">&quot; can read more here: https://jax.readthedocs.io/en/latest/aot.html&quot;</span><span class="s1">)</span>

  <span class="s1">check_callable(fun)</span>
  <span class="s1">static_argnums = _ensure_index_tuple(static_argnums)</span>
  <span class="s1">donate_argnums = _ensure_index_tuple(donate_argnums)</span>
  <span class="s1">donate_argnums = rebase_donate_argnums(donate_argnums</span><span class="s3">, </span><span class="s1">static_argnums)</span>

  <span class="s1">fun_name = getattr(fun</span><span class="s3">, </span><span class="s4">&quot;__name__&quot;</span><span class="s3">, </span><span class="s4">&quot;unknown&quot;</span><span class="s1">)</span>

  <span class="s1">platform = backend </span><span class="s3">if </span><span class="s1">backend </span><span class="s3">is not None else </span><span class="s1">xb.get_backend().platform</span>

  <span class="s3">def </span><span class="s1">make_axis_env(nreps):</span>
    <span class="s3">if </span><span class="s1">axis_env </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">sharding_impls.AxisEnv(nreps</span><span class="s3">, </span><span class="s1">()</span><span class="s3">, </span><span class="s1">())</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">nreps = nreps * math.prod(size </span><span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">size </span><span class="s3">in </span><span class="s1">axis_env)</span>
      <span class="s1">names</span><span class="s3">, </span><span class="s1">sizes = unzip2(axis_env)</span>
      <span class="s3">return </span><span class="s1">sharding_impls.AxisEnv(nreps</span><span class="s3">, </span><span class="s1">names</span><span class="s3">, </span><span class="s1">sizes)</span>

  <span class="s1">@wraps(fun)</span>
  <span class="s1">@api_boundary</span>
  <span class="s3">def </span><span class="s1">computation_maker(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s3">if </span><span class="s1">max(static_argnums + donate_argnums</span><span class="s3">, </span><span class="s1">default=-</span><span class="s5">1</span><span class="s1">) &gt;= len(args):</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;jitted function has </span><span class="s3">{</span><span class="s1">static_argnums=</span><span class="s3">}</span><span class="s4">, </span><span class="s3">{</span><span class="s1">donate_argnums=</span><span class="s3">} </span><span class="s4">but &quot;</span>
                       <span class="s4">f&quot;was called with only </span><span class="s3">{</span><span class="s1">len(args)</span><span class="s3">} </span><span class="s4">positional arguments.&quot;</span><span class="s1">)</span>

    <span class="s1">f = lu.wrap_init(fun)</span>
    <span class="s1">f</span><span class="s3">, </span><span class="s1">dyn_args = argnums_partial_except(f</span><span class="s3">, </span><span class="s1">static_argnums</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">allow_invalid=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">args_flat</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten((dyn_args</span><span class="s3">, </span><span class="s1">kwargs))</span>
    <span class="s3">if </span><span class="s1">donate_argnums:</span>
      <span class="s1">donated_invars = donation_vector(donate_argnums</span><span class="s3">, </span><span class="s1">dyn_args</span><span class="s3">, </span><span class="s1">kwargs)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">donated_invars = (</span><span class="s3">False,</span><span class="s1">) * len(args_flat)</span>

    <span class="s1">jaxtree_fun</span><span class="s3">, </span><span class="s1">out_tree = flatten_fun(f</span><span class="s3">, </span><span class="s1">in_tree)</span>
    <span class="s1">avals = map(shaped_abstractify</span><span class="s3">, </span><span class="s1">args_flat)</span>
    <span class="s3">with </span><span class="s1">ExitStack() </span><span class="s3">as </span><span class="s1">stack:</span>
      <span class="s3">for </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">size </span><span class="s3">in </span><span class="s1">axis_env </span><span class="s3">or </span><span class="s1">[]:</span>
        <span class="s1">stack.enter_context(core.extend_axis_env(axis_name</span><span class="s3">, </span><span class="s1">size</span><span class="s3">, None</span><span class="s1">))</span>
      <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">out_avals</span><span class="s3">, </span><span class="s1">consts = pe.trace_to_jaxpr_dynamic(jaxtree_fun</span><span class="s3">, </span><span class="s1">avals)</span>
      <span class="s1">jaxpr = dispatch.apply_outfeed_rewriter(jaxpr)</span>
      <span class="s1">axis_env_ = make_axis_env(dispatch.jaxpr_replicas(jaxpr))</span>
      <span class="s1">ordered_effects = list(</span>
          <span class="s1">effects.ordered_effects.filter_in(jaxpr.effects))</span>
      <span class="s1">lowering_result = mlir.lower_jaxpr_to_module(</span>
          <span class="s4">f&quot;xla_computation_</span><span class="s3">{</span><span class="s1">fun_name</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s3">,</span>
          <span class="s1">core.ClosedJaxpr(jaxpr</span><span class="s3">, </span><span class="s1">consts)</span><span class="s3">,</span>
          <span class="s1">ordered_effects=ordered_effects</span><span class="s3">,</span>
          <span class="s1">backend_or_name=backend</span><span class="s3">,</span>
          <span class="s1">platform=platform</span><span class="s3">,</span>
          <span class="s1">axis_context=sharding_impls.ReplicaAxisContext(axis_env_)</span><span class="s3">,</span>
          <span class="s1">name_stack=source_info_util.new_name_stack(</span>
              <span class="s1">wrap_name(fun_name</span><span class="s3">, </span><span class="s4">&quot;xla_computation&quot;</span><span class="s1">))</span><span class="s3">,</span>
          <span class="s1">donated_args=donated_invars</span><span class="s3">,</span>
          <span class="s1">arg_shardings=</span><span class="s3">None,</span>
          <span class="s1">result_shardings=</span><span class="s3">None</span><span class="s1">)</span>
      <span class="s1">built = xc._xla.mlir.mlir_module_to_xla_computation(</span>
          <span class="s1">mlir.module_to_string(lowering_result.module)</span><span class="s3">,</span>
          <span class="s1">use_tuple_args=tuple_args</span><span class="s3">,</span>
          <span class="s1">return_tuple=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">out_shapes_flat = [</span>
        <span class="s1">ShapeDtypeStruct(a.shape</span><span class="s3">, </span><span class="s1">a.dtype</span><span class="s3">, </span><span class="s1">a.named_shape) </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">out_avals]</span>
    <span class="s1">out_shape = tree_unflatten(out_tree()</span><span class="s3">, </span><span class="s1">out_shapes_flat)</span>
    <span class="s3">for </span><span class="s1">out_aval </span><span class="s3">in </span><span class="s1">out_avals:</span>
      <span class="s3">if not </span><span class="s1">isinstance(out_aval</span><span class="s3">, </span><span class="s1">ShapedArray):</span>
        <span class="s3">raise </span><span class="s1">RuntimeError(</span><span class="s4">&quot;As we want to propagate the weak_type, we need &quot;</span>
                           <span class="s4">&quot;to get a ShapedArray, otherwise this &quot;</span>
                           <span class="s4">&quot;information is lost&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">return_shape:</span>
      <span class="s3">return </span><span class="s1">built</span><span class="s3">, </span><span class="s1">out_shape</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">built</span>

  <span class="s3">return </span><span class="s1">computation_maker</span>

<span class="s3">def </span><span class="s1">grad(fun: Callable</span><span class="s3">, </span><span class="s1">argnums: Union[int</span><span class="s3">, </span><span class="s1">Sequence[int]] = </span><span class="s5">0</span><span class="s3">,</span>
         <span class="s1">has_aux: bool = </span><span class="s3">False, </span><span class="s1">holomorphic: bool = </span><span class="s3">False,</span>
         <span class="s1">allow_int: bool = </span><span class="s3">False,</span>
         <span class="s1">reduce_axes: Sequence[AxisName] = ()) -&gt; Callable:</span>
  <span class="s2">&quot;&quot;&quot;Creates a function that evaluates the gradient of ``fun``. 
 
  Args: 
    fun: Function to be differentiated. Its arguments at positions specified by 
      ``argnums`` should be arrays, scalars, or standard Python containers. 
      Argument arrays in the positions specified by ``argnums`` must be of 
      inexact (i.e., floating-point or complex) type. It 
      should return a scalar (which includes arrays with shape ``()`` but not 
      arrays with shape ``(1,)`` etc.) 
    argnums: Optional, integer or sequence of integers. Specifies which 
      positional argument(s) to differentiate with respect to (default 0). 
    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the 
      first element is considered the output of the mathematical function to be 
      differentiated and the second element is auxiliary data. Default False. 
    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be 
      holomorphic. If True, inputs and outputs must be complex. Default False. 
    allow_int: Optional, bool. Whether to allow differentiating with 
      respect to integer valued inputs. The gradient of an integer input will 
      have a trivial vector-space dtype (float0). Default False. 
    reduce_axes: Optional, tuple of axis names. If an axis is listed here, and 
      ``fun`` implicitly broadcasts a value over that axis, the backward pass 
      will perform a ``psum`` of the corresponding gradient. Otherwise, the 
      gradient will be per-example over named axes. For example, if ``'batch'`` 
      is a named batch axis, ``grad(f, reduce_axes=('batch',))`` will create a 
      function that computes the total gradient while ``grad(f)`` will create 
      one that computes the per-example gradient. 
 
  Returns: 
    A function with the same arguments as ``fun``, that evaluates the gradient 
    of ``fun``. If ``argnums`` is an integer then the gradient has the same 
    shape and type as the positional argument indicated by that integer. If 
    argnums is a tuple of integers, the gradient is a tuple of values with the 
    same shapes and types as the corresponding arguments. If ``has_aux`` is True 
    then a pair of (gradient, auxiliary_data) is returned. 
 
  For example: 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; grad_tanh = jax.grad(jax.numpy.tanh) 
  &gt;&gt;&gt; print(grad_tanh(0.2)) 
  0.961043 
  &quot;&quot;&quot;</span>
  <span class="s1">value_and_grad_f = value_and_grad(fun</span><span class="s3">, </span><span class="s1">argnums</span><span class="s3">, </span><span class="s1">has_aux=has_aux</span><span class="s3">,</span>
                                    <span class="s1">holomorphic=holomorphic</span><span class="s3">,</span>
                                    <span class="s1">allow_int=allow_int</span><span class="s3">,</span>
                                    <span class="s1">reduce_axes=reduce_axes)</span>

  <span class="s1">docstr = (</span><span class="s4">&quot;Gradient of {fun} with respect to positional argument(s) &quot;</span>
            <span class="s4">&quot;{argnums}. Takes the same arguments as {fun} but returns the &quot;</span>
            <span class="s4">&quot;gradient, which has the same shape as the arguments at &quot;</span>
            <span class="s4">&quot;positions {argnums}.&quot;</span><span class="s1">)</span>

  <span class="s1">@wraps(fun</span><span class="s3">, </span><span class="s1">docstr=docstr</span><span class="s3">, </span><span class="s1">argnums=argnums)</span>
  <span class="s1">@api_boundary</span>
  <span class="s3">def </span><span class="s1">grad_f(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">g = value_and_grad_f(*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>
    <span class="s3">return </span><span class="s1">g</span>

  <span class="s1">@wraps(fun</span><span class="s3">, </span><span class="s1">docstr=docstr</span><span class="s3">, </span><span class="s1">argnums=argnums)</span>
  <span class="s1">@api_boundary</span>
  <span class="s3">def </span><span class="s1">grad_f_aux(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">(_</span><span class="s3">, </span><span class="s1">aux)</span><span class="s3">, </span><span class="s1">g = value_and_grad_f(*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>
    <span class="s3">return </span><span class="s1">g</span><span class="s3">, </span><span class="s1">aux</span>

  <span class="s3">return </span><span class="s1">grad_f_aux </span><span class="s3">if </span><span class="s1">has_aux </span><span class="s3">else </span><span class="s1">grad_f</span>

<span class="s3">def </span><span class="s1">value_and_grad(fun: Callable</span><span class="s3">, </span><span class="s1">argnums: Union[int</span><span class="s3">, </span><span class="s1">Sequence[int]] = </span><span class="s5">0</span><span class="s3">,</span>
                   <span class="s1">has_aux: bool = </span><span class="s3">False, </span><span class="s1">holomorphic: bool = </span><span class="s3">False,</span>
                   <span class="s1">allow_int: bool = </span><span class="s3">False, </span><span class="s1">reduce_axes: Sequence[AxisName] = ()</span>
  <span class="s1">) -&gt; Callable[...</span><span class="s3">, </span><span class="s1">Tuple[Any</span><span class="s3">, </span><span class="s1">Any]]:</span>
  <span class="s2">&quot;&quot;&quot;Create a function that evaluates both ``fun`` and the gradient of ``fun``. 
 
  Args: 
    fun: Function to be differentiated. Its arguments at positions specified by 
      ``argnums`` should be arrays, scalars, or standard Python containers. It 
      should return a scalar (which includes arrays with shape ``()`` but not 
      arrays with shape ``(1,)`` etc.) 
    argnums: Optional, integer or sequence of integers. Specifies which 
      positional argument(s) to differentiate with respect to (default 0). 
    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the 
      first element is considered the output of the mathematical function to be 
      differentiated and the second element is auxiliary data. Default False. 
    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be 
      holomorphic. If True, inputs and outputs must be complex. Default False. 
    allow_int: Optional, bool. Whether to allow differentiating with 
      respect to integer valued inputs. The gradient of an integer input will 
      have a trivial vector-space dtype (float0). Default False. 
    reduce_axes: Optional, tuple of axis names. If an axis is listed here, and 
      ``fun`` implicitly broadcasts a value over that axis, the backward pass 
      will perform a ``psum`` of the corresponding gradient. Otherwise, the 
      gradient will be per-example over named axes. For example, if ``'batch'`` 
      is a named batch axis, ``value_and_grad(f, reduce_axes=('batch',))`` will 
      create a function that computes the total gradient while 
      ``value_and_grad(f)`` will create one that computes the per-example 
      gradient. 
 
  Returns: 
    A function with the same arguments as ``fun`` that evaluates both ``fun`` 
    and the gradient of ``fun`` and returns them as a pair (a two-element 
    tuple). If ``argnums`` is an integer then the gradient has the same shape 
    and type as the positional argument indicated by that integer. If argnums is 
    a sequence of integers, the gradient is a tuple of values with the same 
    shapes and types as the corresponding arguments. If ``has_aux`` is True 
    then a tuple of ((value, auxiliary_data), gradient) is returned. 
  &quot;&quot;&quot;</span>

  <span class="s1">docstr = (</span><span class="s4">&quot;Value and gradient of {fun} with respect to positional &quot;</span>
            <span class="s4">&quot;argument(s) {argnums}. Takes the same arguments as {fun} but &quot;</span>
            <span class="s4">&quot;returns a two-element tuple where the first element is the value &quot;</span>
            <span class="s4">&quot;of {fun} and the second element is the gradient, which has the &quot;</span>
            <span class="s4">&quot;same shape as the arguments at positions {argnums}.&quot;</span><span class="s1">)</span>

  <span class="s1">check_callable(fun)</span>
  <span class="s1">argnums = core.concrete_or_error(_ensure_index</span><span class="s3">, </span><span class="s1">argnums)</span>
  <span class="s1">reduce_axes = _ensure_str_tuple(reduce_axes)  </span><span class="s0"># type: ignore</span>

  <span class="s1">@wraps(fun</span><span class="s3">, </span><span class="s1">docstr=docstr</span><span class="s3">, </span><span class="s1">argnums=argnums)</span>
  <span class="s1">@api_boundary</span>
  <span class="s3">def </span><span class="s1">value_and_grad_f(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">max_argnum = argnums </span><span class="s3">if </span><span class="s1">isinstance(argnums</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">else </span><span class="s1">max(argnums)</span>
    <span class="s3">if </span><span class="s1">max_argnum &gt;= len(args):</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;differentiating with respect to </span><span class="s3">{</span><span class="s1">argnums=</span><span class="s3">} </span><span class="s4">requires at least &quot;</span>
                      <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">max_argnum + </span><span class="s5">1</span><span class="s3">} </span><span class="s4">positional arguments to be passed by the caller, &quot;</span>
                      <span class="s4">f&quot;but got only </span><span class="s3">{</span><span class="s1">len(args)</span><span class="s3">} </span><span class="s4">positional arguments.&quot;</span><span class="s1">)</span>

    <span class="s1">f = lu.wrap_init(fun</span><span class="s3">, </span><span class="s1">kwargs)</span>
    <span class="s1">f_partial</span><span class="s3">, </span><span class="s1">dyn_args = argnums_partial(f</span><span class="s3">, </span><span class="s1">argnums</span><span class="s3">, </span><span class="s1">args</span><span class="s3">,</span>
                                          <span class="s1">require_static_args_hashable=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s3">for </span><span class="s1">leaf </span><span class="s3">in </span><span class="s1">tree_leaves(dyn_args):</span>
      <span class="s1">_check_input_dtype_grad(holomorphic</span><span class="s3">, </span><span class="s1">allow_int</span><span class="s3">, </span><span class="s1">leaf)</span>
    <span class="s3">if not </span><span class="s1">has_aux:</span>
      <span class="s1">ans</span><span class="s3">, </span><span class="s1">vjp_py = _vjp(f_partial</span><span class="s3">, </span><span class="s1">*dyn_args</span><span class="s3">, </span><span class="s1">reduce_axes=reduce_axes)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">ans</span><span class="s3">, </span><span class="s1">vjp_py</span><span class="s3">, </span><span class="s1">aux = _vjp(</span>
          <span class="s1">f_partial</span><span class="s3">, </span><span class="s1">*dyn_args</span><span class="s3">, </span><span class="s1">has_aux=</span><span class="s3">True, </span><span class="s1">reduce_axes=reduce_axes)</span>
    <span class="s1">_check_scalar(ans)</span>
    <span class="s1">tree_map(partial(_check_output_dtype_grad</span><span class="s3">, </span><span class="s1">holomorphic)</span><span class="s3">, </span><span class="s1">ans)</span>
    <span class="s1">g = vjp_py(lax_internal._one(ans))</span>
    <span class="s1">g = g[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">isinstance(argnums</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">else </span><span class="s1">g</span>
    <span class="s3">if not </span><span class="s1">has_aux:</span>
      <span class="s3">return </span><span class="s1">ans</span><span class="s3">, </span><span class="s1">g</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">(ans</span><span class="s3">, </span><span class="s1">aux)</span><span class="s3">, </span><span class="s1">g</span>

  <span class="s3">return </span><span class="s1">value_and_grad_f</span>

<span class="s3">def </span><span class="s1">_check_scalar(x):</span>
  <span class="s1">msg = </span><span class="s4">&quot;Gradient only defined for scalar-output functions. Output {}.&quot;</span><span class="s1">.format</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">aval = core.get_aval(x)</span>
  <span class="s3">except </span><span class="s1">TypeError </span><span class="s3">as </span><span class="s1">e:</span>
    <span class="s3">raise </span><span class="s1">TypeError(msg(</span><span class="s4">f&quot;was </span><span class="s3">{</span><span class="s1">x</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)) </span><span class="s3">from </span><span class="s1">e</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">isinstance(aval</span><span class="s3">, </span><span class="s1">ShapedArray):</span>
      <span class="s3">if </span><span class="s1">aval.shape != ():</span>
        <span class="s3">raise </span><span class="s1">TypeError(msg(</span><span class="s4">f&quot;had shape: </span><span class="s3">{</span><span class="s1">aval.shape</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">TypeError(msg(</span><span class="s4">f&quot;had abstract value </span><span class="s3">{</span><span class="s1">aval</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">))</span>

<span class="s3">def </span><span class="s1">_check_input_dtype_revderiv(name</span><span class="s3">, </span><span class="s1">holomorphic</span><span class="s3">, </span><span class="s1">allow_int</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s1">dispatch.check_arg(x)</span>
  <span class="s1">aval = core.get_aval(x)</span>
  <span class="s3">if </span><span class="s1">holomorphic:</span>
    <span class="s3">if not </span><span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.complexfloating):</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">with holomorphic=True requires inputs with complex dtype, &quot;</span>
                      <span class="s4">f&quot;but got </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">(dtypes.is_opaque_dtype(aval.dtype) </span><span class="s3">or</span>
      <span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.integer) </span><span class="s3">or</span>
      <span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.bool_)):</span>
    <span class="s3">if not </span><span class="s1">allow_int:</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">requires real- or complex-valued inputs (input dtype &quot;</span>
                      <span class="s4">f&quot;that is a sub-dtype of np.inexact), but got </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">. &quot;</span>
                      <span class="s4">&quot;If you want to use Boolean- or integer-valued inputs, use vjp &quot;</span>
                      <span class="s4">&quot;or set allow_int to True.&quot;</span><span class="s1">)</span>
  <span class="s3">elif not </span><span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.inexact):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">requires numerical-valued inputs (input dtype that is a &quot;</span>
                    <span class="s4">f&quot;sub-dtype of np.bool_ or np.number), but got </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
<span class="s1">_check_input_dtype_grad = partial(_check_input_dtype_revderiv</span><span class="s3">, </span><span class="s4">&quot;grad&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_check_output_dtype_revderiv(name</span><span class="s3">, </span><span class="s1">holomorphic</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s1">aval = core.get_aval(x)</span>
  <span class="s3">if </span><span class="s1">dtypes.is_opaque_dtype(aval.dtype):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span>
        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">with output element type </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">holomorphic:</span>
    <span class="s3">if not </span><span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.complexfloating):</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">with holomorphic=True requires outputs with complex dtype, &quot;</span>
                      <span class="s4">f&quot;but got </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
  <span class="s3">elif </span><span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.complexfloating):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">requires real-valued outputs (output dtype that is &quot;</span>
                    <span class="s4">f&quot;a sub-dtype of np.floating), but got </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">. &quot;</span>
                    <span class="s4">&quot;For holomorphic differentiation, pass holomorphic=True. &quot;</span>
                    <span class="s4">&quot;For differentiation of non-holomorphic functions involving complex &quot;</span>
                    <span class="s4">&quot;outputs, use jax.vjp directly.&quot;</span><span class="s1">)</span>
  <span class="s3">elif not </span><span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.floating):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">requires real-valued outputs (output dtype that is &quot;</span>
                    <span class="s4">f&quot;a sub-dtype of np.floating), but got </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">. &quot;</span>
                    <span class="s4">&quot;For differentiation of functions with integer outputs, use &quot;</span>
                    <span class="s4">&quot;jax.vjp directly.&quot;</span><span class="s1">)</span>
<span class="s1">_check_output_dtype_grad = partial(_check_output_dtype_revderiv</span><span class="s3">, </span><span class="s4">&quot;grad&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">jacfwd(fun: Callable</span><span class="s3">, </span><span class="s1">argnums: Union[int</span><span class="s3">, </span><span class="s1">Sequence[int]] = </span><span class="s5">0</span><span class="s3">,</span>
           <span class="s1">has_aux: bool = </span><span class="s3">False, </span><span class="s1">holomorphic: bool = </span><span class="s3">False</span><span class="s1">) -&gt; Callable:</span>
  <span class="s2">&quot;&quot;&quot;Jacobian of ``fun`` evaluated column-by-column using forward-mode AD. 
 
  Args: 
    fun: Function whose Jacobian is to be computed. 
    argnums: Optional, integer or sequence of integers. Specifies which 
      positional argument(s) to differentiate with respect to (default ``0``). 
    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the 
      first element is considered the output of the mathematical function to be 
      differentiated and the second element is auxiliary data. Default False. 
    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be 
      holomorphic. Default False. 
 
  Returns: 
    A function with the same arguments as ``fun``, that evaluates the Jacobian of 
    ``fun`` using forward-mode automatic differentiation. If ``has_aux`` is True 
    then a pair of (jacobian, auxiliary_data) is returned. 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; import jax.numpy as jnp 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; def f(x): 
  ...   return jnp.asarray( 
  ...     [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])]) 
  ... 
  &gt;&gt;&gt; print(jax.jacfwd(f)(jnp.array([1., 2., 3.]))) 
  [[ 1.       0.       0.     ] 
   [ 0.       0.       5.     ] 
   [ 0.      16.      -2.     ] 
   [ 1.6209   0.       0.84147]] 
  &quot;&quot;&quot;</span>
  <span class="s1">check_callable(fun)</span>
  <span class="s1">argnums = _ensure_index(argnums)</span>

  <span class="s1">docstr = (</span><span class="s4">&quot;Jacobian of {fun} with respect to positional argument(s) &quot;</span>
            <span class="s4">&quot;{argnums}. Takes the same arguments as {fun} but returns the &quot;</span>
            <span class="s4">&quot;jacobian of the output with respect to the arguments at &quot;</span>
            <span class="s4">&quot;positions {argnums}.&quot;</span><span class="s1">)</span>

  <span class="s1">@wraps(fun</span><span class="s3">, </span><span class="s1">docstr=docstr</span><span class="s3">, </span><span class="s1">argnums=argnums)</span>
  <span class="s3">def </span><span class="s1">jacfun(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">f = lu.wrap_init(fun</span><span class="s3">, </span><span class="s1">kwargs)</span>
    <span class="s1">f_partial</span><span class="s3">, </span><span class="s1">dyn_args = argnums_partial(f</span><span class="s3">, </span><span class="s1">argnums</span><span class="s3">, </span><span class="s1">args</span><span class="s3">,</span>
                                          <span class="s1">require_static_args_hashable=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">tree_map(partial(_check_input_dtype_jacfwd</span><span class="s3">, </span><span class="s1">holomorphic)</span><span class="s3">, </span><span class="s1">dyn_args)</span>
    <span class="s3">if not </span><span class="s1">has_aux:</span>
      <span class="s1">pushfwd: Callable = partial(_jvp</span><span class="s3">, </span><span class="s1">f_partial</span><span class="s3">, </span><span class="s1">dyn_args)</span>
      <span class="s1">y</span><span class="s3">, </span><span class="s1">jac = vmap(pushfwd</span><span class="s3">, </span><span class="s1">out_axes=(</span><span class="s3">None, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))(_std_basis(dyn_args))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">pushfwd: Callable = partial(_jvp</span><span class="s3">, </span><span class="s1">f_partial</span><span class="s3">, </span><span class="s1">dyn_args</span><span class="s3">, </span><span class="s1">has_aux=</span><span class="s3">True</span><span class="s1">)</span>
      <span class="s1">y</span><span class="s3">, </span><span class="s1">jac</span><span class="s3">, </span><span class="s1">aux = vmap(pushfwd</span><span class="s3">, </span><span class="s1">out_axes=(</span><span class="s3">None, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, None</span><span class="s1">))(_std_basis(dyn_args))</span>
    <span class="s1">tree_map(partial(_check_output_dtype_jacfwd</span><span class="s3">, </span><span class="s1">holomorphic)</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">example_args = dyn_args[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">isinstance(argnums</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">else </span><span class="s1">dyn_args</span>
    <span class="s1">jac_tree = tree_map(partial(_jacfwd_unravel</span><span class="s3">, </span><span class="s1">example_args)</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">jac)</span>
    <span class="s3">if not </span><span class="s1">has_aux:</span>
      <span class="s3">return </span><span class="s1">jac_tree</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">jac_tree</span><span class="s3">, </span><span class="s1">aux</span>

  <span class="s3">return </span><span class="s1">jacfun</span>

<span class="s3">def </span><span class="s1">_check_input_dtype_jacfwd(holomorphic: bool</span><span class="s3">, </span><span class="s1">x: Any) -&gt; </span><span class="s3">None</span><span class="s1">:</span>
  <span class="s1">dispatch.check_arg(x)</span>
  <span class="s1">aval = core.get_aval(x)</span>
  <span class="s3">if </span><span class="s1">dtypes.is_opaque_dtype(aval.dtype):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span>
        <span class="s4">f&quot;jacfwd with input element type </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">holomorphic:</span>
    <span class="s3">if not </span><span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.complexfloating):</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;jacfwd with holomorphic=True requires inputs with complex &quot;</span>
                      <span class="s4">f&quot;dtype, but got </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
  <span class="s3">elif not </span><span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.floating):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;jacfwd requires real-valued inputs (input dtype that is &quot;</span>
                    <span class="s4">f&quot;a sub-dtype of np.floating), but got </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">. &quot;</span>
                    <span class="s4">&quot;For holomorphic differentiation, pass holomorphic=True. &quot;</span>
                    <span class="s4">&quot;For differentiation of non-holomorphic functions involving &quot;</span>
                    <span class="s4">&quot;complex inputs or integer inputs, use jax.jvp directly.&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_check_output_dtype_jacfwd(holomorphic</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s1">aval = core.get_aval(x)</span>
  <span class="s3">if </span><span class="s1">holomorphic:</span>
    <span class="s3">if not </span><span class="s1">dtypes.issubdtype(aval.dtype</span><span class="s3">, </span><span class="s1">np.complexfloating):</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;jacfwd with holomorphic=True requires outputs with complex dtype, &quot;</span>
                      <span class="s4">f&quot;but got </span><span class="s3">{</span><span class="s1">aval.dtype.name</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">jacrev(fun: Callable</span><span class="s3">, </span><span class="s1">argnums: Union[int</span><span class="s3">, </span><span class="s1">Sequence[int]] = </span><span class="s5">0</span><span class="s3">,</span>
           <span class="s1">has_aux: bool = </span><span class="s3">False, </span><span class="s1">holomorphic: bool = </span><span class="s3">False, </span><span class="s1">allow_int: bool = </span><span class="s3">False</span><span class="s1">) -&gt; Callable:</span>
  <span class="s2">&quot;&quot;&quot;Jacobian of ``fun`` evaluated row-by-row using reverse-mode AD. 
 
  Args: 
    fun: Function whose Jacobian is to be computed. 
    argnums: Optional, integer or sequence of integers. Specifies which 
      positional argument(s) to differentiate with respect to (default ``0``). 
    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the 
      first element is considered the output of the mathematical function to be 
      differentiated and the second element is auxiliary data. Default False. 
    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be 
      holomorphic. Default False. 
    allow_int: Optional, bool. Whether to allow differentiating with 
      respect to integer valued inputs. The gradient of an integer input will 
      have a trivial vector-space dtype (float0). Default False. 
 
  Returns: 
    A function with the same arguments as ``fun``, that evaluates the Jacobian of 
    ``fun`` using reverse-mode automatic differentiation. If ``has_aux`` is True 
    then a pair of (jacobian, auxiliary_data) is returned. 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; import jax.numpy as jnp 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; def f(x): 
  ...   return jnp.asarray( 
  ...     [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])]) 
  ... 
  &gt;&gt;&gt; print(jax.jacrev(f)(jnp.array([1., 2., 3.]))) 
  [[ 1.       0.       0.     ] 
   [ 0.       0.       5.     ] 
   [ 0.      16.      -2.     ] 
   [ 1.6209   0.       0.84147]] 
  &quot;&quot;&quot;</span>
  <span class="s1">check_callable(fun)</span>

  <span class="s1">docstr = (</span><span class="s4">&quot;Jacobian of {fun} with respect to positional argument(s) &quot;</span>
            <span class="s4">&quot;{argnums}. Takes the same arguments as {fun} but returns the &quot;</span>
            <span class="s4">&quot;jacobian of the output with respect to the arguments at &quot;</span>
            <span class="s4">&quot;positions {argnums}.&quot;</span><span class="s1">)</span>

  <span class="s1">@wraps(fun</span><span class="s3">, </span><span class="s1">docstr=docstr</span><span class="s3">, </span><span class="s1">argnums=argnums)</span>
  <span class="s3">def </span><span class="s1">jacfun(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">f = lu.wrap_init(fun</span><span class="s3">, </span><span class="s1">kwargs)</span>
    <span class="s1">f_partial</span><span class="s3">, </span><span class="s1">dyn_args = argnums_partial(f</span><span class="s3">, </span><span class="s1">argnums</span><span class="s3">, </span><span class="s1">args</span><span class="s3">,</span>
                                          <span class="s1">require_static_args_hashable=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">tree_map(partial(_check_input_dtype_jacrev</span><span class="s3">, </span><span class="s1">holomorphic</span><span class="s3">, </span><span class="s1">allow_int)</span><span class="s3">, </span><span class="s1">dyn_args)</span>
    <span class="s3">if not </span><span class="s1">has_aux:</span>
      <span class="s1">y</span><span class="s3">, </span><span class="s1">pullback = _vjp(f_partial</span><span class="s3">, </span><span class="s1">*dyn_args)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">y</span><span class="s3">, </span><span class="s1">pullback</span><span class="s3">, </span><span class="s1">aux = _vjp(f_partial</span><span class="s3">, </span><span class="s1">*dyn_args</span><span class="s3">, </span><span class="s1">has_aux=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">tree_map(partial(_check_output_dtype_jacrev</span><span class="s3">, </span><span class="s1">holomorphic)</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">jac = vmap(pullback)(_std_basis(y))</span>
    <span class="s1">jac = jac[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">isinstance(argnums</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">else </span><span class="s1">jac</span>
    <span class="s1">example_args = dyn_args[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">isinstance(argnums</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">else </span><span class="s1">dyn_args</span>
    <span class="s1">jac_tree = tree_map(partial(_jacrev_unravel</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">, </span><span class="s1">example_args</span><span class="s3">, </span><span class="s1">jac)</span>
    <span class="s1">jac_tree = tree_transpose(tree_structure(example_args)</span><span class="s3">, </span><span class="s1">tree_structure(y)</span><span class="s3">, </span><span class="s1">jac_tree)</span>
    <span class="s3">if not </span><span class="s1">has_aux:</span>
      <span class="s3">return </span><span class="s1">jac_tree</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">jac_tree</span><span class="s3">, </span><span class="s1">aux</span>

  <span class="s3">return </span><span class="s1">jacfun</span>
<span class="s1">jacobian = jacrev</span>

<span class="s1">_check_input_dtype_jacrev = partial(_check_input_dtype_revderiv</span><span class="s3">, </span><span class="s4">&quot;jacrev&quot;</span><span class="s1">)</span>
<span class="s1">_check_output_dtype_jacrev = partial(_check_output_dtype_revderiv</span><span class="s3">, </span><span class="s4">&quot;jacrev&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">hessian(fun: Callable</span><span class="s3">, </span><span class="s1">argnums: Union[int</span><span class="s3">, </span><span class="s1">Sequence[int]] = </span><span class="s5">0</span><span class="s3">,</span>
            <span class="s1">has_aux: bool = </span><span class="s3">False, </span><span class="s1">holomorphic: bool = </span><span class="s3">False</span><span class="s1">) -&gt; Callable:</span>
  <span class="s2">&quot;&quot;&quot;Hessian of ``fun`` as a dense array. 
 
  Args: 
    fun: Function whose Hessian is to be computed.  Its arguments at positions 
      specified by ``argnums`` should be arrays, scalars, or standard Python 
      containers thereof. It should return arrays, scalars, or standard Python 
      containers thereof. 
    argnums: Optional, integer or sequence of integers. Specifies which 
      positional argument(s) to differentiate with respect to (default ``0``). 
    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the 
      first element is considered the output of the mathematical function to be 
      differentiated and the second element is auxiliary data. Default False. 
    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be 
      holomorphic. Default False. 
 
  Returns: 
    A function with the same arguments as ``fun``, that evaluates the Hessian of 
    ``fun``. 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; g = lambda x: x[0]**3 - 2*x[0]*x[1] - x[1]**6 
  &gt;&gt;&gt; print(jax.hessian(g)(jax.numpy.array([1., 2.]))) 
  [[   6.   -2.] 
   [  -2. -480.]] 
 
  :py:func:`hessian` is a generalization of the usual definition of the Hessian 
  that supports nested Python containers (i.e. pytrees) as inputs and outputs. 
  The tree structure of ``jax.hessian(fun)(x)`` is given by forming a tree 
  product of the structure of ``fun(x)`` with a tree product of two copies of 
  the structure of ``x``. A tree product of two tree structures is formed by 
  replacing each leaf of the first tree with a copy of the second. For example: 
 
  &gt;&gt;&gt; import jax.numpy as jnp 
  &gt;&gt;&gt; f = lambda dct: {&quot;c&quot;: jnp.power(dct[&quot;a&quot;], dct[&quot;b&quot;])} 
  &gt;&gt;&gt; print(jax.hessian(f)({&quot;a&quot;: jnp.arange(2.) + 1., &quot;b&quot;: jnp.arange(2.) + 2.})) 
  {'c': {'a': {'a': Array([[[ 2.,  0.], [ 0.,  0.]], 
                           [[ 0.,  0.], [ 0., 12.]]], dtype=float32), 
               'b': Array([[[ 1.      ,  0.      ], [ 0.      ,  0.      ]], 
                           [[ 0.      ,  0.      ], [ 0.      , 12.317766]]], dtype=float32)}, 
         'b': {'a': Array([[[ 1.      ,  0.      ], [ 0.      ,  0.      ]], 
                           [[ 0.      ,  0.      ], [ 0.      , 12.317766]]], dtype=float32), 
               'b': Array([[[0.      , 0.      ], [0.      , 0.      ]], 
                           [[0.      , 0.      ], [0.      , 3.843624]]], dtype=float32)}}} 
 
  Thus each leaf in the tree structure of ``jax.hessian(fun)(x)`` corresponds to 
  a leaf of ``fun(x)`` and a pair of leaves of ``x``. For each leaf in 
  ``jax.hessian(fun)(x)``, if the corresponding array leaf of ``fun(x)`` has 
  shape ``(out_1, out_2, ...)`` and the corresponding array leaves of ``x`` have 
  shape ``(in_1_1, in_1_2, ...)`` and ``(in_2_1, in_2_2, ...)`` respectively, 
  then the Hessian leaf has shape ``(out_1, out_2, ..., in_1_1, in_1_2, ..., 
  in_2_1, in_2_2, ...)``. In other words, the Python tree structure represents 
  the block structure of the Hessian, with blocks determined by the input and 
  output pytrees. 
 
  In particular, an array is produced (with no pytrees involved) when the 
  function input ``x`` and output ``fun(x)`` are each a single array, as in the 
  ``g`` example above. If ``fun(x)`` has shape ``(out1, out2, ...)`` and ``x`` 
  has shape ``(in1, in2, ...)`` then ``jax.hessian(fun)(x)`` has shape 
  ``(out1, out2, ..., in1, in2, ..., in1, in2, ...)``. To flatten pytrees into 
  1D vectors, consider using :py:func:`jax.flatten_util.flatten_pytree`. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">jacfwd(jacrev(fun</span><span class="s3">, </span><span class="s1">argnums</span><span class="s3">, </span><span class="s1">has_aux=has_aux</span><span class="s3">, </span><span class="s1">holomorphic=holomorphic)</span><span class="s3">,</span>
                <span class="s1">argnums</span><span class="s3">, </span><span class="s1">has_aux=has_aux</span><span class="s3">, </span><span class="s1">holomorphic=holomorphic)</span>

<span class="s3">def </span><span class="s1">_std_basis(pytree):</span>
  <span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>
  <span class="s1">leaves</span><span class="s3">, </span><span class="s1">_ = tree_flatten(pytree)</span>
  <span class="s1">ndim = sum(map(np.size</span><span class="s3">, </span><span class="s1">leaves))</span>
  <span class="s1">dtype = dtypes.result_type(*leaves)</span>
  <span class="s1">flat_basis = jnp.eye(ndim</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
  <span class="s3">return </span><span class="s1">_unravel_array_into_pytree(pytree</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">flat_basis)</span>

<span class="s3">def </span><span class="s1">_jacfwd_unravel(input_pytree</span><span class="s3">, </span><span class="s1">output_pytree_leaf</span><span class="s3">, </span><span class="s1">arr):</span>
  <span class="s3">return </span><span class="s1">_unravel_array_into_pytree(</span>
    <span class="s1">input_pytree</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">output_pytree_leaf</span><span class="s3">, </span><span class="s1">arr)</span>

<span class="s3">def </span><span class="s1">_jacrev_unravel(output_pytree</span><span class="s3">, </span><span class="s1">input_pytree_leaf</span><span class="s3">, </span><span class="s1">arr):</span>
  <span class="s3">return </span><span class="s1">_unravel_array_into_pytree(</span>
    <span class="s1">output_pytree</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">input_pytree_leaf</span><span class="s3">, </span><span class="s1">arr)</span>

<span class="s3">def </span><span class="s1">_possible_downcast(x</span><span class="s3">, </span><span class="s1">example):</span>
  <span class="s3">if </span><span class="s1">(dtypes.issubdtype(x.dtype</span><span class="s3">, </span><span class="s1">np.complexfloating) </span><span class="s3">and</span>
      <span class="s3">not </span><span class="s1">dtypes.issubdtype(_dtype(example)</span><span class="s3">, </span><span class="s1">np.complexfloating)):</span>
    <span class="s1">x = x.real</span>
  <span class="s1">dtype = </span><span class="s3">None if </span><span class="s1">example </span><span class="s3">is None else </span><span class="s1">_dtype(example)</span>
  <span class="s1">weak_type = </span><span class="s3">None if </span><span class="s1">example </span><span class="s3">is None else </span><span class="s1">dtypes.is_weakly_typed(example)</span>
  <span class="s3">return </span><span class="s1">lax_internal._convert_element_type(x</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">, </span><span class="s1">weak_type)</span>

<span class="s3">def </span><span class="s1">_unravel_array_into_pytree(pytree</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, </span><span class="s1">example</span><span class="s3">, </span><span class="s1">arr):</span>
  <span class="s2">&quot;&quot;&quot;Unravel an array into a PyTree with a given structure. 
  Args: 
      pytree: The pytree that provides the structure. 
      axis: The parameter axis is either -1, 0, or 1.  It controls the 
        resulting shapes. 
      example: If specified, cast the components to the matching dtype/weak_type, 
        or else use the pytree leaf type if example is None. 
      arr: The array to be unraveled. 
  &quot;&quot;&quot;</span>
  <span class="s1">leaves</span><span class="s3">, </span><span class="s1">treedef = tree_flatten(pytree)</span>
  <span class="s1">axis = axis % arr.ndim</span>
  <span class="s1">shapes = [arr.shape[:axis] + np.shape(l) + arr.shape[axis+</span><span class="s5">1</span><span class="s1">:] </span><span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">leaves]</span>
  <span class="s1">parts = _split(arr</span><span class="s3">, </span><span class="s1">np.cumsum(map(np.size</span><span class="s3">, </span><span class="s1">leaves[:-</span><span class="s5">1</span><span class="s1">]))</span><span class="s3">, </span><span class="s1">axis)</span>
  <span class="s1">reshaped_parts = [</span>
      <span class="s1">_possible_downcast(np.reshape(x</span><span class="s3">, </span><span class="s1">shape)</span><span class="s3">, </span><span class="s1">leaf </span><span class="s3">if </span><span class="s1">example </span><span class="s3">is None else </span><span class="s1">example)</span>
      <span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">leaf </span><span class="s3">in </span><span class="s1">zip(parts</span><span class="s3">, </span><span class="s1">shapes</span><span class="s3">, </span><span class="s1">leaves)]</span>
  <span class="s3">return </span><span class="s1">tree_unflatten(treedef</span><span class="s3">, </span><span class="s1">reshaped_parts)</span>

<span class="s3">def </span><span class="s1">_split(x</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">axis):</span>
  <span class="s3">if </span><span class="s1">isinstance(x</span><span class="s3">, </span><span class="s1">np.ndarray):</span>
    <span class="s3">return </span><span class="s1">np.split(x</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">axis)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">x._split(indices</span><span class="s3">, </span><span class="s1">axis)</span>


<span class="s3">def </span><span class="s1">vmap(fun: F</span><span class="s3">,</span>
         <span class="s1">in_axes: Union[int</span><span class="s3">, None, </span><span class="s1">Sequence[Any]] = </span><span class="s5">0</span><span class="s3">,</span>
         <span class="s1">out_axes: Any = </span><span class="s5">0</span><span class="s3">,</span>
         <span class="s1">axis_name: Optional[AxisName] = </span><span class="s3">None,</span>
         <span class="s1">axis_size: Optional[int] = </span><span class="s3">None,</span>
         <span class="s1">spmd_axis_name: Optional[Union[AxisName</span><span class="s3">, </span><span class="s1">Tuple[AxisName</span><span class="s3">, </span><span class="s1">...]]] = </span><span class="s3">None</span>
         <span class="s1">) -&gt; F:</span>
  <span class="s2">&quot;&quot;&quot;Vectorizing map. Creates a function which maps ``fun`` over argument axes. 
 
  Args: 
    fun: Function to be mapped over additional axes. 
    in_axes: An integer, None, or (nested) standard Python container 
      (tuple/list/dict) thereof specifying which input array axes to map over. 
 
      If each positional argument to ``fun`` is an array, then ``in_axes`` can 
      be an integer, a None, or a tuple of integers and Nones with length equal 
      to the number of positional arguments to ``fun``. An integer or ``None`` 
      indicates which array axis to map over for all arguments (with ``None`` 
      indicating not to map any axis), and a tuple indicates which axis to map 
      for each corresponding positional argument. Axis integers must be in the 
      range ``[-ndim, ndim)`` for each array, where ``ndim`` is the number of 
      dimensions (axes) of the corresponding input array. 
 
      If the positional arguments to ``fun`` are container (pytree) types, the 
      corresponding element of ``in_axes`` can itself be a matching container, 
      so that distinct array axes can be mapped for different container 
      elements. ``in_axes`` must be a container tree prefix of the positional 
      argument tuple passed to ``fun``. See this link for more detail: 
      https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees 
 
      Either ``axis_size`` must be provided explicitly, or at least one 
      positional argument must have ``in_axes`` not None. The sizes of the 
      mapped input axes for all mapped positional arguments must all be equal. 
 
      Arguments passed as keywords are always mapped over their leading axis 
      (i.e. axis index 0). 
 
      See below for examples. 
 
    out_axes: An integer, None, or (nested) standard Python container 
      (tuple/list/dict) thereof indicating where the mapped axis should appear 
      in the output. All outputs with a mapped axis must have a non-None 
      ``out_axes`` specification. Axis integers must be in the range ``[-ndim, 
      ndim)`` for each output array, where ``ndim`` is the number of dimensions 
      (axes) of the array returned by the :func:`vmap`-ed function, which is one 
      more than the number of dimensions (axes) of the corresponding array 
      returned by ``fun``. 
    axis_name: Optional, a hashable Python object used to identify the mapped 
      axis so that parallel collectives can be applied. 
    axis_size: Optional, an integer indicating the size of the axis to be 
      mapped. If not provided, the mapped axis size is inferred from arguments. 
 
  Returns: 
    Batched/vectorized version of ``fun`` with arguments that correspond to 
    those of ``fun``, but with extra array axes at positions indicated by 
    ``in_axes``, and a return value that corresponds to that of ``fun``, but 
    with extra array axes at positions indicated by ``out_axes``. 
 
  For example, we can implement a matrix-matrix product using a vector dot 
  product: 
 
  &gt;&gt;&gt; import jax.numpy as jnp 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; vv = lambda x, y: jnp.vdot(x, y)  #  ([a], [a]) -&gt; [] 
  &gt;&gt;&gt; mv = vmap(vv, (0, None), 0)      #  ([b,a], [a]) -&gt; [b]      (b is the mapped axis) 
  &gt;&gt;&gt; mm = vmap(mv, (None, 1), 1)      #  ([b,a], [a,c]) -&gt; [b,c]  (c is the mapped axis) 
 
  Here we use ``[a,b]`` to indicate an array with shape (a,b). Here are some 
  variants: 
 
  &gt;&gt;&gt; mv1 = vmap(vv, (0, 0), 0)   #  ([b,a], [b,a]) -&gt; [b]        (b is the mapped axis) 
  &gt;&gt;&gt; mv2 = vmap(vv, (0, 1), 0)   #  ([b,a], [a,b]) -&gt; [b]        (b is the mapped axis) 
  &gt;&gt;&gt; mm2 = vmap(mv2, (1, 1), 0)  #  ([b,c,a], [a,c,b]) -&gt; [c,b]  (c is the mapped axis) 
 
  Here's an example of using container types in ``in_axes`` to specify which 
  axes of the container elements to map over: 
 
  &gt;&gt;&gt; A, B, C, D = 2, 3, 4, 5 
  &gt;&gt;&gt; x = jnp.ones((A, B)) 
  &gt;&gt;&gt; y = jnp.ones((B, C)) 
  &gt;&gt;&gt; z = jnp.ones((C, D)) 
  &gt;&gt;&gt; def foo(tree_arg): 
  ...   x, (y, z) = tree_arg 
  ...   return jnp.dot(x, jnp.dot(y, z)) 
  &gt;&gt;&gt; tree = (x, (y, z)) 
  &gt;&gt;&gt; print(foo(tree)) 
  [[12. 12. 12. 12. 12.] 
   [12. 12. 12. 12. 12.]] 
  &gt;&gt;&gt; from jax import vmap 
  &gt;&gt;&gt; K = 6  # batch size 
  &gt;&gt;&gt; x = jnp.ones((K, A, B))  # batch axis in different locations 
  &gt;&gt;&gt; y = jnp.ones((B, K, C)) 
  &gt;&gt;&gt; z = jnp.ones((C, D, K)) 
  &gt;&gt;&gt; tree = (x, (y, z)) 
  &gt;&gt;&gt; vfoo = vmap(foo, in_axes=((0, (1, 2)),)) 
  &gt;&gt;&gt; print(vfoo(tree).shape) 
  (6, 2, 5) 
 
  Here's another example using container types in ``in_axes``, this time a 
  dictionary, to specify the elements of the container to map over: 
 
  &gt;&gt;&gt; dct = {'a': 0., 'b': jnp.arange(5.)} 
  &gt;&gt;&gt; x = 1. 
  &gt;&gt;&gt; def foo(dct, x): 
  ...  return dct['a'] + dct['b'] + x 
  &gt;&gt;&gt; out = vmap(foo, in_axes=({'a': None, 'b': 0}, None))(dct, x) 
  &gt;&gt;&gt; print(out) 
  [1. 2. 3. 4. 5.] 
 
  The results of a vectorized function can be mapped or unmapped. For example, 
  the function below returns a pair with the first element mapped and the second 
  unmapped. Only for unmapped results we can specify ``out_axes`` to be ``None`` 
  (to keep it unmapped). 
 
  &gt;&gt;&gt; print(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=(0, None))(jnp.arange(2.), 4.)) 
  (Array([4., 5.], dtype=float32), 8.0) 
 
  If the ``out_axes`` is specified for an unmapped result, the result is 
  broadcast across the mapped axis: 
 
  &gt;&gt;&gt; print(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=0)(jnp.arange(2.), 4.)) 
  (Array([4., 5.], dtype=float32), Array([8., 8.], dtype=float32, weak_type=True)) 
 
  If the ``out_axes`` is specified for a mapped result, the result is transposed 
  accordingly. 
 
  Finally, here's an example using ``axis_name`` together with collectives: 
 
  &gt;&gt;&gt; xs = jnp.arange(3. * 4.).reshape(3, 4) 
  &gt;&gt;&gt; print(vmap(lambda x: lax.psum(x, 'i'), axis_name='i')(xs)) 
  [[12. 15. 18. 21.] 
   [12. 15. 18. 21.] 
   [12. 15. 18. 21.]] 
 
  See the :py:func:`jax.pmap` docstring for more examples involving collectives. 
  &quot;&quot;&quot;</span>
  <span class="s1">check_callable(fun)</span>
  <span class="s1">docstr = (</span><span class="s4">&quot;Vectorized version of {fun}. Takes similar arguments as {fun} &quot;</span>
            <span class="s4">&quot;but with additional array axes over which {fun} is mapped.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">fun.__doc__:</span>
    <span class="s1">docstr += </span><span class="s4">&quot;</span><span class="s3">\n\n</span><span class="s4">Original documentation:</span><span class="s3">\n\n</span><span class="s4">&quot;</span>
    <span class="s1">docstr += fun.__doc__</span>

  <span class="s1">axis_name = core.no_axis_name </span><span class="s3">if </span><span class="s1">axis_name </span><span class="s3">is None else </span><span class="s1">axis_name</span>
  <span class="s3">if </span><span class="s1">spmd_axis_name </span><span class="s3">is not None and </span><span class="s1">type(spmd_axis_name) </span><span class="s3">is not </span><span class="s1">tuple:</span>
    <span class="s1">spmd_axis_name = (spmd_axis_name</span><span class="s3">,</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">isinstance(in_axes</span><span class="s3">, </span><span class="s1">list):</span>
    <span class="s0"># To be a tree prefix of the positional args tuple, in_axes can never be a</span>
    <span class="s0"># list: if in_axes is not a leaf, it must be a tuple of trees. However,</span>
    <span class="s0"># in cases like these users expect tuples and lists to be treated</span>
    <span class="s0"># essentially interchangeably, so we canonicalize lists to tuples here</span>
    <span class="s0"># rather than raising an error. https://github.com/google/jax/issues/2367</span>
    <span class="s1">in_axes = tuple(in_axes)</span>

  <span class="s3">if not </span><span class="s1">all(type(l) </span><span class="s3">is </span><span class="s1">int </span><span class="s3">or </span><span class="s1">type(l) </span><span class="s3">in </span><span class="s1">batching.spec_types</span>
             <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">tree_leaves(in_axes)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;vmap in_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="s4">f&quot;with those types as leaves, but got </span><span class="s3">{</span><span class="s1">in_axes</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">all(type(l) </span><span class="s3">is </span><span class="s1">int </span><span class="s3">or </span><span class="s1">type(l) </span><span class="s3">in </span><span class="s1">batching.spec_types</span>
               <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">tree_leaves(out_axes)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;vmap out_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="s4">f&quot;with those types as leaves, but got </span><span class="s3">{</span><span class="s1">out_axes</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>

  <span class="s1">@wraps(fun</span><span class="s3">, </span><span class="s1">docstr=docstr)</span>
  <span class="s1">@api_boundary</span>
  <span class="s3">def </span><span class="s1">vmap_f(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">args_flat</span><span class="s3">, </span><span class="s1">in_tree  = tree_flatten((args</span><span class="s3">, </span><span class="s1">kwargs)</span><span class="s3">, </span><span class="s1">is_leaf=batching.is_vmappable)</span>
    <span class="s1">f = lu.wrap_init(fun)</span>
    <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">out_tree = batching.flatten_fun_for_vmap(f</span><span class="s3">, </span><span class="s1">in_tree)</span>
    <span class="s1">in_axes_flat = flatten_axes(</span><span class="s4">&quot;vmap in_axes&quot;</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">(in_axes</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">kws=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">axis_size_ = (axis_size </span><span class="s3">if </span><span class="s1">axis_size </span><span class="s3">is not None else</span>
                  <span class="s1">_mapped_axis_size(fun</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">args_flat</span><span class="s3">, </span><span class="s1">in_axes_flat</span><span class="s3">, </span><span class="s4">&quot;vmap&quot;</span><span class="s1">))</span>
    <span class="s1">out_flat = batching.batch(</span>
        <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_size_</span><span class="s3">, </span><span class="s1">in_axes_flat</span><span class="s3">,</span>
        <span class="s3">lambda</span><span class="s1">: flatten_axes(</span><span class="s4">&quot;vmap out_axes&quot;</span><span class="s3">, </span><span class="s1">out_tree()</span><span class="s3">, </span><span class="s1">out_axes)</span><span class="s3">,</span>
        <span class="s1">spmd_axis_name=spmd_axis_name</span>
    <span class="s1">).call_wrapped(*args_flat)</span>
    <span class="s3">return </span><span class="s1">tree_unflatten(out_tree()</span><span class="s3">, </span><span class="s1">out_flat)</span>

  <span class="s3">return </span><span class="s1">cast(F</span><span class="s3">, </span><span class="s1">vmap_f)</span>

<span class="s3">def </span><span class="s1">_mapped_axis_size(fn</span><span class="s3">, </span><span class="s1">tree</span><span class="s3">, </span><span class="s1">vals</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">name):</span>
  <span class="s3">if not </span><span class="s1">vals:</span>
    <span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs = tree_unflatten(tree</span><span class="s3">, </span><span class="s1">vals)</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">wrapped function must be passed at least one argument &quot;</span>
        <span class="s4">f&quot;containing an array, got empty *args=</span><span class="s3">{</span><span class="s1">args</span><span class="s3">} </span><span class="s4">and **kwargs=</span><span class="s3">{</span><span class="s1">kwargs</span><span class="s3">}</span><span class="s4">&quot;</span>
    <span class="s1">)</span>

  <span class="s3">def </span><span class="s1">_get_axis_size(name: str</span><span class="s3">, </span><span class="s1">shape: Tuple[core.AxisSize</span><span class="s3">, </span><span class="s1">...]</span><span class="s3">, </span><span class="s1">axis: int</span>
                     <span class="s1">) -&gt; core.AxisSize:</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">shape[axis]</span>
    <span class="s3">except </span><span class="s1">(IndexError</span><span class="s3">, </span><span class="s1">TypeError) </span><span class="s3">as </span><span class="s1">e:</span>
      <span class="s1">min_rank = axis + </span><span class="s5">1 </span><span class="s3">if </span><span class="s1">axis &gt;= </span><span class="s5">0 </span><span class="s3">else </span><span class="s1">-axis</span>
      <span class="s0"># TODO(mattjj): better error message here</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span>
          <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">was requested to map its argument along axis </span><span class="s3">{</span><span class="s1">axis</span><span class="s3">}</span><span class="s4">, &quot;</span>
          <span class="s4">f&quot;which implies that its rank should be at least </span><span class="s3">{</span><span class="s1">min_rank</span><span class="s3">}</span><span class="s4">, &quot;</span>
          <span class="s4">f&quot;but is only </span><span class="s3">{</span><span class="s1">len(shape)</span><span class="s3">} </span><span class="s4">(its shape is </span><span class="s3">{</span><span class="s1">shape</span><span class="s3">}</span><span class="s4">)&quot;</span><span class="s1">) </span><span class="s3">from </span><span class="s1">e</span>

  <span class="s1">sizes = core.dedup_referents(_get_axis_size(name</span><span class="s3">, </span><span class="s1">np.shape(x)</span><span class="s3">, </span><span class="s1">d)</span>
                               <span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">zip(vals</span><span class="s3">, </span><span class="s1">dims) </span><span class="s3">if </span><span class="s1">d </span><span class="s3">is not None</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">len(sizes) == </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s1">sz</span><span class="s3">, </span><span class="s1">= sizes</span>
    <span class="s3">return </span><span class="s1">sz</span>
  <span class="s3">if not </span><span class="s1">sizes:</span>
    <span class="s1">msg = </span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">must have at least one non-None value in in_axes&quot;</span>
    <span class="s3">raise </span><span class="s1">ValueError(msg)</span>

  <span class="s1">msg = [</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">got inconsistent sizes for array axes to be mapped:</span><span class="s3">\n</span><span class="s4">&quot;</span><span class="s1">]</span>
  <span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs = tree_unflatten(tree</span><span class="s3">, </span><span class="s1">vals)</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">ba = inspect.signature(fn).bind(*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>
  <span class="s3">except </span><span class="s1">(TypeError</span><span class="s3">, </span><span class="s1">ValueError):</span>
    <span class="s1">ba = </span><span class="s3">None</span>
  <span class="s3">if </span><span class="s1">ba </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">args_paths = [</span><span class="s4">f'args</span><span class="s3">{</span><span class="s1">keystr(p)</span><span class="s3">} </span><span class="s4">'</span>
                  <span class="s4">f'of type </span><span class="s3">{</span><span class="s1">shaped_abstractify(x).str_short()</span><span class="s3">}</span><span class="s4">'</span>
                  <span class="s3">for </span><span class="s1">p</span><span class="s3">, </span><span class="s1">x </span><span class="s3">in </span><span class="s1">generate_key_paths(args)]</span>
    <span class="s1">kwargs_paths = [</span><span class="s4">f'kwargs</span><span class="s3">{</span><span class="s1">keystr(p)</span><span class="s3">} </span><span class="s4">'</span>
                    <span class="s4">f'of type </span><span class="s3">{</span><span class="s1">shaped_abstractify(x).str_short()</span><span class="s3">}</span><span class="s4">'</span>
                    <span class="s3">for </span><span class="s1">p</span><span class="s3">, </span><span class="s1">x </span><span class="s3">in </span><span class="s1">generate_key_paths(kwargs)]</span>
    <span class="s1">key_paths = [*args_paths</span><span class="s3">, </span><span class="s1">*kwargs_paths]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">key_paths = [</span><span class="s4">f'argument </span><span class="s3">{</span><span class="s1">name</span><span class="s3">}{</span><span class="s1">keystr(p)</span><span class="s3">} </span><span class="s4">'</span>
                 <span class="s4">f'of type </span><span class="s3">{</span><span class="s1">shaped_abstractify(x).str_short()</span><span class="s3">}</span><span class="s4">'</span>
                 <span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">ba.arguments.items()</span>
                 <span class="s3">for </span><span class="s1">p</span><span class="s3">, </span><span class="s1">x </span><span class="s3">in </span><span class="s1">generate_key_paths(arg)]</span>
  <span class="s1">all_sizes = [_get_axis_size(name</span><span class="s3">, </span><span class="s1">np.shape(x)</span><span class="s3">, </span><span class="s1">d) </span><span class="s3">if </span><span class="s1">d </span><span class="s3">is not None else None</span>
               <span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">zip(vals</span><span class="s3">, </span><span class="s1">dims)]</span>
  <span class="s1">size_counts = collections.Counter(s </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">all_sizes </span><span class="s3">if </span><span class="s1">s </span><span class="s3">is not None</span><span class="s1">)</span>
  <span class="s1">(sz</span><span class="s3">, </span><span class="s1">ct)</span><span class="s3">, </span><span class="s1">*other_counts = counts = size_counts.most_common()</span>
  <span class="s3">def </span><span class="s1">_all_sizes_index(sz):</span>
    <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">isz </span><span class="s3">in </span><span class="s1">enumerate(all_sizes):</span>
      <span class="s3">if </span><span class="s1">core.symbolic_equal_dim(isz</span><span class="s3">, </span><span class="s1">sz): </span><span class="s3">return </span><span class="s1">i</span>
    <span class="s3">assert False, </span><span class="s1">(sz</span><span class="s3">, </span><span class="s1">all_sizes)</span>

  <span class="s1">ex</span><span class="s3">, </span><span class="s1">*examples = [key_paths[_all_sizes_index(sz)] </span><span class="s3">for </span><span class="s1">sz</span><span class="s3">, </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">counts]</span>
  <span class="s1">ax</span><span class="s3">, </span><span class="s1">*axs = [dims[_all_sizes_index(sz)] </span><span class="s3">for </span><span class="s1">sz</span><span class="s3">, </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">counts]</span>
  <span class="s3">if </span><span class="s1">ct == </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s1">msg.append(</span><span class="s4">f&quot;  * one axis had size </span><span class="s3">{</span><span class="s1">sz</span><span class="s3">}</span><span class="s4">: axis </span><span class="s3">{</span><span class="s1">ax</span><span class="s3">} </span><span class="s4">of </span><span class="s3">{</span><span class="s1">ex</span><span class="s3">}</span><span class="s4">;</span><span class="s3">\n</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">msg.append(</span><span class="s4">f&quot;  * most axes (</span><span class="s3">{</span><span class="s1">ct</span><span class="s3">} </span><span class="s4">of them) had size </span><span class="s3">{</span><span class="s1">sz</span><span class="s3">}</span><span class="s4">, e.g. axis </span><span class="s3">{</span><span class="s1">ax</span><span class="s3">} </span><span class="s4">of </span><span class="s3">{</span><span class="s1">ex</span><span class="s3">}</span><span class="s4">;</span><span class="s3">\n</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">for </span><span class="s1">ex</span><span class="s3">, </span><span class="s1">ax</span><span class="s3">, </span><span class="s1">(sz</span><span class="s3">, </span><span class="s1">ct) </span><span class="s3">in </span><span class="s1">zip(examples</span><span class="s3">, </span><span class="s1">axs</span><span class="s3">, </span><span class="s1">other_counts):</span>
    <span class="s3">if </span><span class="s1">ct == </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">msg.append(</span><span class="s4">f&quot;  * one axis had size </span><span class="s3">{</span><span class="s1">sz</span><span class="s3">}</span><span class="s4">: axis </span><span class="s3">{</span><span class="s1">ax</span><span class="s3">} </span><span class="s4">of </span><span class="s3">{</span><span class="s1">ex</span><span class="s3">}</span><span class="s4">;</span><span class="s3">\n</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">msg.append(</span><span class="s4">f&quot;  * some axes (</span><span class="s3">{</span><span class="s1">ct</span><span class="s3">} </span><span class="s4">of them) had size </span><span class="s3">{</span><span class="s1">sz</span><span class="s3">}</span><span class="s4">, e.g. axis </span><span class="s3">{</span><span class="s1">ax</span><span class="s3">} </span><span class="s4">of </span><span class="s3">{</span><span class="s1">ex</span><span class="s3">}</span><span class="s4">;</span><span class="s3">\n</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">''</span><span class="s1">.join(msg)[:-</span><span class="s5">2</span><span class="s1">])  </span><span class="s0"># remove last semicolon and newline</span>


<span class="s3">def </span><span class="s1">pmap(</span>
    <span class="s1">fun: Callable</span><span class="s3">,</span>
    <span class="s1">axis_name: Optional[AxisName] = </span><span class="s3">None,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">in_axes=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">out_axes=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">static_broadcasted_argnums: Union[int</span><span class="s3">, </span><span class="s1">Iterable[int]] = ()</span><span class="s3">,</span>
    <span class="s1">devices: Optional[Sequence[xc.Device]] = </span><span class="s3">None,  </span><span class="s0"># noqa: F811</span>
    <span class="s1">backend: Optional[str] = </span><span class="s3">None,</span>
    <span class="s1">axis_size: Optional[int] = </span><span class="s3">None,</span>
    <span class="s1">donate_argnums: Union[int</span><span class="s3">, </span><span class="s1">Iterable[int]] = ()</span><span class="s3">,</span>
    <span class="s1">global_arg_shapes: Optional[Tuple[Tuple[int</span><span class="s3">, </span><span class="s1">...]</span><span class="s3">, </span><span class="s1">...]] = </span><span class="s3">None,</span>
  <span class="s1">) -&gt; Any:</span>
  <span class="s2">&quot;&quot;&quot;Parallel map with support for collective operations. 
 
  The purpose of :py:func:`pmap` is to express single-program multiple-data 
  (SPMD) programs. Applying :py:func:`pmap` to a function will compile the 
  function with XLA (similarly to :py:func:`jit`), then execute it in parallel 
  on XLA devices, such as multiple GPUs or multiple TPU cores. Semantically it 
  is comparable to :py:func:`vmap` because both transformations map a function 
  over array axes, but where :py:func:`vmap` vectorizes functions by pushing the 
  mapped axis down into primitive operations, :py:func:`pmap` instead replicates 
  the function and executes each replica on its own XLA device in parallel. 
 
  The mapped axis size must be less than or equal to the number of local XLA 
  devices available, as returned by :py:func:`jax.local_device_count()` (unless 
  ``devices`` is specified, see below). For nested :py:func:`pmap` calls, the 
  product of the mapped axis sizes must be less than or equal to the number of 
  XLA devices. 
 
  .. note:: 
    :py:func:`pmap` compiles ``fun``, so while it can be combined with 
    :py:func:`jit`, it's usually unnecessary. 
 
  :py:func:`pmap` requires that all of the participating devices are identical. 
  For example, it is not possible to use :py:func:`pmap` to parallelize a 
  computation across two different models of GPU. It is currently an error for 
  the same device to participate twice in the same `pmap`. 
 
  **Multi-process platforms:** On multi-process platforms such as TPU pods, 
  :py:func:`pmap` is designed to be used in SPMD Python programs, where every 
  process is running the same Python code such that all processes run the same 
  pmapped function in the same order. Each process should still call the pmapped 
  function with mapped axis size equal to the number of *local* devices (unless 
  ``devices`` is specified, see below), and an array of the same leading axis 
  size will be returned as usual. However, any collective operations in ``fun`` 
  will be computed over *all* participating devices, including those on other 
  processes, via device-to-device communication.  Conceptually, this can be 
  thought of as running a pmap over a single array sharded across processes, 
  where each process &quot;sees&quot; only its local shard of the input and output. The 
  SPMD model requires that the same multi-process pmaps must be run in the same 
  order on all devices, but they can be interspersed with arbitrary operations 
  running in a single process. 
 
  Args: 
    fun: Function to be mapped over argument axes. Its arguments and return 
      value should be arrays, scalars, or (nested) standard Python containers 
      (tuple/list/dict) thereof. Positional arguments indicated by 
      ``static_broadcasted_argnums`` can be anything at all, provided they are 
      hashable and have an equality operation defined. 
    axis_name: Optional, a hashable Python object used to identify the mapped 
      axis so that parallel collectives can be applied. 
    in_axes: A non-negative integer, None, or nested Python container thereof 
      that specifies which axes of positional arguments to map over. Arguments 
      passed as keywords are always mapped over their leading axis (i.e. axis 
      index 0). See :py:func:`vmap` for details. 
    out_axes: A non-negative integer, None, or nested Python container thereof 
      indicating where the mapped axis should appear in the output. All outputs 
      with a mapped axis must have a non-None ``out_axes`` specification 
      (see :py:func:`vmap`). 
    static_broadcasted_argnums: An int or collection of ints specifying which 
      positional arguments to treat as static (compile-time constant). 
      Operations that only depend on static arguments will be constant-folded. 
      Calling the pmapped function with different values for these constants 
      will trigger recompilation. If the pmapped function is called with fewer 
      positional arguments than indicated by ``static_argnums`` then an error is 
      raised. Each of the static arguments will be broadcasted to all devices. 
      Arguments that are not arrays or containers thereof must be marked as 
      static. Defaults to (). 
 
      Static arguments must be hashable, meaning both ``__hash__`` and 
      ``__eq__`` are implemented, and should be immutable. 
 
    devices: This is an experimental feature and the API is likely to change. 
      Optional, a sequence of Devices to map over. (Available devices can be 
      retrieved via jax.devices()). Must be given identically for each process 
      in multi-process settings (and will therefore include devices across 
      processes). If specified, the size of the mapped axis must be equal to 
      the number of devices in the sequence local to the given process. Nested 
      :py:func:`pmap` s with ``devices`` specified in either the inner or outer 
      :py:func:`pmap` are not yet supported. 
    backend: This is an experimental feature and the API is likely to change. 
      Optional, a string representing the XLA backend. 'cpu', 'gpu', or 'tpu'. 
    axis_size: Optional; the size of the mapped axis. 
    donate_argnums: Specify which positional argument buffers are &quot;donated&quot; to 
      the computation. It is safe to donate argument buffers if you no longer need 
      them once the computation has finished. In some cases XLA can make use of 
      donated buffers to reduce the amount of memory needed to perform a 
      computation, for example recycling one of your input buffers to store a 
      result. You should not reuse buffers that you donate to a computation, JAX 
      will raise an error if you try to. 
      Note that donate_argnums only work for positional arguments, and keyword 
      arguments will not be donated. 
 
      For more details on buffer donation see the 
      `FAQ &lt;https://jax.readthedocs.io/en/latest/faq.html#buffer-donation&gt;`_. 
 
  Returns: 
    A parallelized version of ``fun`` with arguments that correspond to those of 
    ``fun`` but with extra array axes at positions indicated by ``in_axes`` and 
    with output that has an additional leading array axis (with the same size). 
 
  For example, assuming 8 XLA devices are available, :py:func:`pmap` can be used 
  as a map along a leading array axis: 
 
  &gt;&gt;&gt; import jax.numpy as jnp 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; out = pmap(lambda x: x ** 2)(jnp.arange(8))  # doctest: +SKIP 
  &gt;&gt;&gt; print(out)  # doctest: +SKIP 
  [0, 1, 4, 9, 16, 25, 36, 49] 
 
  When the leading dimension is smaller than the number of available devices JAX 
  will simply run on a subset of devices: 
 
  &gt;&gt;&gt; x = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2)) 
  &gt;&gt;&gt; y = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2)) ** 2 
  &gt;&gt;&gt; out = pmap(jnp.dot)(x, y)  # doctest: +SKIP 
  &gt;&gt;&gt; print(out)  # doctest: +SKIP 
  [[[    4.     9.] 
    [   12.    29.]] 
   [[  244.   345.] 
    [  348.   493.]] 
   [[ 1412.  1737.] 
    [ 1740.  2141.]]] 
 
  If your leading dimension is larger than the number of available devices you 
  will get an error: 
 
  &gt;&gt;&gt; pmap(lambda x: x ** 2)(jnp.arange(9))  # doctest: +SKIP 
  ValueError: ... requires 9 replicas, but only 8 XLA devices are available 
 
  As with :py:func:`vmap`, using ``None`` in ``in_axes`` indicates that an 
  argument doesn't have an extra axis and should be broadcasted, rather than 
  mapped, across the replicas: 
 
  &gt;&gt;&gt; x, y = jnp.arange(2.), 4. 
  &gt;&gt;&gt; out = pmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None))(x, y)  # doctest: +SKIP 
  &gt;&gt;&gt; print(out)  # doctest: +SKIP 
  ([4., 5.], [8., 8.]) 
 
  Note that :py:func:`pmap` always returns values mapped over their leading axis, 
  equivalent to using ``out_axes=0`` in :py:func:`vmap`. 
 
  In addition to expressing pure maps, :py:func:`pmap` can also be used to express 
  parallel single-program multiple-data (SPMD) programs that communicate via 
  collective operations. For example: 
 
  &gt;&gt;&gt; f = lambda x: x / jax.lax.psum(x, axis_name='i') 
  &gt;&gt;&gt; out = pmap(f, axis_name='i')(jnp.arange(4.))  # doctest: +SKIP 
  &gt;&gt;&gt; print(out)  # doctest: +SKIP 
  [ 0.          0.16666667  0.33333334  0.5       ] 
  &gt;&gt;&gt; print(out.sum())  # doctest: +SKIP 
  1.0 
 
  In this example, ``axis_name`` is a string, but it can be any Python object 
  with ``__hash__`` and ``__eq__`` defined. 
 
  The argument ``axis_name`` to :py:func:`pmap` names the mapped axis so that 
  collective operations, like :func:`jax.lax.psum`, can refer to it. Axis names 
  are important particularly in the case of nested :py:func:`pmap` functions, 
  where collective operations can operate over distinct axes: 
 
  &gt;&gt;&gt; from functools import partial 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; @partial(pmap, axis_name='rows') 
  ... @partial(pmap, axis_name='cols') 
  ... def normalize(x): 
  ...   row_normed = x / jax.lax.psum(x, 'rows') 
  ...   col_normed = x / jax.lax.psum(x, 'cols') 
  ...   doubly_normed = x / jax.lax.psum(x, ('rows', 'cols')) 
  ...   return row_normed, col_normed, doubly_normed 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; x = jnp.arange(8.).reshape((4, 2)) 
  &gt;&gt;&gt; row_normed, col_normed, doubly_normed = normalize(x)  # doctest: +SKIP 
  &gt;&gt;&gt; print(row_normed.sum(0))  # doctest: +SKIP 
  [ 1.  1.] 
  &gt;&gt;&gt; print(col_normed.sum(1))  # doctest: +SKIP 
  [ 1.  1.  1.  1.] 
  &gt;&gt;&gt; print(doubly_normed.sum((0, 1)))  # doctest: +SKIP 
  1.0 
 
  On multi-process platforms, collective operations operate over all devices, 
  including those on other processes. For example, assuming the following code 
  runs on two processes with 4 XLA devices each: 
 
  &gt;&gt;&gt; f = lambda x: x + jax.lax.psum(x, axis_name='i') 
  &gt;&gt;&gt; data = jnp.arange(4) if jax.process_index() == 0 else jnp.arange(4, 8) 
  &gt;&gt;&gt; out = pmap(f, axis_name='i')(data)  # doctest: +SKIP 
  &gt;&gt;&gt; print(out)  # doctest: +SKIP 
  [28 29 30 31] # on process 0 
  [32 33 34 35] # on process 1 
 
  Each process passes in a different length-4 array, corresponding to its 4 
  local devices, and the psum operates over all 8 values. Conceptually, the two 
  length-4 arrays can be thought of as a sharded length-8 array (in this example 
  equivalent to jnp.arange(8)) that is mapped over, with the length-8 mapped 
  axis given name 'i'. The pmap call on each process then returns the 
  corresponding length-4 output shard. 
 
  The ``devices`` argument can be used to specify exactly which devices are used 
  to run the parallel computation. For example, again assuming a single process 
  with 8 devices, the following code defines two parallel computations, one 
  which runs on the first six devices and one on the remaining two: 
 
  &gt;&gt;&gt; from functools import partial 
  &gt;&gt;&gt; @partial(pmap, axis_name='i', devices=jax.devices()[:6]) 
  ... def f1(x): 
  ...   return x / jax.lax.psum(x, axis_name='i') 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; @partial(pmap, axis_name='i', devices=jax.devices()[-2:]) 
  ... def f2(x): 
  ...   return jax.lax.psum(x ** 2, axis_name='i') 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; print(f1(jnp.arange(6.)))  # doctest: +SKIP 
  [0.         0.06666667 0.13333333 0.2        0.26666667 0.33333333] 
  &gt;&gt;&gt; print(f2(jnp.array([2., 3.])))  # doctest: +SKIP 
  [ 13.  13.] 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">global_arg_shapes </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">&quot;global_arg_shapes only worked with sharded_jit which has long been&quot;</span>
        <span class="s4">&quot; removed from JAX. Please migrate to pjit and remove global_arg_shapes&quot;</span>
        <span class="s4">&quot; from pmap.&quot;</span><span class="s1">)</span>

  <span class="s0"># TODO(yashkatariya): Move this out after shard_map is out of experimental and</span>
  <span class="s0"># in _src</span>
  <span class="s3">if </span><span class="s1">config.jax_pmap_shmap_merge:</span>
    <span class="s3">from </span><span class="s1">jax.experimental.shard_map </span><span class="s3">import </span><span class="s1">pmap</span>
    <span class="s3">return </span><span class="s1">pmap(fun</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">in_axes=in_axes</span><span class="s3">, </span><span class="s1">out_axes=out_axes</span><span class="s3">,</span>
                <span class="s1">static_broadcasted_argnums=static_broadcasted_argnums</span><span class="s3">,</span>
                <span class="s1">devices=devices</span><span class="s3">, </span><span class="s1">backend=backend</span><span class="s3">,</span>
                <span class="s1">axis_size=axis_size</span><span class="s3">,</span>
                <span class="s1">donate_argnums=donate_argnums)</span>

  <span class="s3">return </span><span class="s1">_cpp_pmap(</span>
      <span class="s1">fun</span><span class="s3">,</span>
      <span class="s1">axis_name</span><span class="s3">,</span>
      <span class="s1">in_axes=in_axes</span><span class="s3">,</span>
      <span class="s1">out_axes=out_axes</span><span class="s3">,</span>
      <span class="s1">static_broadcasted_argnums=static_broadcasted_argnums</span><span class="s3">,</span>
      <span class="s1">devices=devices</span><span class="s3">,</span>
      <span class="s1">backend=backend</span><span class="s3">,</span>
      <span class="s1">axis_size=axis_size</span><span class="s3">,</span>
      <span class="s1">donate_argnums=donate_argnums)</span>


<span class="s3">class </span><span class="s1">PmapCallInfo(NamedTuple):</span>
  <span class="s1">flat_fun: lu.WrappedFun</span>
  <span class="s1">in_tree: PyTreeDef</span>
  <span class="s1">out_tree: Callable[[]</span><span class="s3">, </span><span class="s1">PyTreeDef]</span>
  <span class="s1">flat_args: Sequence[Any]</span>
  <span class="s1">donated_invars: Sequence[bool]</span>
  <span class="s1">in_axes_flat: Sequence[Optional[int]]</span>
  <span class="s1">local_axis_size: int</span>
  <span class="s1">out_axes_thunk: Callable</span>
  <span class="s1">devices: Optional[Sequence[xc.Device]]</span>
  <span class="s1">global_axis_size: int</span>
  <span class="s1">is_explicit_global_axis_size: bool</span>


<span class="s3">def </span><span class="s1">_get_global_axis_size(local_axis_size: int</span><span class="s3">, </span><span class="s1">in_devices</span><span class="s3">, </span><span class="s1">backend_name: str</span><span class="s3">,</span>
                          <span class="s1">global_axis_size: Optional[int]):</span>
  <span class="s2">&quot;&quot;&quot;Determine global_axis_size for multi-host pmap.&quot;&quot;&quot;</span>
  <span class="s0"># TODO(mattjj,skyewm): revive this check (inner_pmap always False now)</span>
  <span class="s0"># if xb.process_count() &gt; 1 and global_axis_size is None and inner_pmap:</span>
  <span class="s0">#   raise ValueError(&quot;'axis_size' must be specified for nested multi-host pmaps&quot;)</span>
  <span class="s3">if </span><span class="s1">(xb.process_count() == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">global_axis_size </span><span class="s3">is not None and</span>
      <span class="s1">global_axis_size != local_axis_size):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">f&quot;Specified axis_size </span><span class="s3">{</span><span class="s1">global_axis_size</span><span class="s3">} </span><span class="s4">doesn't match received &quot;</span>
        <span class="s4">f&quot;axis_size </span><span class="s3">{</span><span class="s1">local_axis_size</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">in_devices </span><span class="s3">is not None and </span><span class="s1">backend_name </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">backend = xb.get_device_backend(in_devices[</span><span class="s5">0</span><span class="s1">])</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">backend = xb.get_backend(backend_name)</span>

  <span class="s3">if </span><span class="s1">global_axis_size </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">xb.process_count(backend) == </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">global_axis_size = local_axis_size</span>
    <span class="s3">elif </span><span class="s1">in_devices:</span>
      <span class="s1">global_axis_size = len(in_devices)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">global_axis_size = local_axis_size * xb.process_count(backend)</span>
      <span class="s3">assert </span><span class="s1">all(</span>
          <span class="s1">len(xb.local_devices(pi</span><span class="s3">, </span><span class="s1">backend)) == xb.local_device_count(backend)</span>
          <span class="s3">for </span><span class="s1">pi </span><span class="s3">in </span><span class="s1">range(xb.process_count(backend)))</span>
  <span class="s3">return </span><span class="s1">global_axis_size</span>

<span class="s3">def </span><span class="s1">_prepare_pmap(fun</span><span class="s3">, </span><span class="s1">in_axes</span><span class="s3">, </span><span class="s1">out_axes</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple</span><span class="s3">,</span>
                  <span class="s1">donate_tuple</span><span class="s3">, </span><span class="s1">in_devices</span><span class="s3">, </span><span class="s1">backend_name</span><span class="s3">,</span>
                  <span class="s1">axis_size</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs):</span>
  <span class="s3">if </span><span class="s1">in_devices </span><span class="s3">is not None and </span><span class="s1">len(in_devices) == </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;'devices' argument to pmap must be non-empty, or None.&quot;</span><span class="s1">)</span>

  <span class="s1">dbg = debug_info(</span><span class="s4">'pmap'</span><span class="s3">, </span><span class="s1">fun</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple</span><span class="s3">, </span><span class="s1">())</span>

  <span class="s1">f = lu.wrap_init(fun)</span>
  <span class="s3">if </span><span class="s1">static_broadcasted_tuple:</span>
    <span class="s3">if </span><span class="s1">max(static_broadcasted_tuple) &gt;= len(args):</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span>
          <span class="s4">f&quot;pmapped function has static_broadcasted_argnums=</span><span class="s3">{</span><span class="s1">static_broadcasted_tuple</span><span class="s3">}</span><span class="s4">&quot;</span>
          <span class="s4">f&quot; but was called with only </span><span class="s3">{</span><span class="s1">len(args)</span><span class="s3">} </span><span class="s4">positional &quot;</span>
          <span class="s4">f&quot;argument</span><span class="s3">{</span><span class="s4">'s' </span><span class="s3">if </span><span class="s1">len(args) &gt; </span><span class="s5">1 </span><span class="s3">else </span><span class="s4">''</span><span class="s3">}</span><span class="s4">. &quot;</span>
          <span class="s4">&quot;All static broadcasted arguments must be passed positionally.&quot;</span><span class="s1">)</span>
    <span class="s1">dyn_argnums = [i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(args))</span>
                   <span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">static_broadcasted_tuple]</span>
    <span class="s1">f</span><span class="s3">, </span><span class="s1">dyn_args = argnums_partial(f</span><span class="s3">, </span><span class="s1">dyn_argnums</span><span class="s3">, </span><span class="s1">args)</span>

    <span class="s3">if </span><span class="s1">isinstance(in_axes</span><span class="s3">, </span><span class="s1">tuple):</span>
      <span class="s1">dyn_in_axes = tuple(in_axes[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">dyn_argnums)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">dyn_in_axes = in_axes</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">dyn_args</span><span class="s3">, </span><span class="s1">dyn_in_axes = args</span><span class="s3">, </span><span class="s1">in_axes</span>
  <span class="s1">args</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten((dyn_args</span><span class="s3">, </span><span class="s1">kwargs))</span>

  <span class="s3">if </span><span class="s1">donate_tuple </span><span class="s3">and not </span><span class="s1">config.jax_debug_nans:</span>
    <span class="s1">donated_invars = donation_vector(donate_tuple</span><span class="s3">, </span><span class="s1">dyn_args</span><span class="s3">, </span><span class="s1">kwargs)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">donated_invars = (</span><span class="s3">False,</span><span class="s1">) * len(args)</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">in_axes_flat = tuple(broadcast_prefix((dyn_in_axes</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(dyn_args</span><span class="s3">, </span><span class="s1">kwargs)</span><span class="s3">,</span>
                                          <span class="s1">is_leaf=</span><span class="s3">lambda </span><span class="s1">x: x </span><span class="s3">is None</span><span class="s1">))</span>
  <span class="s3">except </span><span class="s1">ValueError:</span>
    <span class="s1">e</span><span class="s3">, </span><span class="s1">*_ = prefix_errors((dyn_in_axes</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(dyn_args</span><span class="s3">, </span><span class="s1">kwargs))</span>
    <span class="s1">ex = e(</span><span class="s4">'pmap in_axes'</span><span class="s1">)</span>
    <span class="s1">msg</span><span class="s3">, </span><span class="s1">= ex.args</span>
    <span class="s1">msg += (</span><span class="s4">&quot;</span><span class="s3">\n\n</span><span class="s4">The 'full pytree' here is the tuple of arguments passed &quot;</span>
            <span class="s4">&quot;positionally to the pmapped function, and the value of `in_axes` &quot;</span>
            <span class="s4">&quot;must be a tree prefix of that tuple. But it was not a prefix.&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">kwargs:</span>
      <span class="s1">msg += (</span><span class="s4">&quot;</span><span class="s3">\n\n</span><span class="s4">When some arguments are passed by keyword to the pmapped &quot;</span>
              <span class="s4">&quot;function, they are not included in the comparison to `in_axes`. &quot;</span>
              <span class="s4">&quot;Instead, each argument passed by keyword is mapped over its &quot;</span>
              <span class="s4">&quot;leading axis. See the description of `in_axes` in the `pmap` &quot;</span>
              <span class="s4">&quot;docstring: &quot;</span>
              <span class="s4">&quot;https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html#jax.pmap&quot;</span><span class="s1">)</span>
    <span class="s1">msg += (</span><span class="s4">&quot;</span><span class="s3">\n\n</span><span class="s4">Check that the value of the `in_axes` argument to `pmap` &quot;</span>
            <span class="s4">&quot;is a tree prefix of the tuple of arguments passed positionally to &quot;</span>
            <span class="s4">&quot;the pmapped function.&quot;</span><span class="s1">)</span>
    <span class="s3">raise </span><span class="s1">ValueError(msg) </span><span class="s3">from None</span>
  <span class="s1">local_axis_size = _mapped_axis_size(fun</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">in_axes_flat</span><span class="s3">, </span><span class="s4">&quot;pmap&quot;</span><span class="s1">)</span>

  <span class="s1">f</span><span class="s3">, </span><span class="s1">res_paths = result_paths(f)</span>
  <span class="s1">f</span><span class="s3">, </span><span class="s1">out_axes_thunk = flat_out_axes(f</span><span class="s3">, </span><span class="s1">out_axes)</span>
  <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">out_tree = flatten_fun(f</span><span class="s3">, </span><span class="s1">in_tree)</span>
  <span class="s1">flat_fun = debug_info_final(flat_fun</span><span class="s3">, </span><span class="s1">dbg</span><span class="s3">, </span><span class="s1">res_paths)</span>

  <span class="s1">is_explicit_global_axis_size = axis_size </span><span class="s3">is not None</span>
  <span class="s1">global_axis_size = _get_global_axis_size(local_axis_size</span><span class="s3">, </span><span class="s1">in_devices</span><span class="s3">,</span>
                                           <span class="s1">backend_name</span><span class="s3">, </span><span class="s1">axis_size)</span>
  <span class="s3">return </span><span class="s1">PmapCallInfo(flat_fun=flat_fun</span><span class="s3">,</span>
                      <span class="s1">in_tree=in_tree</span><span class="s3">,</span>
                      <span class="s1">out_tree=out_tree</span><span class="s3">,</span>
                      <span class="s1">flat_args=args</span><span class="s3">,</span>
                      <span class="s1">donated_invars=donated_invars</span><span class="s3">,</span>
                      <span class="s1">in_axes_flat=in_axes_flat</span><span class="s3">,</span>
                      <span class="s1">local_axis_size=local_axis_size</span><span class="s3">,</span>
                      <span class="s1">out_axes_thunk=out_axes_thunk</span><span class="s3">,</span>
                      <span class="s1">devices=</span><span class="s3">None if </span><span class="s1">in_devices </span><span class="s3">is None else </span><span class="s1">tuple(in_devices)</span><span class="s3">,</span>
                      <span class="s1">global_axis_size=global_axis_size</span><span class="s3">,</span>
                      <span class="s1">is_explicit_global_axis_size=is_explicit_global_axis_size)</span>


<span class="s3">def </span><span class="s1">_shared_code_pmap(fun</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">static_broadcasted_argnums</span><span class="s3">,</span>
                      <span class="s1">donate_argnums</span><span class="s3">, </span><span class="s1">in_axes</span><span class="s3">, </span><span class="s1">out_axes):</span>
  <span class="s0"># axis_size is an optional integer representing the global axis size.  The</span>
  <span class="s0"># aggregate size (across all processes) size of the mapped axis must match the</span>
  <span class="s0"># given value.</span>
  <span class="s1">check_callable(fun)</span>
  <span class="s1">axis_name = core._TempAxisName(fun) </span><span class="s3">if </span><span class="s1">axis_name </span><span class="s3">is None else </span><span class="s1">axis_name</span>
  <span class="s1">static_broadcasted_tuple = _ensure_index_tuple(static_broadcasted_argnums)</span>
  <span class="s1">donate_tuple = rebase_donate_argnums(</span>
      <span class="s1">_ensure_index_tuple(donate_argnums)</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple)</span>

  <span class="s3">if not </span><span class="s1">all(type(l) </span><span class="s3">is </span><span class="s1">int </span><span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">tree_leaves(in_axes)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;pmap in_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="s4">f&quot;with those types as leaves, but got </span><span class="s3">{</span><span class="s1">in_axes</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">all(type(l) </span><span class="s3">is </span><span class="s1">int </span><span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">tree_leaves(out_axes)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;pmap out_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="s4">f&quot;with those types as leaves, but got </span><span class="s3">{</span><span class="s1">out_axes</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple</span><span class="s3">, </span><span class="s1">donate_tuple</span>


<span class="s3">class </span><span class="s1">_PmapFastpathData(NamedTuple):</span>
  <span class="s1">version: int  </span><span class="s0"># For forward and backward compatibility</span>
  <span class="s1">xla_executable: xc.LoadedExecutable</span>
  <span class="s1">in_handler: Any</span>
  <span class="s1">out_handler: Any</span>
  <span class="s1">out_pytree_def: Any</span>
  <span class="s0"># Data needed to handle the inputs.</span>
  <span class="s1">input_sharding_specs: Sequence[sharding_specs.ShardingSpec]</span>
  <span class="s1">input_devices: Sequence[xc.Device]</span>
  <span class="s1">input_indices: Sequence[sharding_specs.Index]</span>
  <span class="s1">input_array_shardings: Sequence[Any]</span>
  <span class="s0"># Data needed to build the Array from C++.</span>
  <span class="s1">out_sharding_specs: Sequence[sharding_specs.ShardingSpec]</span>
  <span class="s1">out_indices: Sequence[sharding_specs.Index]</span>
  <span class="s1">out_avals: Sequence[Any]</span>
  <span class="s1">out_array_shardings: Sequence[Any]</span>
  <span class="s1">out_committed: Sequence[Any]</span>


<span class="s3">def </span><span class="s1">_cpp_pmap(</span>
    <span class="s1">fun: Callable</span><span class="s3">,</span>
    <span class="s1">axis_name: Optional[AxisName] = </span><span class="s3">None,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">in_axes=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">out_axes=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">static_broadcasted_argnums: Union[int</span><span class="s3">, </span><span class="s1">Iterable[int]] = ()</span><span class="s3">,</span>
    <span class="s1">devices: Optional[Sequence[xc.Device]] = </span><span class="s3">None,  </span><span class="s0"># noqa: F811</span>
    <span class="s1">backend: Optional[str] = </span><span class="s3">None,</span>
    <span class="s1">axis_size: Optional[int] = </span><span class="s3">None,</span>
    <span class="s1">donate_argnums: Union[int</span><span class="s3">, </span><span class="s1">Iterable[int]] = ()</span><span class="s3">,</span>
  <span class="s1">) -&gt; Any:</span>
  <span class="s1">axis_name</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple</span><span class="s3">, </span><span class="s1">donate_tuple = _shared_code_pmap(</span>
      <span class="s1">fun</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">static_broadcasted_argnums</span><span class="s3">, </span><span class="s1">donate_argnums</span><span class="s3">, </span><span class="s1">in_axes</span><span class="s3">,</span>
      <span class="s1">out_axes)</span>
  <span class="s3">del </span><span class="s1">static_broadcasted_argnums</span><span class="s3">, </span><span class="s1">donate_argnums</span>

  <span class="s1">@api_boundary</span>
  <span class="s3">def </span><span class="s1">cache_miss(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">p = _prepare_pmap(fun</span><span class="s3">, </span><span class="s1">in_axes</span><span class="s3">, </span><span class="s1">out_axes</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple</span><span class="s3">,</span>
                      <span class="s1">donate_tuple</span><span class="s3">, </span><span class="s1">devices</span><span class="s3">, </span><span class="s1">backend</span><span class="s3">,</span>
                      <span class="s1">axis_size</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs)</span>
    <span class="s3">for </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">p.flat_args:</span>
      <span class="s1">dispatch.check_arg(arg)</span>

    <span class="s1">params = dict(</span>
        <span class="s1">backend=backend</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
        <span class="s1">axis_size=p.local_axis_size</span><span class="s3">,</span>
        <span class="s1">global_axis_size=p.global_axis_size</span><span class="s3">,</span>
        <span class="s1">devices=p.devices</span><span class="s3">,</span>
        <span class="s1">in_axes=p.in_axes_flat</span><span class="s3">,</span>
        <span class="s1">out_axes_thunk=p.out_axes_thunk</span><span class="s3">,</span>
        <span class="s1">name=p.flat_fun.__name__</span><span class="s3">,</span>
        <span class="s1">donated_invars=p.donated_invars</span><span class="s3">,</span>
        <span class="s1">is_explicit_global_axis_size=p.is_explicit_global_axis_size</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s1">map_bind_continuation</span><span class="s3">, </span><span class="s1">top_trace</span><span class="s3">, </span><span class="s1">fun_</span><span class="s3">, </span><span class="s1">tracers</span><span class="s3">, </span><span class="s1">params = (</span>
        <span class="s1">core.map_bind_with_continuation(pxla.xla_pmap_p</span><span class="s3">, </span><span class="s1">p.flat_fun</span><span class="s3">,</span>
                                        <span class="s1">*p.flat_args</span><span class="s3">, </span><span class="s1">**params))</span>
    <span class="s1">execute: Optional[Callable] = </span><span class="s3">None</span>
    <span class="s3">if </span><span class="s1">isinstance(top_trace</span><span class="s3">, </span><span class="s1">core.EvalTrace):</span>
      <span class="s1">execute = pxla.xla_pmap_impl_lazy(fun_</span><span class="s3">, </span><span class="s1">*tracers</span><span class="s3">, </span><span class="s1">**params)</span>
      <span class="s1">out = map_bind_continuation(execute(*tracers))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">out = map_bind_continuation(</span>
          <span class="s1">pxla.xla_pmap_p.process(top_trace</span><span class="s3">, </span><span class="s1">fun_</span><span class="s3">, </span><span class="s1">tracers</span><span class="s3">, </span><span class="s1">params))</span>

    <span class="s1">out_tree</span><span class="s3">, </span><span class="s1">out_flat = p.out_tree</span><span class="s3">, </span><span class="s1">out</span>
    <span class="s1">out_pytree_def = out_tree()</span>
    <span class="s1">out = tree_unflatten(out_pytree_def</span><span class="s3">, </span><span class="s1">out_flat)</span>

    <span class="s0">### Decide whether we can support the C++ fast path</span>
    <span class="s1">use_fastpath = </span><span class="s3">False</span>
    <span class="s3">if </span><span class="s1">execute </span><span class="s3">is not None and </span><span class="s1">isinstance(execute</span><span class="s3">, </span><span class="s1">pxla.ExecuteReplicated):</span>
      <span class="s1">execute_replicated = typing.cast(pxla.ExecuteReplicated</span><span class="s3">, </span><span class="s1">execute)</span>
      <span class="s1">use_fastpath = (</span>
        <span class="s0"># TODO(sharadmv): Enable effects in replicated computation</span>
        <span class="s3">not </span><span class="s1">execute_replicated.has_unordered_effects</span>
        <span class="s3">and not </span><span class="s1">execute_replicated.has_host_callbacks </span><span class="s3">and</span>
        <span class="s0"># No tracers in the outputs.</span>
        <span class="s1">all(isinstance(x</span><span class="s3">, </span><span class="s1">xc.ArrayImpl) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">out_flat))</span>

    <span class="s0">### If we can use the fastpath, we return required info to the caller.</span>
    <span class="s3">if </span><span class="s1">use_fastpath:</span>
      <span class="s1">execute_replicated = typing.cast(pxla.ExecuteReplicated</span><span class="s3">, </span><span class="s1">execute)</span>
      <span class="s1">out_handler = execute_replicated.out_handler</span>
      <span class="s1">in_handler = execute_replicated.in_handler</span>
      <span class="s1">out_indices = [tuple(s.devices_indices_map(a.shape).values())</span>
                     <span class="s3">for </span><span class="s1">s</span><span class="s3">, </span><span class="s1">a </span><span class="s3">in </span><span class="s1">safe_zip(out_handler.out_shardings</span><span class="s3">, </span><span class="s1">out_handler.out_avals)]</span>

      <span class="s1">out_array_shardings = [out.sharding </span><span class="s3">for </span><span class="s1">out </span><span class="s3">in </span><span class="s1">out_flat]</span>
      <span class="s1">out_committed = [out._committed </span><span class="s3">for </span><span class="s1">out </span><span class="s3">in </span><span class="s1">out_flat]</span>

      <span class="s1">fastpath_data = _PmapFastpathData(</span>
          <span class="s1">version=</span><span class="s5">1</span><span class="s3">,</span>
          <span class="s1">xla_executable=execute_replicated.xla_executable</span><span class="s3">,</span>
          <span class="s1">in_handler=in_handler</span><span class="s3">,</span>
          <span class="s1">out_handler=out_handler</span><span class="s3">,</span>
          <span class="s1">out_pytree_def=out_pytree_def</span><span class="s3">,</span>
          <span class="s1">input_sharding_specs=[i.sharding_spec </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">in_handler.in_shardings]</span><span class="s3">,</span>
          <span class="s1">input_devices=in_handler.local_devices</span><span class="s3">,</span>
          <span class="s1">input_indices=in_handler.input_indices</span><span class="s3">,</span>
          <span class="s1">input_array_shardings=in_handler.in_shardings</span><span class="s3">,</span>
          <span class="s1">out_sharding_specs=[s.sharding_spec </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">out_handler.out_shardings]</span><span class="s3">,</span>
          <span class="s1">out_indices=out_indices</span><span class="s3">,</span>
          <span class="s1">out_avals=out_handler.out_avals</span><span class="s3">,</span>
          <span class="s1">out_array_shardings=out_array_shardings</span><span class="s3">,</span>
          <span class="s1">out_committed=out_committed</span><span class="s3">,</span>
      <span class="s1">)</span>

    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">fastpath_data = </span><span class="s3">None</span>

    <span class="s3">return </span><span class="s1">out</span><span class="s3">, </span><span class="s1">fastpath_data</span>

  <span class="s1">cpp_mapped_f = pmap_lib.pmap(</span>
      <span class="s1">fun</span><span class="s3">, </span><span class="s1">cache_miss</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple</span><span class="s3">, </span><span class="s1">pxla.shard_arg)</span>
  <span class="s1">_pmap_cache_clears.add(cpp_mapped_f)</span>

  <span class="s1">pmap_f = wraps(fun)(cpp_mapped_f)</span>

  <span class="s1">pmap_f.lower = _pmap_lower(</span>
      <span class="s1">fun</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">in_axes</span><span class="s3">, </span><span class="s1">out_axes</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple</span><span class="s3">, </span><span class="s1">devices</span><span class="s3">,</span>
      <span class="s1">backend</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">donate_tuple)</span>

  <span class="s3">return </span><span class="s1">pmap_f</span>

<span class="s1">_pmap_cache_clears = weakref.WeakSet()  </span><span class="s0"># type: ignore</span>


<span class="s3">def </span><span class="s1">_pmap_lower(fun</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">in_axes</span><span class="s3">, </span><span class="s1">out_axes</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple</span><span class="s3">,</span>
                <span class="s1">devices</span><span class="s3">, </span><span class="s1">backend</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">donate_tuple):  </span><span class="s0"># noqa: F811</span>
  <span class="s2">&quot;&quot;&quot;Make a ``lower`` method for pmapped functions.&quot;&quot;&quot;</span>
  <span class="s0"># If the function we returned from ``pmap`` were a class instance,</span>
  <span class="s0"># this might naturally be a method, with ``fun`` as a ``self`` and</span>
  <span class="s0"># all the other arguments stored as attributes.</span>
  <span class="s1">@api_boundary</span>
  <span class="s3">def </span><span class="s1">lower(*args</span><span class="s3">, </span><span class="s1">**kwargs) -&gt; stages.Lowered:</span>
    <span class="s2">&quot;&quot;&quot;Lower a parallel-mapped form of this function for the given arguments. 
 
    A parallel-mapped and lowered function is staged out of Python and 
    translated to a compiler's input language, possibly in a 
    backend-dependent manner. It is ready for compilation but is not yet 
    compiled. It represents a function intended for SPMD execution on 
    multiple devices. 
 
    Returns: 
      A ``Lowered`` instance representing the post-map lowering. 
    &quot;&quot;&quot;</span>
    <span class="s1">_experimental_lowering_platform = kwargs.pop(</span>
        <span class="s4">'_experimental_lowering_platform'</span><span class="s3">, None</span><span class="s1">)</span>
    <span class="s1">p = _prepare_pmap(</span>
        <span class="s1">fun</span><span class="s3">, </span><span class="s1">in_axes</span><span class="s3">, </span><span class="s1">out_axes</span><span class="s3">, </span><span class="s1">static_broadcasted_tuple</span><span class="s3">, </span><span class="s1">donate_tuple</span><span class="s3">,</span>
        <span class="s1">devices</span><span class="s3">, </span><span class="s1">backend</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs)</span>
    <span class="s1">abstract_args = list(map(shaped_abstractify</span><span class="s3">, </span><span class="s1">p.flat_args))</span>
    <span class="s1">computation = pxla.lower_parallel_callable(</span>
        <span class="s1">p.flat_fun</span><span class="s3">, </span><span class="s1">backend</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
        <span class="s1">axis_size=p.local_axis_size</span><span class="s3">, </span><span class="s1">global_axis_size=p.global_axis_size</span><span class="s3">,</span>
        <span class="s1">devices=p.devices</span><span class="s3">,</span>
        <span class="s1">name=p.flat_fun.__name__</span><span class="s3">,</span>
        <span class="s1">in_axes=p.in_axes_flat</span><span class="s3">,</span>
        <span class="s1">out_axes_thunk=p.out_axes_thunk</span><span class="s3">,</span>
        <span class="s1">donated_invars=p.donated_invars</span><span class="s3">,</span>
        <span class="s1">is_explicit_global_axis_size=p.is_explicit_global_axis_size</span><span class="s3">,</span>
        <span class="s1">avals=abstract_args</span><span class="s3">,</span>
        <span class="s1">lowering_platform=_experimental_lowering_platform)</span>
    <span class="s3">return </span><span class="s1">stages.Lowered.from_flat_info(</span>
        <span class="s1">computation</span><span class="s3">, </span><span class="s1">p.in_tree</span><span class="s3">, </span><span class="s1">abstract_args</span><span class="s3">, </span><span class="s1">donate_tuple</span><span class="s3">, </span><span class="s1">p.out_tree())</span>

  <span class="s3">return </span><span class="s1">lower</span>

<span class="s3">def </span><span class="s1">jvp(</span>
    <span class="s1">fun: Callable</span><span class="s3">, </span><span class="s1">primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">has_aux: bool = </span><span class="s3">False</span>
  <span class="s1">) -&gt; Tuple[Any</span><span class="s3">, </span><span class="s1">...]:</span>
  <span class="s2">&quot;&quot;&quot;Computes a (forward-mode) Jacobian-vector product of ``fun``. 
 
  Args: 
    fun: Function to be differentiated. Its arguments should be arrays, scalars, 
      or standard Python containers of arrays or scalars. It should return an 
      array, scalar, or standard Python container of arrays or scalars. 
    primals: The primal values at which the Jacobian of ``fun`` should be 
      evaluated. Should be either a tuple or a list of arguments, 
      and its length should be equal to the number of positional parameters of 
      ``fun``. 
    tangents: The tangent vector for which the Jacobian-vector product should be 
      evaluated. Should be either a tuple or a list of tangents, with the same 
      tree structure and array shapes as ``primals``. 
    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the 
     first element is considered the output of the mathematical function to be 
     differentiated and the second element is auxiliary data. Default False. 
 
  Returns: 
    If ``has_aux`` is ``False``, returns a ``(primals_out, tangents_out)`` pair, 
    where ``primals_out`` is ``fun(*primals)``, 
    and ``tangents_out`` is the Jacobian-vector product of 
    ``function`` evaluated at ``primals`` with ``tangents``. The 
    ``tangents_out`` value has the same Python tree structure and shapes as 
    ``primals_out``. If ``has_aux`` is ``True``, returns a 
    ``(primals_out, tangents_out, aux)`` tuple where ``aux`` 
    is the auxiliary data returned by ``fun``. 
 
  For example: 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; primals, tangents = jax.jvp(jax.numpy.sin, (0.1,), (0.2,)) 
  &gt;&gt;&gt; print(primals) 
  0.09983342 
  &gt;&gt;&gt; print(tangents) 
  0.19900084 
  &quot;&quot;&quot;</span>
  <span class="s1">check_callable(fun)</span>
  <span class="s3">return </span><span class="s1">_jvp(lu.wrap_init(fun)</span><span class="s3">, </span><span class="s1">primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">has_aux=has_aux)</span>

<span class="s3">def </span><span class="s1">_jvp(fun: lu.WrappedFun</span><span class="s3">, </span><span class="s1">primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">has_aux=</span><span class="s3">False</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Variant of jvp() that takes an lu.WrappedFun.&quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">(</span><span class="s3">not </span><span class="s1">isinstance(primals</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)) </span><span class="s3">or</span>
      <span class="s3">not </span><span class="s1">isinstance(tangents</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list))):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;primal and tangent arguments to jax.jvp must be tuples or lists; &quot;</span>
                    <span class="s4">f&quot;found </span><span class="s3">{</span><span class="s1">type(primals).__name__</span><span class="s3">} </span><span class="s4">and </span><span class="s3">{</span><span class="s1">type(tangents).__name__</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>

  <span class="s1">ps_flat</span><span class="s3">, </span><span class="s1">tree_def = tree_flatten(primals)</span>
  <span class="s1">ts_flat</span><span class="s3">, </span><span class="s1">tree_def_2 = tree_flatten(tangents)</span>
  <span class="s3">if </span><span class="s1">tree_def != tree_def_2:</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;primal and tangent arguments to jax.jvp must have the same tree &quot;</span>
                    <span class="s4">f&quot;structure; primals have tree structure </span><span class="s3">{</span><span class="s1">tree_def</span><span class="s3">} </span><span class="s4">whereas tangents have &quot;</span>
                    <span class="s4">f&quot;tree structure </span><span class="s3">{</span><span class="s1">tree_def_2</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
  <span class="s3">for </span><span class="s1">p</span><span class="s3">, </span><span class="s1">t </span><span class="s3">in </span><span class="s1">safe_zip(ps_flat</span><span class="s3">, </span><span class="s1">ts_flat):</span>
    <span class="s3">if </span><span class="s1">core.primal_dtype_to_tangent_dtype(_dtype(p)) != _dtype(t):</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;primal and tangent arguments to jax.jvp do not match; &quot;</span>
                      <span class="s4">&quot;dtypes must be equal, or in case of int/bool primal dtype &quot;</span>
                      <span class="s4">&quot;the tangent dtype must be float0.&quot;</span>
                      <span class="s4">f&quot;Got primal dtype </span><span class="s3">{</span><span class="s1">_dtype(p)</span><span class="s3">} </span><span class="s4">and so expected tangent dtype &quot;</span>
                      <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">core.primal_dtype_to_tangent_dtype(_dtype(p))</span><span class="s3">}</span><span class="s4">, but got &quot;</span>
                      <span class="s4">f&quot;tangent dtype </span><span class="s3">{</span><span class="s1">_dtype(t)</span><span class="s3">} </span><span class="s4">instead.&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">np.shape(p) != np.shape(t):</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;jvp called with different primal and tangent shapes;&quot;</span>
                       <span class="s4">f&quot;Got primal shape </span><span class="s3">{</span><span class="s1">np.shape(p)</span><span class="s3">} </span><span class="s4">and tangent shape as </span><span class="s3">{</span><span class="s1">np.shape(t)</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

  <span class="s3">if not </span><span class="s1">has_aux:</span>
    <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">out_tree = flatten_fun_nokwargs(fun</span><span class="s3">, </span><span class="s1">tree_def)</span>
    <span class="s1">out_primals</span><span class="s3">, </span><span class="s1">out_tangents = ad.jvp(flat_fun).call_wrapped(ps_flat</span><span class="s3">, </span><span class="s1">ts_flat)</span>
    <span class="s1">out_tree = out_tree()</span>
    <span class="s3">return </span><span class="s1">(tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out_primals)</span><span class="s3">,</span>
            <span class="s1">tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out_tangents))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">out_aux_trees = flatten_fun_nokwargs2(fun</span><span class="s3">, </span><span class="s1">tree_def)</span>
    <span class="s1">jvp_fun</span><span class="s3">, </span><span class="s1">aux = ad.jvp(flat_fun</span><span class="s3">, </span><span class="s1">has_aux=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">out_primals</span><span class="s3">, </span><span class="s1">out_tangents = jvp_fun.call_wrapped(ps_flat</span><span class="s3">, </span><span class="s1">ts_flat)</span>
    <span class="s1">out_tree</span><span class="s3">, </span><span class="s1">aux_tree = out_aux_trees()</span>
    <span class="s3">return </span><span class="s1">(tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out_primals)</span><span class="s3">,</span>
            <span class="s1">tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out_tangents)</span><span class="s3">,</span>
            <span class="s1">tree_unflatten(aux_tree</span><span class="s3">, </span><span class="s1">aux()))</span>

<span class="s3">def </span><span class="s1">linearize(fun: Callable</span><span class="s3">, </span><span class="s1">*primals) -&gt; Tuple[Any</span><span class="s3">, </span><span class="s1">Callable]:</span>
  <span class="s2">&quot;&quot;&quot;Produces a linear approximation to ``fun`` using :py:func:`jvp` and partial eval. 
 
  Args: 
    fun: Function to be differentiated. Its arguments should be arrays, scalars, 
      or standard Python containers of arrays or scalars. It should return an 
      array, scalar, or standard python container of arrays or scalars. 
    primals: The primal values at which the Jacobian of ``fun`` should be 
      evaluated. Should be a tuple of arrays, scalar, or standard Python 
      container thereof. The length of the tuple is equal to the number of 
      positional parameters of ``fun``. 
 
  Returns: 
    A pair where the first element is the value of ``f(*primals)`` and the 
    second element is a function that evaluates the (forward-mode) 
    Jacobian-vector product of ``fun`` evaluated at ``primals`` without re-doing 
    the linearization work. 
 
  In terms of values computed, :py:func:`linearize` behaves much like a curried 
  :py:func:`jvp`, where these two code blocks compute the same values:: 
 
    y, out_tangent = jax.jvp(f, (x,), (in_tangent,)) 
 
    y, f_jvp = jax.linearize(f, x) 
    out_tangent = f_jvp(in_tangent) 
 
  However, the difference is that :py:func:`linearize` uses partial evaluation 
  so that the function ``f`` is not re-linearized on calls to ``f_jvp``. In 
  general that means the memory usage scales with the size of the computation, 
  much like in reverse-mode. (Indeed, :py:func:`linearize` has a similar 
  signature to :py:func:`vjp`!) 
 
  This function is mainly useful if you want to apply ``f_jvp`` multiple times, 
  i.e. to evaluate a pushforward for many different input tangent vectors at the 
  same linearization point. Moreover if all the input tangent vectors are known 
  at once, it can be more efficient to vectorize using :py:func:`vmap`, as in:: 
 
    pushfwd = partial(jvp, f, (x,)) 
    y, out_tangents = vmap(pushfwd, out_axes=(None, 0))((in_tangents,)) 
 
  By using :py:func:`vmap` and :py:func:`jvp` together like this we avoid the stored-linearization 
  memory cost that scales with the depth of the computation, which is incurred 
  by both :py:func:`linearize` and :py:func:`vjp`. 
 
  Here's a more complete example of using :py:func:`linearize`: 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; import jax.numpy as jnp 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; def f(x): return 3. * jnp.sin(x) + jnp.cos(x / 2.) 
  ... 
  &gt;&gt;&gt; jax.jvp(f, (2.,), (3.,)) 
  (Array(3.26819, dtype=float32, weak_type=True), Array(-5.00753, dtype=float32, weak_type=True)) 
  &gt;&gt;&gt; y, f_jvp = jax.linearize(f, 2.) 
  &gt;&gt;&gt; print(y) 
  3.2681944 
  &gt;&gt;&gt; print(f_jvp(3.)) 
  -5.007528 
  &gt;&gt;&gt; print(f_jvp(4.)) 
  -6.676704 
  &quot;&quot;&quot;</span>
  <span class="s1">check_callable(fun)</span>
  <span class="s1">f = lu.wrap_init(fun)</span>
  <span class="s1">primals_flat</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten((primals</span><span class="s3">, </span><span class="s1">{}))</span>
  <span class="s1">jaxtree_fun</span><span class="s3">, </span><span class="s1">out_tree = flatten_fun(f</span><span class="s3">, </span><span class="s1">in_tree)</span>
  <span class="s1">out_primals</span><span class="s3">, </span><span class="s1">out_pvals</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">consts = ad.linearize(jaxtree_fun</span><span class="s3">, </span><span class="s1">*primals_flat)</span>
  <span class="s1">out_tree = out_tree()</span>
  <span class="s1">out_primal_py = tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out_primals)</span>
  <span class="s1">primal_avals = list(map(core.get_aval</span><span class="s3">, </span><span class="s1">primals_flat))</span>
  <span class="s0"># Ensure that lifted_jvp is a PyTree</span>
  <span class="s1">lifted_jvp = Partial(partial(_lift_linearized</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">primal_avals</span><span class="s3">,</span>
                               <span class="s1">(in_tree</span><span class="s3">, </span><span class="s1">out_tree)</span><span class="s3">, </span><span class="s1">out_pvals)</span><span class="s3">, </span><span class="s1">consts)</span>
  <span class="s3">return </span><span class="s1">out_primal_py</span><span class="s3">, </span><span class="s1">lifted_jvp</span>

<span class="s3">def </span><span class="s1">_lift_linearized(jaxpr</span><span class="s3">, </span><span class="s1">primal_avals</span><span class="s3">, </span><span class="s1">io_tree</span><span class="s3">, </span><span class="s1">out_pvals</span><span class="s3">, </span><span class="s1">consts</span><span class="s3">, </span><span class="s1">*py_args):</span>
  <span class="s3">def </span><span class="s1">fun(*tangents):</span>
    <span class="s1">tangent_avals = list(map(core.get_aval</span><span class="s3">, </span><span class="s1">tangents))</span>
    <span class="s3">for </span><span class="s1">primal_aval</span><span class="s3">, </span><span class="s1">tangent_aval </span><span class="s3">in </span><span class="s1">zip(primal_avals</span><span class="s3">, </span><span class="s1">tangent_avals):</span>
      <span class="s3">if not </span><span class="s1">core.typecompat(primal_aval.at_least_vspace()</span><span class="s3">, </span><span class="s1">tangent_aval):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;linearized function called on tangent values inconsistent with &quot;</span>
                         <span class="s4">&quot;the original primal values: &quot;</span>
                         <span class="s4">f&quot;got </span><span class="s3">{</span><span class="s1">tangent_aval</span><span class="s3">} </span><span class="s4">for primal aval </span><span class="s3">{</span><span class="s1">primal_aval</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s1">tangents_out = eval_jaxpr(jaxpr</span><span class="s3">, </span><span class="s1">consts</span><span class="s3">, </span><span class="s1">*tangents)</span>
    <span class="s1">tangents_out_ = iter(tangents_out)</span>
    <span class="s1">full_out = [pval.get_known() </span><span class="s3">if </span><span class="s1">pval.is_known() </span><span class="s3">else </span><span class="s1">next(tangents_out_)</span>
                <span class="s3">for </span><span class="s1">pval </span><span class="s3">in </span><span class="s1">out_pvals]</span>
    <span class="s3">assert </span><span class="s1">next(tangents_out_</span><span class="s3">, None</span><span class="s1">) </span><span class="s3">is None</span>
    <span class="s3">return </span><span class="s1">full_out</span>

  <span class="s3">return </span><span class="s1">apply_flat_fun(fun</span><span class="s3">, </span><span class="s1">io_tree</span><span class="s3">, </span><span class="s1">*py_args)</span>

<span class="s3">def </span><span class="s1">_vjp_pullback_wrapper(name</span><span class="s3">, </span><span class="s1">cotangent_dtypes</span><span class="s3">, </span><span class="s1">cotangent_shapes</span><span class="s3">, </span><span class="s1">io_tree</span><span class="s3">,</span>
                          <span class="s1">fun</span><span class="s3">, </span><span class="s1">*py_args_):</span>
  <span class="s3">if </span><span class="s1">len(py_args_) != </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s1">msg = (</span><span class="s4">f&quot;The function returned by `jax.vjp` applied to </span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">was called &quot;</span>
           <span class="s4">f&quot;with </span><span class="s3">{</span><span class="s1">len(py_args_)</span><span class="s3">} </span><span class="s4">arguments, but functions returned by &quot;</span>
           <span class="s4">&quot;`jax.vjp` must be called with a single argument corresponding to &quot;</span>
           <span class="s4">f&quot;the single value returned by </span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s4">(even if that returned &quot;</span>
           <span class="s4">&quot;value is a tuple or other container).</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;For example, if we have:</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;  def f(x):</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;    return (x, x)</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;  _, f_vjp = jax.vjp(f, 1.0)</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;the function `f` returns a single tuple as output, and so we call &quot;</span>
           <span class="s4">&quot;`f_vjp` with a single tuple as its argument:</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;  x_bar, = f_vjp((2.0, 2.0))</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
           <span class="s4">&quot;If we instead call `f_vjp(2.0, 2.0)`, with the values 'splatted &quot;</span>
           <span class="s4">&quot;out' as arguments rather than in a tuple, this error can arise.&quot;</span><span class="s1">)</span>
    <span class="s3">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s1">py_args</span><span class="s3">, </span><span class="s1">= py_args_</span>
  <span class="s1">in_tree_expected</span><span class="s3">, </span><span class="s1">out_tree = io_tree</span>
  <span class="s1">args</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten(py_args)</span>
  <span class="s3">if </span><span class="s1">in_tree != in_tree_expected:</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;Tree structure of cotangent input </span><span class="s3">{</span><span class="s1">in_tree</span><span class="s3">}</span><span class="s4">, does not match structure of &quot;</span>
                    <span class="s4">f&quot;primal output </span><span class="s3">{</span><span class="s1">in_tree_expected</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
  <span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">ct_dtype</span><span class="s3">, </span><span class="s1">ct_shape </span><span class="s3">in </span><span class="s1">safe_zip(args</span><span class="s3">, </span><span class="s1">cotangent_dtypes</span><span class="s3">, </span><span class="s1">cotangent_shapes):</span>
    <span class="s1">expected_tangent_dtype = core.primal_dtype_to_tangent_dtype(_dtype(arg))</span>
    <span class="s3">if </span><span class="s1">expected_tangent_dtype != ct_dtype:</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span>
          <span class="s4">f&quot;Type of cotangent input to vjp pullback function (</span><span class="s3">{</span><span class="s1">ct_dtype</span><span class="s3">}</span><span class="s4">) is not &quot;</span>
          <span class="s4">f&quot;the expected tangent type (</span><span class="s3">{</span><span class="s1">expected_tangent_dtype</span><span class="s3">}</span><span class="s4">) of corresponding primal output &quot;</span>
          <span class="s4">f&quot;with dtype </span><span class="s3">{</span><span class="s1">_dtype(arg)</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">np.shape(arg) != ct_shape:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span>
          <span class="s4">f&quot;Shape of cotangent input to vjp pullback function </span><span class="s3">{</span><span class="s1">np.shape(arg)</span><span class="s3">} </span><span class="s4">&quot;</span>
          <span class="s4">&quot;must be the same as the shape of corresponding primal input &quot;</span>
          <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">ct_shape</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
  <span class="s1">ans = fun(*args)</span>
  <span class="s3">return </span><span class="s1">tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">ans)</span>

<span class="s1">@overload</span>
<span class="s3">def </span><span class="s1">vjp(fun: Callable[...</span><span class="s3">, </span><span class="s1">T]</span><span class="s3">,</span>
        <span class="s1">*primals: Any</span><span class="s3">,</span>
        <span class="s1">has_aux: Literal[</span><span class="s3">False</span><span class="s1">] = </span><span class="s3">False,</span>
        <span class="s1">reduce_axes: Sequence[AxisName] = ()) -&gt; Tuple[T</span><span class="s3">, </span><span class="s1">Callable]:</span>
  <span class="s1">...</span>

<span class="s1">@overload</span>
<span class="s3">def </span><span class="s1">vjp(fun: Callable[...</span><span class="s3">, </span><span class="s1">Tuple[T</span><span class="s3">, </span><span class="s1">U]]</span><span class="s3">, </span><span class="s1">*primals: Any</span><span class="s3">,</span>
        <span class="s1">has_aux: Literal[</span><span class="s3">True</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">reduce_axes: Sequence[AxisName] = ()) -&gt; Tuple[T</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">U]:</span>
  <span class="s1">...</span>
<span class="s3">def </span><span class="s1">vjp(  </span><span class="s0"># type: ignore</span>
    <span class="s1">fun: Callable</span><span class="s3">, </span><span class="s1">*primals</span><span class="s3">, </span><span class="s1">has_aux: bool = </span><span class="s3">False, </span><span class="s1">reduce_axes=()</span>
  <span class="s1">) -&gt; Union[Tuple[Any</span><span class="s3">, </span><span class="s1">Callable]</span><span class="s3">, </span><span class="s1">Tuple[Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Any]]:</span>
  <span class="s2">&quot;&quot;&quot;Compute a (reverse-mode) vector-Jacobian product of ``fun``. 
 
  :py:func:`grad` is implemented as a special case of :py:func:`vjp`. 
 
  Args: 
    fun: Function to be differentiated. Its arguments should be arrays, scalars, 
      or standard Python containers of arrays or scalars. It should return an 
      array, scalar, or standard Python container of arrays or scalars. 
    primals: A sequence of primal values at which the Jacobian of ``fun`` 
      should be evaluated. The number of ``primals`` should be equal to the 
      number of positional parameters of ``fun``. Each primal value should be 
      an array, a scalar, or a pytree (standard Python containers) thereof. 
    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the 
     first element is considered the output of the mathematical function to be 
     differentiated and the second element is auxiliary data. Default False. 
    reduce_axes: Optional, tuple of axis names. If an axis is listed here, and 
      ``fun`` implicitly broadcasts a value over that axis, the backward pass 
      will perform a ``psum`` of the corresponding gradient. Otherwise, the 
      VJP will be per-example over named axes. For example, if ``'batch'`` 
      is a named batch axis, ``vjp(f, *args, reduce_axes=('batch',))`` will 
      create a VJP function that sums over the batch while ``vjp(f, *args)`` 
      will create a per-example VJP. 
 
  Returns: 
    If ``has_aux`` is ``False``, returns a ``(primals_out, vjpfun)`` pair, where 
    ``primals_out`` is ``fun(*primals)``. If ``has_aux`` is ``True``, returns a 
    ``(primals_out, vjpfun, aux)`` tuple where ``aux`` is the auxiliary data 
    returned by ``fun``. 
 
    ``vjpfun`` is a function from a cotangent vector with the same shape as 
    ``primals_out`` to a tuple of cotangent vectors with the same number and 
    shapes as ``primals``, representing the vector-Jacobian product of ``fun`` 
    evaluated at ``primals``. 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; def f(x, y): 
  ...   return jax.numpy.sin(x), jax.numpy.cos(y) 
  ... 
  &gt;&gt;&gt; primals, f_vjp = jax.vjp(f, 0.5, 1.0) 
  &gt;&gt;&gt; xbar, ybar = f_vjp((-0.7, 0.3)) 
  &gt;&gt;&gt; print(xbar) 
  -0.61430776 
  &gt;&gt;&gt; print(ybar) 
  -0.2524413 
  &quot;&quot;&quot;</span>
  <span class="s1">check_callable(fun)</span>
  <span class="s1">reduce_axes = _ensure_str_tuple(reduce_axes)  </span><span class="s0"># type: ignore</span>
  <span class="s3">return </span><span class="s1">_vjp(</span>
      <span class="s1">lu.wrap_init(fun)</span><span class="s3">, </span><span class="s1">*primals</span><span class="s3">, </span><span class="s1">has_aux=has_aux</span><span class="s3">, </span><span class="s1">reduce_axes=reduce_axes)</span>

<span class="s3">def </span><span class="s1">_vjp(fun: lu.WrappedFun</span><span class="s3">, </span><span class="s1">*primals</span><span class="s3">, </span><span class="s1">has_aux=</span><span class="s3">False, </span><span class="s1">reduce_axes=()):</span>
  <span class="s2">&quot;&quot;&quot;Variant of vjp() that takes an lu.WrappedFun.&quot;&quot;&quot;</span>
  <span class="s1">primals_flat</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten(primals)</span>
  <span class="s3">for </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">primals_flat: dispatch.check_arg(arg)</span>
  <span class="s3">if not </span><span class="s1">has_aux:</span>
    <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">out_tree = flatten_fun_nokwargs(fun</span><span class="s3">, </span><span class="s1">in_tree)</span>
    <span class="s1">out_primal</span><span class="s3">, </span><span class="s1">out_vjp = ad.vjp(</span>
        <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">primals_flat</span><span class="s3">, </span><span class="s1">reduce_axes=reduce_axes)</span>
    <span class="s1">out_tree = out_tree()</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">out_aux_trees = flatten_fun_nokwargs2(fun</span><span class="s3">, </span><span class="s1">in_tree)</span>
    <span class="s1">out_primal</span><span class="s3">, </span><span class="s1">out_vjp</span><span class="s3">, </span><span class="s1">aux = ad.vjp(</span>
        <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">primals_flat</span><span class="s3">, </span><span class="s1">has_aux=</span><span class="s3">True, </span><span class="s1">reduce_axes=reduce_axes)</span>
    <span class="s1">out_tree</span><span class="s3">, </span><span class="s1">aux_tree = out_aux_trees()</span>
  <span class="s1">out_primal_py = tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out_primal)</span>
  <span class="s1">ct_dtypes = [core.primal_dtype_to_tangent_dtype(_dtype(x)) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">out_primal]</span>
  <span class="s1">ct_shapes = [np.shape(x) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">out_primal]</span>
  <span class="s0"># Ensure that vjp_py is a PyTree so that we can pass it from the forward to the</span>
  <span class="s0"># backward pass in a custom VJP.</span>
  <span class="s1">vjp_py = Partial(partial(_vjp_pullback_wrapper</span><span class="s3">, </span><span class="s1">fun.__name__</span><span class="s3">,</span>
                           <span class="s1">ct_dtypes</span><span class="s3">, </span><span class="s1">ct_shapes</span><span class="s3">, </span><span class="s1">(out_tree</span><span class="s3">, </span><span class="s1">in_tree))</span><span class="s3">,</span>
                   <span class="s1">out_vjp)</span>
  <span class="s3">if not </span><span class="s1">has_aux:</span>
    <span class="s3">return </span><span class="s1">out_primal_py</span><span class="s3">, </span><span class="s1">vjp_py</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">out_primal_py</span><span class="s3">, </span><span class="s1">vjp_py</span><span class="s3">, </span><span class="s1">tree_unflatten(aux_tree</span><span class="s3">, </span><span class="s1">aux)</span>


<span class="s3">def </span><span class="s1">linear_transpose(fun: Callable</span><span class="s3">, </span><span class="s1">*primals</span><span class="s3">, </span><span class="s1">reduce_axes=()) -&gt; Callable:</span>
  <span class="s2">&quot;&quot;&quot;Transpose a function that is promised to be linear. 
 
  For linear functions, this transformation is equivalent to :py:func:`vjp`, but 
  avoids the overhead of computing the forward pass. 
 
  The outputs of the transposed function will always have the exact same dtypes 
  as ``primals``, even if some values are truncated (e.g., from complex to 
  float, or from float64 to float32). To avoid truncation, use dtypes in 
  ``primals`` that match the full range of desired outputs from the transposed 
  function. Integer dtypes are not supported. 
 
  Args: 
    fun: the linear function to be transposed. 
    *primals: a positional argument tuple of arrays, scalars, or (nested) 
      standard Python containers (tuples, lists, dicts, namedtuples, i.e., 
      pytrees) of those types used for evaluating the shape/dtype of 
      ``fun(*primals)``. These arguments may be real scalars/ndarrays, but that 
      is not required: only the ``shape`` and ``dtype`` attributes are accessed. 
      See below for an example. (Note that the duck-typed objects cannot be 
      namedtuples because those are treated as standard Python containers.) 
    reduce_axes: Optional, tuple of axis names. If an axis is listed here, and 
      ``fun`` implicitly broadcasts a value over that axis, the backward pass 
      will perform a ``psum`` of the corresponding cotangent. Otherwise, the 
      transposed function will be per-example over named axes. For example, if 
      ``'batch'`` is a named batch axis, ``linear_transpose(f, *args, 
      reduce_axes=('batch',))`` will create a transpose function that sums over 
      the batch while ``linear_transpose(f, args)`` will create a per-example 
      transpose. 
 
  Returns: 
    A callable that calculates the transpose of ``fun``. Valid input into this 
    function must have the same shape/dtypes/structure as the result of 
    ``fun(*primals)``. Output will be a tuple, with the same 
    shape/dtypes/structure as ``primals``. 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; import types 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; f = lambda x, y: 0.5 * x - 0.5 * y 
  &gt;&gt;&gt; scalar = types.SimpleNamespace(shape=(), dtype=np.dtype(np.float32)) 
  &gt;&gt;&gt; f_transpose = jax.linear_transpose(f, scalar, scalar) 
  &gt;&gt;&gt; f_transpose(1.0) 
  (Array(0.5, dtype=float32), Array(-0.5, dtype=float32)) 
  &quot;&quot;&quot;</span>
  <span class="s1">reduce_axes = _ensure_str_tuple(reduce_axes)</span>
  <span class="s1">primals_flat</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten(primals)</span>
  <span class="s1">flat_fun</span><span class="s3">, </span><span class="s1">out_tree = flatten_fun_nokwargs(lu.wrap_init(fun)</span><span class="s3">, </span><span class="s1">in_tree)</span>
  <span class="s1">in_avals = map(shaped_abstractify</span><span class="s3">, </span><span class="s1">primals_flat)</span>
  <span class="s1">in_dtypes = map(dtypes.dtype</span><span class="s3">, </span><span class="s1">in_avals)</span>

  <span class="s1">in_pvals = map(pe.PartialVal.unknown</span><span class="s3">, </span><span class="s1">in_avals)</span>
  <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">out_pvals</span><span class="s3">, </span><span class="s1">const = pe.trace_to_jaxpr_nounits(flat_fun</span><span class="s3">, </span><span class="s1">in_pvals</span><span class="s3">,</span>
                                                      <span class="s1">instantiate=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">_ = pe.dce_jaxpr(jaxpr</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True</span><span class="s1">] * len(jaxpr.outvars)</span><span class="s3">, True</span><span class="s1">)</span>
  <span class="s1">out_avals</span><span class="s3">, </span><span class="s1">_ = unzip2(out_pvals)</span>
  <span class="s1">out_dtypes = map(dtypes.dtype</span><span class="s3">, </span><span class="s1">out_avals)</span>
  <span class="s3">if not </span><span class="s1">(all(dtypes.issubdtype(d</span><span class="s3">, </span><span class="s1">np.inexact) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">in_dtypes + out_dtypes)</span>
          <span class="s3">or </span><span class="s1">all(dtypes.issubdtype(d</span><span class="s3">, </span><span class="s1">np.integer)</span>
                 <span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">in_dtypes + out_dtypes)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;linear_transpose only supports [float or complex] -&gt; &quot;</span>
                    <span class="s4">&quot;[float or complex], and integer -&gt; integer functions, &quot;</span>
                    <span class="s4">f&quot;but got </span><span class="s3">{</span><span class="s1">in_dtypes</span><span class="s3">} </span><span class="s4">-&gt; </span><span class="s3">{</span><span class="s1">out_dtypes</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>

  <span class="s1">@api_boundary</span>
  <span class="s3">def </span><span class="s1">transposed_fun(const</span><span class="s3">, </span><span class="s1">out_cotangent):</span>
    <span class="s1">out_cts</span><span class="s3">, </span><span class="s1">out_tree2 = tree_flatten(out_cotangent)</span>
    <span class="s3">if </span><span class="s1">out_tree() != out_tree2:</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;cotangent tree does not match function output, &quot;</span>
                      <span class="s4">f&quot;expected </span><span class="s3">{</span><span class="s1">out_tree()</span><span class="s3">} </span><span class="s4">but got </span><span class="s3">{</span><span class="s1">out_tree2</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s3">if not </span><span class="s1">all(map(core.typecheck</span><span class="s3">, </span><span class="s1">out_avals</span><span class="s3">, </span><span class="s1">out_cts)):</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;cotangent type does not match function output, &quot;</span>
                      <span class="s4">f&quot;expected </span><span class="s3">{</span><span class="s1">out_avals</span><span class="s3">} </span><span class="s4">but got </span><span class="s3">{</span><span class="s1">out_cts</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s1">dummies = [ad.UndefinedPrimal(a) </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">in_avals]</span>
    <span class="s1">in_cts = ad.backward_pass(jaxpr</span><span class="s3">, </span><span class="s1">reduce_axes</span><span class="s3">, True, </span><span class="s1">const</span><span class="s3">, </span><span class="s1">dummies</span><span class="s3">, </span><span class="s1">out_cts)</span>
    <span class="s1">in_cts = map(ad.instantiate_zeros</span><span class="s3">, </span><span class="s1">in_cts)</span>
    <span class="s3">return </span><span class="s1">tree_unflatten(in_tree</span><span class="s3">, </span><span class="s1">in_cts)</span>

  <span class="s0"># Ensure that transposed_fun is a PyTree</span>
  <span class="s3">return </span><span class="s1">Partial(transposed_fun</span><span class="s3">, </span><span class="s1">const)</span>


<span class="s3">def </span><span class="s1">_flat_axes_specs(abstracted_axes</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs</span>
                     <span class="s1">) -&gt; List[pe.AbstractedAxesSpec]:</span>
  <span class="s3">if </span><span class="s1">kwargs: </span><span class="s3">raise </span><span class="s1">NotImplementedError</span>
  <span class="s3">def </span><span class="s1">ax_leaf(l):</span>
    <span class="s3">return </span><span class="s1">(isinstance(l</span><span class="s3">, </span><span class="s1">dict) </span><span class="s3">and </span><span class="s1">all_leaves(l.values()) </span><span class="s3">or</span>
            <span class="s1">isinstance(l</span><span class="s3">, </span><span class="s1">tuple) </span><span class="s3">and </span><span class="s1">all_leaves(l</span><span class="s3">, lambda </span><span class="s1">x: x </span><span class="s3">is None</span><span class="s1">))</span>
  <span class="s3">return </span><span class="s1">broadcast_prefix(abstracted_axes</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">ax_leaf)</span>


<span class="s0"># TODO(phawkins): for some reason mypy cannot determine these overloads are</span>
<span class="s0"># non-overlapping. Pytype is happy with them.</span>
<span class="s1">@overload</span>
<span class="s3">def </span><span class="s1">make_jaxpr(fun: Callable</span><span class="s3">,  </span><span class="s0"># type: ignore</span>
               <span class="s1">static_argnums: Union[int</span><span class="s3">, </span><span class="s1">Iterable[int]] = ()</span><span class="s3">,</span>
               <span class="s1">axis_env: Optional[Sequence[Tuple[AxisName</span><span class="s3">, </span><span class="s1">int]]] = </span><span class="s3">None,</span>
               <span class="s1">return_shape: Literal[</span><span class="s3">False</span><span class="s1">] = ...</span><span class="s3">,</span>
               <span class="s1">abstracted_axes: Optional[Any] = </span><span class="s3">None,</span>
               <span class="s1">) -&gt; Callable[...</span><span class="s3">, </span><span class="s1">core.ClosedJaxpr]:</span>
  <span class="s1">...</span>

<span class="s1">@overload</span>
<span class="s3">def </span><span class="s1">make_jaxpr(fun: Callable</span><span class="s3">,  </span><span class="s0"># type: ignore</span>
               <span class="s1">static_argnums: Union[int</span><span class="s3">, </span><span class="s1">Iterable[int]] = ()</span><span class="s3">,</span>
               <span class="s1">axis_env: Optional[Sequence[Tuple[AxisName</span><span class="s3">, </span><span class="s1">int]]] = </span><span class="s3">None,</span>
               <span class="s1">return_shape: Literal[</span><span class="s3">True</span><span class="s1">] = ...</span><span class="s3">,</span>
               <span class="s1">abstracted_axes: Optional[Any] = </span><span class="s3">None,</span>
               <span class="s1">) -&gt; Callable[...</span><span class="s3">, </span><span class="s1">Tuple[core.ClosedJaxpr</span><span class="s3">, </span><span class="s1">Any]]:</span>
  <span class="s1">...</span>

<span class="s3">def </span><span class="s1">make_jaxpr(fun: Callable</span><span class="s3">,</span>
               <span class="s1">static_argnums: Union[int</span><span class="s3">, </span><span class="s1">Iterable[int]] = ()</span><span class="s3">,</span>
               <span class="s1">axis_env: Optional[Sequence[Tuple[AxisName</span><span class="s3">, </span><span class="s1">int]]] = </span><span class="s3">None,</span>
               <span class="s1">return_shape: bool = </span><span class="s3">False,</span>
               <span class="s1">abstracted_axes: Optional[Any] = </span><span class="s3">None,</span>
               <span class="s1">) -&gt; Callable[...</span><span class="s3">, </span><span class="s1">Union[core.ClosedJaxpr</span><span class="s3">,</span>
                                        <span class="s1">Tuple[core.ClosedJaxpr</span><span class="s3">, </span><span class="s1">Any]]]:</span>
  <span class="s2">&quot;&quot;&quot;Creates a function that produces its jaxpr given example args. 
 
  Args: 
    fun: The function whose ``jaxpr`` is to be computed. Its positional 
      arguments and return value should be arrays, scalars, or standard Python 
      containers (tuple/list/dict) thereof. 
    static_argnums: See the :py:func:`jax.jit` docstring. 
    axis_env: Optional, a sequence of pairs where the first element is an axis 
      name and the second element is a positive integer representing the size of 
      the mapped axis with that name. This parameter is useful when lowering 
      functions that involve parallel communication collectives, and it 
      specifies the axis name/size environment that would be set up by 
      applications of :py:func:`jax.pmap`. 
    return_shape: Optional boolean, defaults to ``False``. If ``True``, the 
      wrapped function returns a pair where the first element is the XLA 
      computation and the second element is a pytree with the same structure as 
      the output of ``fun`` and where the leaves are objects with ``shape``, 
      ``dtype``, and ``named_shape`` attributes representing the corresponding 
      types of the output leaves. 
 
  Returns: 
    A wrapped version of ``fun`` that when applied to example arguments returns 
    a ``ClosedJaxpr`` representation of ``fun`` on those arguments. If the 
    argument ``return_shape`` is ``True``, then the returned function instead 
    returns a pair where the first element is the ``ClosedJaxpr`` 
    representation of ``fun`` and the second element is a pytree representing 
    the structure, shape, dtypes, and named shapes of the output of ``fun``. 
 
  A ``jaxpr`` is JAX's intermediate representation for program traces. The 
  ``jaxpr`` language is based on the simply-typed first-order lambda calculus 
  with let-bindings. :py:func:`make_jaxpr` adapts a function to return its 
  ``jaxpr``, which we can inspect to understand what JAX is doing internally. 
  The ``jaxpr`` returned is a trace of ``fun`` abstracted to 
  :py:class:`ShapedArray` level. Other levels of abstraction exist internally. 
 
  We do not describe the semantics of the ``jaxpr`` language in detail here, but 
  instead give a few examples. 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; def f(x): return jax.numpy.sin(jax.numpy.cos(x)) 
  &gt;&gt;&gt; print(f(3.0)) 
  -0.83602 
  &gt;&gt;&gt; jax.make_jaxpr(f)(3.0) 
  { lambda ; a:f32[]. let b:f32[] = cos a; c:f32[] = sin b in (c,) } 
  &gt;&gt;&gt; jax.make_jaxpr(jax.grad(f))(3.0) 
  { lambda ; a:f32[]. let 
      b:f32[] = cos a 
      c:f32[] = sin a 
      _:f32[] = sin b 
      d:f32[] = cos b 
      e:f32[] = mul 1.0 d 
      f:f32[] = neg e 
      g:f32[] = mul f c 
    in (g,) } 
  &quot;&quot;&quot;</span>
  <span class="s1">check_callable(fun)</span>
  <span class="s1">static_argnums = _ensure_index_tuple(static_argnums)</span>

  <span class="s3">def </span><span class="s1">abstractify(args</span><span class="s3">, </span><span class="s1">kwargs):</span>
    <span class="s1">flat_args</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten((args</span><span class="s3">, </span><span class="s1">kwargs))</span>
    <span class="s3">if </span><span class="s1">abstracted_axes </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">map(shaped_abstractify</span><span class="s3">, </span><span class="s1">flat_args)</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True</span><span class="s1">] * len(flat_args)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">axes_specs = _flat_axes_specs(abstracted_axes</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>
      <span class="s1">in_type = pe.infer_lambda_input_type(axes_specs</span><span class="s3">, </span><span class="s1">flat_args)</span>
      <span class="s1">in_avals</span><span class="s3">, </span><span class="s1">keep_inputs = unzip2(in_type)</span>
      <span class="s3">return </span><span class="s1">in_avals</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">keep_inputs</span>

  <span class="s1">@wraps(fun)</span>
  <span class="s1">@api_boundary</span>
  <span class="s3">def </span><span class="s1">make_jaxpr_f(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">f = lu.wrap_init(fun)</span>
    <span class="s3">if </span><span class="s1">static_argnums:</span>
      <span class="s1">dyn_argnums = [i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(args)) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">static_argnums]</span>
      <span class="s1">f</span><span class="s3">, </span><span class="s1">args = argnums_partial(f</span><span class="s3">, </span><span class="s1">dyn_argnums</span><span class="s3">, </span><span class="s1">args)</span>
    <span class="s1">in_avals</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">keep_inputs = abstractify(args</span><span class="s3">, </span><span class="s1">kwargs)</span>
    <span class="s1">in_type = tuple(zip(in_avals</span><span class="s3">, </span><span class="s1">keep_inputs))</span>
    <span class="s1">f</span><span class="s3">, </span><span class="s1">out_tree = flatten_fun(f</span><span class="s3">, </span><span class="s1">in_tree)</span>
    <span class="s1">f = lu.annotate(f</span><span class="s3">, </span><span class="s1">in_type)</span>
    <span class="s1">debug_info = pe.debug_info(fun</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">out_tree</span><span class="s3">, True, </span><span class="s4">'make_jaxpr'</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">ExitStack() </span><span class="s3">as </span><span class="s1">stack:</span>
      <span class="s3">for </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">size </span><span class="s3">in </span><span class="s1">axis_env </span><span class="s3">or </span><span class="s1">[]:</span>
        <span class="s1">stack.enter_context(core.extend_axis_env(axis_name</span><span class="s3">, </span><span class="s1">size</span><span class="s3">, None</span><span class="s1">))</span>
      <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">out_type</span><span class="s3">, </span><span class="s1">consts = pe.trace_to_jaxpr_dynamic2(</span>
          <span class="s1">f</span><span class="s3">, </span><span class="s1">debug_info=debug_info)</span>
    <span class="s1">closed_jaxpr = core.ClosedJaxpr(jaxpr</span><span class="s3">, </span><span class="s1">consts)</span>
    <span class="s3">if </span><span class="s1">return_shape:</span>
      <span class="s1">out_avals</span><span class="s3">, </span><span class="s1">_ = unzip2(out_type)</span>
      <span class="s1">out_shapes_flat = [</span>
          <span class="s1">ShapeDtypeStruct(a.shape</span><span class="s3">, </span><span class="s1">a.dtype</span><span class="s3">, </span><span class="s1">a.named_shape) </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">out_avals]</span>
      <span class="s3">return </span><span class="s1">closed_jaxpr</span><span class="s3">, </span><span class="s1">tree_unflatten(out_tree()</span><span class="s3">, </span><span class="s1">out_shapes_flat)</span>
    <span class="s3">return </span><span class="s1">closed_jaxpr</span>

  <span class="s1">make_jaxpr_f.__name__ = </span><span class="s4">f&quot;make_jaxpr(</span><span class="s3">{</span><span class="s1">make_jaxpr.__name__</span><span class="s3">}</span><span class="s4">)&quot;</span>
  <span class="s3">return </span><span class="s1">make_jaxpr_f</span>

<span class="s3">def </span><span class="s1">_infer_src_sharding(src</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s3">if </span><span class="s1">src </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">src</span>
  <span class="s3">return </span><span class="s1">x.sharding </span><span class="s3">if </span><span class="s1">isinstance(x</span><span class="s3">, </span><span class="s1">array.ArrayImpl) </span><span class="s3">else None</span>


<span class="s3">def </span><span class="s1">device_put(</span>
    <span class="s1">x</span><span class="s3">,</span>
    <span class="s1">device: Union[</span><span class="s3">None, </span><span class="s1">xc.Device</span><span class="s3">, </span><span class="s1">Sharding</span><span class="s3">, </span><span class="s1">Any] = </span><span class="s3">None,</span>
    <span class="s1">*</span><span class="s3">, </span><span class="s1">src: Union[</span><span class="s3">None, </span><span class="s1">xc.Device</span><span class="s3">, </span><span class="s1">Sharding</span><span class="s3">, </span><span class="s1">Any] = </span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Transfers ``x`` to ``device``. 
 
  Args: 
    x: An array, scalar, or (nested) standard Python container thereof. 
    device: The (optional) :py:class:`Device`, `Sharding`, or a (nested) 
      `Sharding` in standard Python container (must be a tree prefix of ``x``), 
      representing the device(s) to which ``x`` should be transferred. If 
      given, then the result is committed to the device(s). 
 
  Returns: 
    A copy of ``x`` that resides on ``device``. 
 
  If the ``device`` parameter is ``None``, then this operation behaves like the 
  identity function if the operand is on any device already, otherwise it 
  transfers the data to the default device, uncommitted. 
 
  For more details on data placement see the 
  :ref:`FAQ on data placement &lt;faq-data-placement&gt;`. 
 
  This function is always asynchronous, i.e. returns immediately without 
  blocking the calling Python thread until any transfers are completed. 
  &quot;&quot;&quot;</span>
  <span class="s3">with </span><span class="s1">config_explicit_device_put_scope():</span>
    <span class="s3">if </span><span class="s1">((device </span><span class="s3">is None or </span><span class="s1">isinstance(device</span><span class="s3">, </span><span class="s1">(xc.Device</span><span class="s3">, </span><span class="s1">Sharding))) </span><span class="s3">and</span>
        <span class="s1">(src </span><span class="s3">is None or </span><span class="s1">isinstance(src</span><span class="s3">, </span><span class="s1">(xc.Device</span><span class="s3">, </span><span class="s1">Sharding)))):</span>
      <span class="s3">return </span><span class="s1">tree_map(</span>
          <span class="s3">lambda </span><span class="s1">y: dispatch.device_put_p.bind(</span>
              <span class="s1">y</span><span class="s3">, </span><span class="s1">device=device</span><span class="s3">, </span><span class="s1">src=_infer_src_sharding(src</span><span class="s3">, </span><span class="s1">y))</span><span class="s3">, </span><span class="s1">x)</span>

    <span class="s1">x_flat</span><span class="s3">, </span><span class="s1">treedef = tree_flatten(x)</span>
    <span class="s1">device_flat = flatten_axes(</span><span class="s4">&quot;device_put device&quot;</span><span class="s3">, </span><span class="s1">treedef</span><span class="s3">, </span><span class="s1">device)</span>
    <span class="s1">src_flat = flatten_axes(</span><span class="s4">&quot;device_put source&quot;</span><span class="s3">, </span><span class="s1">treedef</span><span class="s3">, </span><span class="s1">src)</span>
    <span class="s1">out_flat = [</span>
        <span class="s1">dispatch.device_put_p.bind(y</span><span class="s3">, </span><span class="s1">device=d</span><span class="s3">, </span><span class="s1">src=_infer_src_sharding(s</span><span class="s3">, </span><span class="s1">y))</span>
        <span class="s3">for </span><span class="s1">y</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s1">s </span><span class="s3">in </span><span class="s1">zip(x_flat</span><span class="s3">, </span><span class="s1">device_flat</span><span class="s3">, </span><span class="s1">src_flat)</span>
    <span class="s1">]</span>
    <span class="s3">return </span><span class="s1">tree_unflatten(treedef</span><span class="s3">, </span><span class="s1">out_flat)</span>


<span class="s3">def </span><span class="s1">device_put_sharded(shards: Sequence[Any]</span><span class="s3">, </span><span class="s1">devices: Sequence[xc.Device]):  </span><span class="s0"># noqa: F811</span>
  <span class="s2">&quot;&quot;&quot;Transfer array shards to specified devices and form Array(s). 
 
  Args: 
    shards: A sequence of arrays, scalars, or (nested) standard Python 
      containers thereof representing the shards to be stacked together to form 
      the output. The length of ``shards`` must equal the length of ``devices``. 
    devices: A sequence of :py:class:`Device` instances representing the devices 
      to which corresponding shards in ``shards`` will be transferred. 
 
  This function is always asynchronous, i.e. returns immediately. 
 
  Returns: 
    A Array or (nested) Python container thereof representing the 
    elements of ``shards`` stacked together, with each shard backed by physical 
    device memory specified by the corresponding entry in ``devices``. 
 
  Examples: 
    Passing a list of arrays for ``shards`` results in a sharded array 
    containing a stacked version of the inputs: 
 
    &gt;&gt;&gt; import jax 
    &gt;&gt;&gt; devices = jax.local_devices() 
    &gt;&gt;&gt; x = [jax.numpy.ones(5) for device in devices] 
    &gt;&gt;&gt; y = jax.device_put_sharded(x, devices) 
    &gt;&gt;&gt; np.allclose(y, jax.numpy.stack(x)) 
    True 
 
    Passing a list of nested container objects with arrays at the leaves for 
    ``shards`` corresponds to stacking the shards at each leaf. This requires 
    all entries in the list to have the same tree structure: 
 
    &gt;&gt;&gt; x = [(i, jax.numpy.arange(i, i + 4)) for i in range(len(devices))] 
    &gt;&gt;&gt; y = jax.device_put_sharded(x, devices) 
    &gt;&gt;&gt; type(y) 
    &lt;class 'tuple'&gt; 
    &gt;&gt;&gt; y0 = jax.device_put_sharded([a for a, b in x], devices) 
    &gt;&gt;&gt; y1 = jax.device_put_sharded([b for a, b in x], devices) 
    &gt;&gt;&gt; np.allclose(y[0], y0) 
    True 
    &gt;&gt;&gt; np.allclose(y[1], y1) 
    True 
 
  See Also: 
    - device_put 
    - device_put_replicated 
  &quot;&quot;&quot;</span>
  <span class="s0"># TODO(jakevdp): provide a default for devices that considers both local</span>
  <span class="s0"># devices and pods</span>
  <span class="s3">if not </span><span class="s1">isinstance(shards</span><span class="s3">, </span><span class="s1">Sequence):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;device_put_sharded `shards` input must be a sequence; &quot;</span>
                     <span class="s4">f&quot;got </span><span class="s3">{</span><span class="s1">type(shards)</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">len(shards) != len(devices):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;len(shards) = </span><span class="s3">{</span><span class="s1">len(shards)</span><span class="s3">} </span><span class="s4">must equal &quot;</span>
                     <span class="s4">f&quot;len(devices) = </span><span class="s3">{</span><span class="s1">len(devices)</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">_device_put_sharded(*xs):</span>
    <span class="s1">avals = [core.raise_to_shaped(core.get_aval(x)) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">xs]</span>
    <span class="s3">if not </span><span class="s1">all(a1 == a2 </span><span class="s3">for </span><span class="s1">a1</span><span class="s3">, </span><span class="s1">a2 </span><span class="s3">in </span><span class="s1">zip(avals[:-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">avals[</span><span class="s5">1</span><span class="s1">:])):</span>
      <span class="s1">a1</span><span class="s3">, </span><span class="s1">a2 = next((a1</span><span class="s3">, </span><span class="s1">a2) </span><span class="s3">for </span><span class="s1">a1</span><span class="s3">, </span><span class="s1">a2 </span><span class="s3">in </span><span class="s1">zip(avals[:-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">avals[</span><span class="s5">1</span><span class="s1">:])</span>
                    <span class="s3">if </span><span class="s1">a1 != a2)</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;the shards passed to device_put_sharded must have &quot;</span>
                       <span class="s4">f&quot;consistent shape and dtype, but got </span><span class="s3">{</span><span class="s1">a1</span><span class="s3">} </span><span class="s4">and </span><span class="s3">{</span><span class="s1">a2</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
    <span class="s1">stacked_aval = avals[</span><span class="s5">0</span><span class="s1">].update(shape=(len(devices)</span><span class="s3">,</span><span class="s1">) + avals[</span><span class="s5">0</span><span class="s1">].shape)</span>
    <span class="s1">sharding_spec = sharding_specs.create_pmap_sharding_spec(stacked_aval.shape)</span>
    <span class="s1">sharding = PmapSharding(np.array(devices)</span><span class="s3">, </span><span class="s1">sharding_spec)</span>
    <span class="s3">if </span><span class="s1">dtypes.is_opaque_dtype(stacked_aval.dtype):</span>
      <span class="s3">return </span><span class="s1">stacked_aval.dtype._rules.device_put_sharded(xs</span><span class="s3">, </span><span class="s1">stacked_aval</span><span class="s3">, </span><span class="s1">sharding</span><span class="s3">, </span><span class="s1">devices)</span>
    <span class="s3">return </span><span class="s1">pxla.batched_device_put(stacked_aval</span><span class="s3">, </span><span class="s1">sharding</span><span class="s3">, </span><span class="s1">xs</span><span class="s3">, </span><span class="s1">list(devices))</span>


  <span class="s3">with </span><span class="s1">config_explicit_device_put_scope():</span>
    <span class="s3">return </span><span class="s1">tree_map(_device_put_sharded</span><span class="s3">, </span><span class="s1">*shards)</span>


<span class="s3">def </span><span class="s1">device_put_replicated(x: Any</span><span class="s3">, </span><span class="s1">devices: Sequence[xc.Device]):  </span><span class="s0"># noqa: F811</span>
  <span class="s2">&quot;&quot;&quot;Transfer array(s) to each specified device and form Array(s). 
 
  Args: 
    x: an array, scalar, or (nested) standard Python container thereof 
      representing the array to be replicated to form the output. 
    devices: A sequence of :py:class:`Device` instances representing the devices 
      to which ``x`` will be transferred. 
 
  This function is always asynchronous, i.e. returns immediately. 
 
  Returns: 
    An Array or (nested) Python container thereof representing the 
    value of ``x`` broadcasted along a new leading axis of size 
    ``len(devices)``, with each slice along that new leading axis backed by 
    memory on the device specified by the corresponding entry in ``devices``. 
 
  Examples: 
    Passing an array: 
 
    &gt;&gt;&gt; import jax 
    &gt;&gt;&gt; devices = jax.local_devices() 
    &gt;&gt;&gt; x = jax.numpy.array([1., 2., 3.]) 
    &gt;&gt;&gt; y = jax.device_put_replicated(x, devices) 
    &gt;&gt;&gt; np.allclose(y, jax.numpy.stack([x for _ in devices])) 
    True 
 
  See Also: 
    - device_put 
    - device_put_sharded 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(devices</span><span class="s3">, </span><span class="s1">Sequence) </span><span class="s3">or not </span><span class="s1">devices:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;`devices` argument to `device_put_replicated must be &quot;</span>
                     <span class="s4">&quot;a non-empty sequence.&quot;</span><span class="s1">)</span>
  <span class="s3">def </span><span class="s1">_device_put_replicated(x):</span>
    <span class="s1">aval = core.unmapped_aval(len(devices)</span><span class="s3">, </span><span class="s1">core.no_axis_name</span><span class="s3">, </span><span class="s5">0</span><span class="s3">,</span>
                              <span class="s1">core.raise_to_shaped(core.get_aval(x)))</span>
    <span class="s3">assert </span><span class="s1">isinstance(aval</span><span class="s3">, </span><span class="s1">ShapedArray)</span>
    <span class="s1">sharding_spec = sharding_specs.create_pmap_sharding_spec(aval.shape)</span>
    <span class="s1">buf = device_put(x</span><span class="s3">, </span><span class="s1">devices[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">sharding = PmapSharding(np.array(devices)</span><span class="s3">, </span><span class="s1">sharding_spec)</span>
    <span class="s3">if </span><span class="s1">dtypes.is_opaque_dtype(aval.dtype):</span>
      <span class="s3">return </span><span class="s1">aval.dtype._rules.device_put_replicated(buf</span><span class="s3">, </span><span class="s1">aval</span><span class="s3">, </span><span class="s1">sharding</span><span class="s3">, </span><span class="s1">devices)</span>
    <span class="s3">assert </span><span class="s1">len(xla.aval_to_xla_shapes(aval)) == </span><span class="s5">1</span>
    <span class="s3">return </span><span class="s1">pxla.batched_device_put(aval</span><span class="s3">, </span><span class="s1">sharding</span><span class="s3">, </span><span class="s1">[buf] * len(devices)</span><span class="s3">, </span><span class="s1">devices)</span>

  <span class="s3">with </span><span class="s1">config_explicit_device_put_scope():</span>
    <span class="s3">return </span><span class="s1">tree_map(_device_put_replicated</span><span class="s3">, </span><span class="s1">x)</span>


<span class="s0"># TODO(mattjj): consider revising</span>
<span class="s3">def </span><span class="s1">_device_get(x):</span>
  <span class="s3">if </span><span class="s1">isinstance(x</span><span class="s3">, </span><span class="s1">core.Tracer):</span>
    <span class="s3">return </span><span class="s1">x</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">toarray = x.__array__</span>
  <span class="s3">except </span><span class="s1">AttributeError:</span>
    <span class="s3">return </span><span class="s1">x</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">toarray()</span>

<span class="s3">def </span><span class="s1">device_get(x: Any):</span>
  <span class="s2">&quot;&quot;&quot;Transfer ``x`` to host. 
 
  If ``x`` is a pytree, then the individual buffers are copied in parallel. 
 
  Args: 
    x: An array, scalar, Array or (nested) standard Python container thereof 
      representing the array to be transferred to host. 
 
  Returns: 
    An array or (nested) Python container thereof representing the 
    value of ``x``. 
 
  Examples: 
    Passing a Array: 
 
    &gt;&gt;&gt; import jax 
    &gt;&gt;&gt; x = jax.numpy.array([1., 2., 3.]) 
    &gt;&gt;&gt; jax.device_get(x) 
    array([1., 2., 3.], dtype=float32) 
 
    Passing a scalar (has no effect): 
 
    &gt;&gt;&gt; jax.device_get(1) 
    1 
 
  See Also: 
    - device_put 
    - device_put_sharded 
    - device_put_replicated 
  &quot;&quot;&quot;</span>
  <span class="s3">with </span><span class="s1">config_explicit_device_get_scope():</span>
    <span class="s3">for </span><span class="s1">y </span><span class="s3">in </span><span class="s1">tree_leaves(x):</span>
      <span class="s3">try</span><span class="s1">:</span>
        <span class="s1">y.copy_to_host_async()</span>
      <span class="s3">except </span><span class="s1">AttributeError:</span>
        <span class="s3">pass</span>
    <span class="s3">return </span><span class="s1">tree_map(_device_get</span><span class="s3">, </span><span class="s1">x)</span>


<span class="s3">class </span><span class="s1">ShapeDtypeStruct:</span>
  <span class="s2">&quot;&quot;&quot;A container for the shape, dtype, and other static attributes of an array. 
 
  ``ShapeDtypeStruct`` is often used in conjunction with :func:`jax.eval_shape`. 
 
  Args: 
    shape: a sequence of integers representing an array shape 
    dtype: a dtype-like object 
    named_shape: (optional) a dictionary representing a named shape 
    sharding: (optional) a :class:`jax.Sharding` object 
  &quot;&quot;&quot;</span>
  <span class="s1">__slots__ = [</span><span class="s4">&quot;shape&quot;</span><span class="s3">, </span><span class="s4">&quot;dtype&quot;</span><span class="s3">, </span><span class="s4">&quot;named_shape&quot;</span><span class="s3">, </span><span class="s4">&quot;sharding&quot;</span><span class="s1">]</span>
  <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">, </span><span class="s1">named_shape=</span><span class="s3">None, </span><span class="s1">sharding=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s1">self.shape = tuple(shape)</span>
    <span class="s3">if </span><span class="s1">dtype </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;ShapeDtypeStruct: dtype must be specified.&quot;</span><span class="s1">)</span>
    <span class="s1">self.dtype = dtype </span><span class="s3">if </span><span class="s1">dtypes.is_opaque_dtype(dtype) </span><span class="s3">else </span><span class="s1">np.dtype(dtype)</span>
    <span class="s3">if </span><span class="s1">sharding </span><span class="s3">is not None</span><span class="s1">:</span>
      <span class="s3">if not </span><span class="s1">isinstance(sharding</span><span class="s3">, </span><span class="s1">Sharding):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;sharding should be an instance of `jax.sharding.Sharding`. &quot;</span>
            <span class="s4">f&quot;Got </span><span class="s3">{</span><span class="s1">sharding</span><span class="s3">} </span><span class="s4">of type </span><span class="s3">{</span><span class="s1">type(sharding)</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
      <span class="s1">self.sharding = sharding</span>
    <span class="s1">self.named_shape = {} </span><span class="s3">if </span><span class="s1">named_shape </span><span class="s3">is None else </span><span class="s1">dict(named_shape)</span>

  <span class="s1">size = property(</span><span class="s3">lambda </span><span class="s1">self: math.prod(self.shape))</span>
  <span class="s1">ndim = property(</span><span class="s3">lambda </span><span class="s1">self: len(self.shape))</span>

  <span class="s3">def </span><span class="s1">__len__(self):</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">self.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s3">except </span><span class="s1">IndexError </span><span class="s3">as </span><span class="s1">e:</span>
      <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;len() of unsized object&quot;</span><span class="s1">) </span><span class="s3">from </span><span class="s1">e </span><span class="s0"># same as numpy error</span>

  <span class="s3">def </span><span class="s1">__repr__(self):</span>
    <span class="s1">ns = </span><span class="s4">f&quot;, named_shape=</span><span class="s3">{</span><span class="s1">self.named_shape</span><span class="s3">}</span><span class="s4">&quot; </span><span class="s3">if </span><span class="s1">self.named_shape </span><span class="s3">else </span><span class="s4">&quot;&quot;</span>
    <span class="s1">sh = </span><span class="s4">f&quot;, sharding=</span><span class="s3">{</span><span class="s1">self.sharding</span><span class="s3">}</span><span class="s4">&quot; </span><span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;sharding&quot;</span><span class="s1">) </span><span class="s3">else </span><span class="s4">&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">type(self).__name__</span><span class="s3">}</span><span class="s4">(shape=</span><span class="s3">{</span><span class="s1">self.shape</span><span class="s3">}</span><span class="s4">, &quot;</span>
            <span class="s4">f&quot;dtype=</span><span class="s3">{</span><span class="s1">self.dtype.name</span><span class="s3">}{</span><span class="s1">ns</span><span class="s3">}{</span><span class="s1">sh</span><span class="s3">}</span><span class="s4">)&quot;</span><span class="s1">)</span>

  <span class="s1">__str__ = __repr__</span>

  <span class="s3">def </span><span class="s1">__eq__(self</span><span class="s3">, </span><span class="s1">other):</span>
    <span class="s3">if not </span><span class="s1">isinstance(other</span><span class="s3">, </span><span class="s1">ShapeDtypeStruct):</span>
      <span class="s3">return False</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">other_sh = other.sharding </span><span class="s3">if </span><span class="s1">hasattr(other</span><span class="s3">, </span><span class="s4">&quot;sharding&quot;</span><span class="s1">) </span><span class="s3">else None</span>
      <span class="s1">sh = self.sharding </span><span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;sharding&quot;</span><span class="s1">) </span><span class="s3">else None</span>
      <span class="s3">return </span><span class="s1">((other.shape</span><span class="s3">, </span><span class="s1">other.dtype</span><span class="s3">, </span><span class="s1">other.named_shape</span><span class="s3">, </span><span class="s1">other_sh) ==</span>
              <span class="s1">(self.shape</span><span class="s3">, </span><span class="s1">self.dtype</span><span class="s3">, </span><span class="s1">self.named_shape</span><span class="s3">, </span><span class="s1">sh))</span>

  <span class="s3">def </span><span class="s1">__hash__(self):</span>
    <span class="s0"># TODO(frostig): avoid the conversion from dict by addressing</span>
    <span class="s0"># https://github.com/google/jax/issues/8182</span>
    <span class="s1">named = frozenset(self.named_shape.items())</span>
    <span class="s1">sh = self.sharding </span><span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;sharding&quot;</span><span class="s1">) </span><span class="s3">else None</span>
    <span class="s3">return </span><span class="s1">hash((self.shape</span><span class="s3">, </span><span class="s1">self.dtype</span><span class="s3">, </span><span class="s1">named</span><span class="s3">, </span><span class="s1">sh))</span>

<span class="s1">core.pytype_aval_mappings[ShapeDtypeStruct] = (</span>
    <span class="s3">lambda </span><span class="s1">x: ShapedArray(x.shape</span><span class="s3">, </span><span class="s1">dtypes.canonicalize_dtype(x.dtype</span><span class="s3">, </span><span class="s1">allow_opaque_dtype=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
                          <span class="s1">weak_type=</span><span class="s3">False, </span><span class="s1">named_shape=x.named_shape))</span>

<span class="s1">@api_boundary</span>
<span class="s3">def </span><span class="s1">eval_shape(fun: Callable</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s2">&quot;&quot;&quot;Compute the shape/dtype of ``fun`` without any FLOPs. 
 
  This utility function is useful for performing shape inference. Its 
  input/output behavior is defined by:: 
 
    def eval_shape(fun, *args, **kwargs): 
      out = fun(*args, **kwargs) 
      shape_dtype_struct = lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype) 
      return jax.tree_util.tree_map(shape_dtype_struct, out) 
 
  But instead of applying ``fun`` directly, which might be expensive, it uses 
  JAX's abstract interpretation machinery to evaluate the shapes without doing 
  any FLOPs. 
 
  Using :py:func:`eval_shape` can also catch shape errors, and will raise same 
  shape errors as evaluating ``fun(*args, **kwargs)``. 
 
  Args: 
    fun: The function whose output shape should be evaluated. 
    *args: a positional argument tuple of arrays, scalars, or (nested) standard 
      Python containers (tuples, lists, dicts, namedtuples, i.e. pytrees) of 
      those types. Since only the ``shape`` and ``dtype`` attributes are 
      accessed, one can use :class:`jax.ShapeDtypeStruct` or another container 
      that duck-types as ndarrays (note however that duck-typed objects cannot 
      be namedtuples because those are treated as standard Python containers). 
    **kwargs: a keyword argument dict of arrays, scalars, or (nested) standard 
      Python containers (pytrees) of those types. As in ``args``, array values 
      need only be duck-typed to have ``shape`` and ``dtype`` attributes. 
 
  Returns: 
    out: a nested PyTree containing :class:`jax.ShapeDtypeStruct` objects as leaves. 
 
  For example: 
 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; import jax.numpy as jnp 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; f = lambda A, x: jnp.tanh(jnp.dot(A, x)) 
  &gt;&gt;&gt; A = jax.ShapeDtypeStruct((2000, 3000), jnp.float32) 
  &gt;&gt;&gt; x = jax.ShapeDtypeStruct((3000, 1000), jnp.float32) 
  &gt;&gt;&gt; out = jax.eval_shape(f, A, x)  # no FLOPs performed 
  &gt;&gt;&gt; print(out.shape) 
  (2000, 1000) 
  &gt;&gt;&gt; print(out.dtype) 
  float32 
  &quot;&quot;&quot;</span>
  <span class="s1">args_flat</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten((args</span><span class="s3">, </span><span class="s1">kwargs))</span>
  <span class="s1">wrapped_fun</span><span class="s3">, </span><span class="s1">out_tree = flatten_fun(lu.wrap_init(fun)</span><span class="s3">, </span><span class="s1">in_tree)</span>
  <span class="s1">debug_info = pe.debug_info(fun</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">out_tree</span><span class="s3">, True, </span><span class="s4">&quot;eval_shape&quot;</span><span class="s1">)</span>
  <span class="s1">out = pe.abstract_eval_fun(wrapped_fun.call_wrapped</span><span class="s3">,</span>
                             <span class="s1">*map(shaped_abstractify</span><span class="s3">, </span><span class="s1">args_flat)</span><span class="s3">,</span>
                             <span class="s1">debug_info=debug_info)</span>
  <span class="s1">out = [ShapeDtypeStruct(x.shape</span><span class="s3">, </span><span class="s1">x.dtype</span><span class="s3">, </span><span class="s1">x.named_shape) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">out]</span>
  <span class="s3">return </span><span class="s1">tree_unflatten(out_tree()</span><span class="s3">, </span><span class="s1">out)</span>


<span class="s3">def </span><span class="s1">named_call(</span>
    <span class="s1">fun: Callable[...</span><span class="s3">, </span><span class="s1">Any]</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">name: Optional[str] = </span><span class="s3">None,</span>
  <span class="s1">) -&gt; Callable[...</span><span class="s3">, </span><span class="s1">Any]:</span>
  <span class="s2">&quot;&quot;&quot;Adds a user specified name to a function when staging out JAX computations. 
 
  When staging out computations for just-in-time compilation to XLA (or other 
  backends such as TensorFlow) JAX runs your Python program but by default does 
  not preserve any of the function names or other metadata associated with it. 
  This can make debugging the staged out (and/or compiled) representation of 
  your program complicated because there is limited context information for each 
  operation being executed. 
 
  `named_call` tells JAX to stage the given function out as a subcomputation 
  with a specific name. When the staged out program is compiled with XLA these 
  named subcomputations are preserved and show up in debugging utilities like 
  the TensorFlow Profiler in TensorBoard. Names are also preserved when staging 
  out JAX programs to TensorFlow using :func:`experimental.jax2tf.convert`. 
 
  Args: 
    fun: Function to be wrapped. This can be any Callable. 
    name: Optional. The prefix to use to name all sub computations created 
      within the name scope. Use the fun.__name__ if not specified. 
 
  Returns: 
    A version of `fun` that is wrapped in a name_scope. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">name </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">name = fun.__name__</span>

  <span class="s3">return </span><span class="s1">source_info_util.extend_name_stack(name)(fun)</span>

<span class="s1">@contextmanager</span>
<span class="s3">def </span><span class="s1">named_scope(</span>
    <span class="s1">name: str</span><span class="s3">,</span>
  <span class="s1">) -&gt; Generator[</span><span class="s3">None, None, None</span><span class="s1">]:</span>
  <span class="s2">&quot;&quot;&quot;A context manager that adds a user specified name to the JAX name stack. 
 
  When staging out computations for just-in-time compilation to XLA (or other 
  backends such as TensorFlow) JAX does not, by default, preserve the names 
  (or other source metadata) of Python functions it encounters. 
  This can make debugging the staged out (and/or compiled) representation of 
  your program complicated because there is limited context information for each 
  operation being executed. 
 
  ``named_scope`` tells JAX to stage the given function with additional 
  annotations on the underlying operations. JAX internally keeps track of these 
  annotations in a name stack. When the staged out program is compiled with XLA 
  these annotations are preserved and show up in debugging utilities like the 
  TensorFlow Profiler in TensorBoard. Names are also preserved when staging out 
  JAX programs to TensorFlow using :func:`experimental.jax2tf.convert`. 
 
 
  Args: 
    name: The prefix to use to name all operations created within the name 
      scope. 
  Yields: 
    Yields ``None``, but enters a context in which `name` will be appended to 
    the active name stack. 
 
  Examples: 
    ``named_scope`` can be used as a context manager inside compiled functions: 
 
    &gt;&gt;&gt; import jax 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; @jax.jit 
    ... def layer(w, x): 
    ...   with jax.named_scope(&quot;dot_product&quot;): 
    ...     logits = w.dot(x) 
    ...   with jax.named_scope(&quot;activation&quot;): 
    ...     return jax.nn.relu(logits) 
 
    It can also be used as a decorator: 
 
    &gt;&gt;&gt; @jax.jit 
    ... @jax.named_scope(&quot;layer&quot;) 
    ... def layer(w, x): 
    ...   logits = w.dot(x) 
    ...   return jax.nn.relu(logits) 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(name</span><span class="s3">, </span><span class="s1">str):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;named_scope name argument must be a string.&quot;</span><span class="s1">)</span>
  <span class="s3">with </span><span class="s1">source_info_util.extend_name_stack(name):</span>
    <span class="s3">yield</span>

<span class="s3">def </span><span class="s1">effects_barrier():</span>
  <span class="s2">&quot;&quot;&quot;Waits until existing functions have completed any side-effects.&quot;&quot;&quot;</span>
  <span class="s1">dispatch.runtime_tokens.block_until_ready()</span>

<span class="s3">def </span><span class="s1">block_until_ready(x):</span>
  <span class="s2">&quot;&quot;&quot; 
  Tries to call a ``block_until_ready`` method on pytree leaves. 
 
  Args: 
    x: a pytree, usually with at least some JAX array instances at its leaves. 
 
  Returns: 
    A pytree with the same structure and values of the input, where the values 
    of all JAX array leaves are ready. 
  &quot;&quot;&quot;</span>
  <span class="s3">def </span><span class="s1">try_to_block(x):</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">x.block_until_ready()</span>
    <span class="s3">except </span><span class="s1">AttributeError:</span>
      <span class="s3">return </span><span class="s1">x</span>
  <span class="s3">return </span><span class="s1">tree_map(try_to_block</span><span class="s3">, </span><span class="s1">x)</span>


<span class="s3">def </span><span class="s1">clear_backends():</span>
  <span class="s2">&quot;&quot;&quot; 
  Clear all backend clients so that new backend clients can be created later. 
  &quot;&quot;&quot;</span>
  <span class="s1">xb._clear_backends()</span>
  <span class="s1">xb.local_devices.cache_clear()</span>
  <span class="s1">xb.process_count.cache_clear()</span>
  <span class="s1">dispatch.xla_primitive_callable.cache_clear()</span>
  <span class="s1">pjit._pjit_lower_cached.cache_clear()</span>
  <span class="s1">pjit._create_pjit_jaxpr.cache_clear()  </span><span class="s0"># pytype: disable=attribute-error</span>
  <span class="s1">pjit._cpp_pjit_cache.clear()</span>
  <span class="s1">xc._xla.PjitFunctionCache.clear_all()</span>

<span class="s3">def </span><span class="s1">live_arrays(platform=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Return all live arrays in the backend for `platform`. 
 
  If platform is None, it is the default backend. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">xb.get_backend(platform).live_arrays()</span>

<span class="s3">def </span><span class="s1">clear_caches():</span>
  <span class="s2">&quot;&quot;&quot;Clear all compilation and staging caches.&quot;&quot;&quot;</span>
  <span class="s0"># Clear all lu.cache and util.weakref_lru_cache instances (used for staging</span>
  <span class="s0"># and Python-dispatch compiled executable caches).</span>
  <span class="s1">lu.clear_all_caches()</span>
  <span class="s1">util.clear_all_weakref_lru_caches()</span>

  <span class="s0"># Clear all C++ compiled executable caches for pjit</span>
  <span class="s1">pjit._cpp_pjit_cache.clear()</span>
  <span class="s1">xc._xla.PjitFunctionCache.clear_all()</span>

  <span class="s0"># Clear all C++ compiled executable caches for pmap</span>
  <span class="s3">for </span><span class="s1">fun </span><span class="s3">in </span><span class="s1">_pmap_cache_clears:</span>
    <span class="s1">fun._cache_clear()</span>

  <span class="s0"># Clear particular util.cache instances.</span>
  <span class="s1">dispatch.xla_primitive_callable.cache_clear()</span>
</pre>
</body>
</html>